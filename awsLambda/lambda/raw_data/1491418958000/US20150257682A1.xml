<?xml version="1.0" encoding="utf-8"?>
<!-- Copyright Â©2016 LexisNexis Univentio, The Netherlands. -->
<lexisnexis-patent-document schema-version="1.13" date-produced="20160127" file="US20150257682A1.xml" produced-by="LexisNexis-Univentio" lang="eng" date-inserted="20150917" time-inserted="030213" date-changed="20151002" time-changed="033557">
  <bibliographic-data lang="eng">
    <publication-reference publ-type="Application" publ-desc="Patent Application Publication">
      <document-id id="121315771">
        <country>US</country>
        <doc-number>20150257682</doc-number>
        <kind>A1</kind>
        <date>20150917</date>
      </document-id>
    </publication-reference>
    <application-reference appl-type="utility">
      <document-id>
        <country>US</country>
        <doc-number>14660338</doc-number>
        <date>20150317</date>
      </document-id>
    </application-reference>
    <application-series-code>14</application-series-code>
    <language-of-filing>eng</language-of-filing>
    <language-of-publication>eng</language-of-publication>
    <dates-of-public-availability date-changed="20150924">
      <unexamined-printed-without-grant>
        <date>20150917</date>
      </unexamined-printed-without-grant>
    </dates-of-public-availability>
    <classifications-ipcr date-changed="20150924">
      <classification-ipcr sequence="1">
        <text>A61B   5/11        20060101AFI20150917BHUS        </text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>5</main-group>
        <subgroup>11</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>US</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>A61B   5/00        20060101ALI20150917BHUS        </text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>5</main-group>
        <subgroup>00</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>US</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-ipcr>
      <classification-ipcr sequence="3">
        <text>G06K   9/00        20060101ALI20150917BHUS        </text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>K</subclass>
        <main-group>9</main-group>
        <subgroup>00</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>US</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-ipcr>
    </classifications-ipcr>
    <classifications-cpc date-changed="20150924">
      <classification-cpc sequence="1">
        <text>A61B   5/1128      20130101 FI20150917BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>5</main-group>
        <subgroup>1128</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="2">
        <text>A61B   5/0075      20130101 LI20150917BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>5</main-group>
        <subgroup>0075</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="3">
        <text>A61B   5/1121      20130101 LI20150917BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>5</main-group>
        <subgroup>1121</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="4">
        <text>A61B   5/486       20130101 LI20150917BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>5</main-group>
        <subgroup>486</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="5">
        <text>A61B   5/7475      20130101 LI20150917BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>5</main-group>
        <subgroup>7475</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="6">
        <text>A61B2576/00        20130101 LA20150917BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>2576</main-group>
        <subgroup>00</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="7">
        <text>G06K   9/00348     20130101 LI20150917BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>G</section>
        <class>06</class>
        <subclass>K</subclass>
        <main-group>9</main-group>
        <subgroup>00348</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
    </classifications-cpc>
    <number-of-claims calculated="yes">21</number-of-claims>
    <invention-title id="title_eng" date-changed="20150917" lang="eng" format="original">METHOD AND SYSTEM FOR DELIVERING BIOMECHANICAL FEEDBACK TO HUMAN AND OBJECT MOTION</invention-title>
    <related-documents date-changed="20150917">
      <provisional-application>
        <document-id>
          <country>US</country>
          <doc-number>61954375</doc-number>
          <date>20140317</date>
        </document-id>
      </provisional-application>
    </related-documents>
    <parties date-changed="20150917">
      <applicants>
        <applicant sequence="1" app-type="applicant" designation="us-only">
          <addressbook lang="eng">
            <last-name>Hansen</last-name>
            <first-name>Ben</first-name>
            <address>
              <city>Bradenton</city>
              <state>FL</state>
              <country>US</country>
            </address>
          </addressbook>
          <residence>
            <country>US</country>
          </residence>
        </applicant>
        <applicant sequence="2" app-type="applicant" designation="us-only">
          <addressbook lang="eng">
            <last-name>Nolan</last-name>
            <first-name>Joe</first-name>
            <address>
              <city>Massapequa</city>
              <state>NY</state>
              <country>US</country>
            </address>
          </addressbook>
          <residence>
            <country>US</country>
          </residence>
        </applicant>
        <applicant sequence="3" app-type="applicant" designation="us-only">
          <addressbook lang="eng">
            <last-name>Robinson</last-name>
            <first-name>Keith</first-name>
            <address>
              <city>Seaford</city>
              <state>NY</state>
              <country>US</country>
            </address>
          </addressbook>
          <residence>
            <country>US</country>
          </residence>
        </applicant>
        <applicant sequence="4" app-type="applicant" designation="us-only">
          <addressbook lang="eng">
            <last-name>Fortenbaugh</last-name>
            <first-name>Dave</first-name>
            <address>
              <city>Birmingham</city>
              <state>AL</state>
              <country>US</country>
            </address>
          </addressbook>
          <residence>
            <country>US</country>
          </residence>
        </applicant>
      </applicants>
      <inventors>
        <inventor sequence="1" designation="us-only">
          <addressbook lang="eng">
            <last-name>Hansen</last-name>
            <first-name>Ben</first-name>
            <address>
              <city>Bradenton</city>
              <state>FL</state>
              <country>US</country>
            </address>
          </addressbook>
        </inventor>
        <inventor sequence="2" designation="us-only">
          <addressbook lang="eng">
            <last-name>Nolan</last-name>
            <first-name>Joe</first-name>
            <address>
              <city>Massapequa</city>
              <state>NY</state>
              <country>US</country>
            </address>
          </addressbook>
        </inventor>
        <inventor sequence="3" designation="us-only">
          <addressbook lang="eng">
            <last-name>Robinson</last-name>
            <first-name>Keith</first-name>
            <address>
              <city>Seaford</city>
              <state>NY</state>
              <country>US</country>
            </address>
          </addressbook>
        </inventor>
        <inventor sequence="4" designation="us-only">
          <addressbook lang="eng">
            <last-name>Fortenbaugh</last-name>
            <first-name>Dave</first-name>
            <address>
              <city>Birmingham</city>
              <state>AL</state>
              <country>US</country>
            </address>
          </addressbook>
        </inventor>
      </inventors>
    </parties>
    <patent-family date-changed="20150917">
      <main-family family-id="173669349">
        <family-member>
          <document-id>
            <country>US</country>
            <doc-number>20150257682</doc-number>
            <kind>A1</kind>
            <date>20150917</date>
          </document-id>
          <application-date>
            <date>20150317</date>
          </application-date>
        </family-member>
      </main-family>
      <complete-family family-id="173669347">
        <family-member>
          <document-id>
            <country>US</country>
            <doc-number>20150257682</doc-number>
            <kind>A1</kind>
            <date>20150917</date>
          </document-id>
          <application-date>
            <date>20150317</date>
          </application-date>
        </family-member>
      </complete-family>
    </patent-family>
  </bibliographic-data>
  <abstract id="abstr_eng" date-changed="20150917" lang="eng" format="original">
    <p id="p-a-00001-en" num="0000">A method and system to deliver biomechanical feedback utilizes three major elements: (1) multiple hardware data capture devices, including optical motion capture, inertial measurement units, infrared scanning devices, and two-dimensional RGB consecutive image capture devices, (2) a cross-platform compatible physics engine compatible with optical motion capture, inertial measurement units, infrared scanning devices, and two-dimensional RGB consecutive image capture devices, and (3) interactive platforms and user-interfaces to deliver real-time feedback to motions of human subjects and any objects in their possession and proximity.</p>
  </abstract>
  <legal-data date-changed="20151002">
    <legal-event sequence="1">
      <publication-date>
        <date>20150429</date>
      </publication-date>
      <event-code-1>AS</event-code-1>
      <legal-description>ASSIGNMENT</legal-description>
      <status-identifier>N</status-identifier>
      <docdb-publication-number> US  2015257682A1</docdb-publication-number>
      <docdb-application-id>444511044</docdb-application-id>
      <new-owner>CORE SPORTS TECHNOLOGY GROUP, NEW YORK</new-owner>
      <free-text-description>ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:HANSEN, BEN;NOLAN, JOE;ROBINSON, KEITH;AND OTHERS;SIGNING DATES FROM 20150422 TO 20150428;REEL/FRAME:035525/0207</free-text-description>
    </legal-event>
  </legal-data>
  <description id="descr_eng" lang="eng" format="original" date-changed="20150917">
    <summary>
      <heading id="h-00001-en" level="1">FIELD OF THE INVENTION</heading>
      <p id="p-00001-en" num="0001">This disclosure relates to a method and system for delivering biomechanical feedback to human and object motions.</p>
      <heading id="h-00002-en" level="1">BACKGROUND OF INVENTION</heading>
      <p id="p-00002-en" num="0002">Systems capable of delivering human and object motion currently exist but are typically very sophisticated and expensive. As well, the format of the feedback provided by these systems varies considerably. Typically, there are no exhaustive methods and systems to deliver biomechanical feedback across multiple platforms and entities consistently. For example, optical motion capture is currently used in motion capture studios and sports technology companies to analyze humans and objects. These motion capture laboratories and studios focus on biomechanical research of human subjects to analyze performance, recovery, and injury risk identifiers, amongst others. Many biomechanical researchers develop proprietary software and processing algorithms to extract kinematic and kinetic measures of human and object motion. The application of physics engines to extract these metrics are based on the precision and location of retro-reflective markers on bony anatomical landmarks of the body and objects. Feedback is usually given in highly technical formats that often leave the test-subject unaware of the true underlying biomechanical flaw and fail to address corrective exercises, drills, and training regimens to improve performance and recovery, and reduce the risk of injury. As well, when and if prescriptive feedback is given, there are few methods to assess compliance with such feedback and there are few systems available for appropriate re-evaluation of the human subject and object.</p>
      <p id="p-00003-en" num="0003">Accordingly, a need exists for a system that provides biomechanical feedback in a format which is usable by an actor/athlete to improve their performance and reduce their risk of injury and that is supported by statistical relationship models, interactive platforms, and more advanced hardware platforms.</p>
      <p id="p-00004-en" num="0004">Wearable technology is becoming a primary means of assessing human and object motion. This technology is based upon embedding sensor technology in wearable garments. An example of such use is the ability to extract data from one or more on miniaturized accelerometers and gyroscopes for the purpose of reconstructing three-dimensional motion, however, there are limitations to the precision and accuracy of wearable technology that often misleads users with information that is not supported by statistical models. Wearable technology also does not provide comprehensive motion detection of all body and object segments, and, thus, does not allow for extensive prescriptive feedback to improve performance and recovery, and, thereby, reduce the risk of injury. Wearable technology is often wirelessly interfaced with mobile devices to compute kinematics and kinetics, and to display biomechanical feedback in single sessions. That said, there is no cross-platform continuity to keep users engaged, and few methods to monitor compliance and deliver significant feedback.</p>
      <p id="p-00005-en" num="0005">Accordingly, a further need exists for a system that incorporates advanced physics engine capabilities, advanced hardware processing and sensor technology, and interactive interfaces to produce meaningful data usable by an actor/athlete to improve their performance and reduce their risk of injury.</p>
      <heading id="h-00003-en" level="1">SUMMARY OF THE INVENTION</heading>
      <p id="p-00006-en" num="0006">Disclosed is a method and system to deliver biomechanical feedback to subjects. The system utilizes three major elements: (1) multiple hardware data capture devices, including optical motion capture, inertial measurement units, infrared scanning devices, and two-dimensional RGB consecutive image capture devices, (2) a cross-platform compatible physics engine compatible with optical motion capture, inertial measurement units, infrared scanning devices, and two-dimensional RGB consecutive image capture devices, and (3) interactive platforms and user-interfaces to deliver real-time feedback to motions of humans and any objects in their possession and proximity.</p>
      <p id="p-00007-en" num="0007">The first element, a cross-platform compatible physics engine, imports data captured from a variety of hardware devices. Data imported from the hardware devices are then fed through kinematics and kinetics algorithms to extract desired biomechanical data. Data is then compiled by the physics engine across multiple subjects and across multiple sessions of a single subject. The physics engine also computes mechanical equivalencies to allow compatibility across hardware platforms. Mechanical equivalencies include correlating database data to injury and performance statistics, correlating discrete kinematics and kinetics of a body to discrete kinematics and kinetics of objects, principal component analysis to correlate time-series data of the body to objects, comparison of subject and object data to compiled databases, and extraction of prescriptive feedback based on all mechanical equivalencies</p>
      <p id="p-00008-en" num="0008">The second element, the hardware data capture devices, may comprise a variety of peripheral devices that collect two-dimensional or three-dimensional motion data of subject or object. In one embodiment, such peripheral devices may comprise (1) optical motion capture devices to collect three-dimensional positional data on reflective markers placed on the body and/or an object, (2) inertial measurement units with gyroscopes and accelerometers placed on the body, on an object, or in garments, to collect three-dimensional translation and rotation motion data of object(s) or segment(s) of the subjects body, (3) infrared scanning devices, such as the Microsoft Kinect or Google Tango, to capture three-dimensional point clouds of movement data and incorporate skeletal and object tracking algorithms to extract three-dimensional joint and object positions, and (4) Red Green Blue (RGB) two-dimensional consecutive image capture devices to collect movement images and to incorporate skeletal and object tracking algorithms to extract three-dimensional joint and object positions and kinematics.</p>
      <p id="p-00009-en" num="0009">The hardware data capture devices of the second element may further comprise wearable sensor technology which utilizes motion sensing hardware embedded in wearable garments to compute human and object motion. The wearable technology may be coupled the physics engine and interactive interface to offer prescriptive feedback to flaws identified in the subject's and object's motion. Such feedback may be based on an exhaustive body of object motion correlations made during biomechanical research, and be provided to the user via the interactive interface.</p>
      <p id="p-00010-en" num="0010">The third element, the interactive interface, utilizes information generated by the physics engine to create a user-interface that displays data to the user. This interactive interface may be implemented in variety of forms such as mobile device screens (iOS/Android), Web-Based interfaces (HTML), Heads-Up-Displays (VR/AR Headsets), and Windows/OSX static formats (spreadsheets, PDF's, etc.). The interactive interface has the capability of feeding information to the user, as well as the ability to collect information from the user for the purpose of monitoring compliance and monitoring usability of the interface. The interactive platform may be utilized to provide to the user insight gathered from biomechanical research and marker-based motion capture methods. Such interactive platforms will be able to offer prescriptive feedback based on motion data gathered from either the wearable technology, or the motion sensing mechanism built into the interactive platform and enable compliance monitoring and continuous engagement across a suite of wearable technology methods. Information may be stored on a back-end storage server and also exported to the physic engine and to one or more of the hardware devices.</p>
      <p id="p-00011-en" num="0011">According to one aspect of the disclosure, system for providing prescriptive feedback based on motion of a human subject or object comprises: A) a hardware engine for gathering data about the motion of a subject or object, B) a physics engine operatively coupled to the hardware engine and configured for processing the gathered motion data and correlating the processed motion data with previously stored biomechanical data representing idealized motion models, and C) an interactive interface operatively coupled to the data gathering devices and the physics engine and configured for providing prescriptive feedback on flaws identified in the subject's or object's motion.</p>
      <p id="p-00012-en" num="0012">According to another aspect of the disclosure, a method for providing prescriptive feedback based on motion of a human subject or object comprises: A) gathering data about the motion of a subject or object, B) processing the gathered motion data and correlating the processed motion data with previously stored biomechanical data representing idealized motion models, and C) providing prescriptive feedback on flaws identified in the subject's or object's motion. In one embodiment method further comprises D) enabling compliance monitoring of the motion of the subject and/or object.</p>
      <p id="p-00013-en" num="0013">According to yet another aspect of the disclosure, a method for offering prescriptive feedback based on motion of a human subject and/or objects comprises: A) gathering motion data from either a wearable garment or device having motion sensing hardware embedded therein or from a motion sensing mechanism built into the interactive platform, B) processing motion data, C) correlating the process motion data with previously stored biomechanical data representing idealized motion models, and D) providing prescriptive feedback on flaws identified in the subject's and object's motion based on the motion data gathered from either the wearable technology or interactive platform.</p>
    </summary>
    <description-of-drawings>
      <heading id="h-00004-en" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
      <p id="p-00014-en" num="0014">The present disclosure is illustratively shown and described with reference to the accompanying drawing in which:</p>
      <p id="p-00015-en" num="0015">
        <figref>FIG. 1</figref> illustrates conceptually a system for delivering biomechanical feedback to human and object motion;</p>
      <p id="p-00016-en" num="0016">
        <figref>FIG. 2</figref> illustrates conceptually a method of the system for delivering biomechanical feedback to human and object motion and the interaction of the physics engine;</p>
      <p id="p-00017-en" num="0017">
        <figref>FIG. 3</figref> illustrates conceptually a method of the system for delivering biomechanical feedback to human and object motion by use of an interactive engine;</p>
      <p id="p-00018-en" num="0018">
        <figref>FIGS. 4A-B</figref> illustrate conceptually a method of the system for delivering biomechanical feedback to human and object motion by use of hardware engines;</p>
      <p id="p-00019-en" num="0019">
        <figref>FIGS. 5A-D</figref> illustrate conceptually specific marker sets used in the method of biomechanical research and its relation to optical motion capture;</p>
      <p id="p-00020-en" num="0020">
        <figref>FIGS. 6A-H</figref> illustrate conceptually embodiments of wearable technology in accordance with the disclosure;</p>
      <p id="p-00021-en" num="0021">
        <figref>FIG. 7</figref> illustrates conceptually a diagram of an exemplary computer architecture in accordance with the present disclosure; and</p>
      <p id="p-00022-en" num="0022">
        <figref>FIG. 8</figref> illustrates conceptually a diagram of an exemplary network topology in which the system may be implemented in accordance with the present disclosure.</p>
    </description-of-drawings>
    <detailed-desc>
      <heading id="h-00005-en" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
      <p id="p-00023-en" num="0023">The present disclosure will be more completely understood through the following description, which should be read in conjunction with the drawings. In this description, like numbers refer to similar elements within various embodiments of the present disclosure. The skilled artisan will readily appreciate that the methods, apparatus and systems described herein are merely exemplary and that variations can be made without departing from the spirit and scope of the disclosure.</p>
      <p id="p-00024-en" num="0024">Is used herein, the term âengineâ means one or more elements, whether implemented in hardware, software, firmware, or any combination thereof, capable of performing any function described herein, including a collection of such elements which collaboratively perform functions wholly or partially, serially or in parallel, synchronously or asynchronously, remotely or locally, regardless of data formats or protocols.</p>
      <p id="p-00025-en" num="0025">Referring to <figref>FIG. 1</figref>, a system for providing biomechanical feedback comprises a physics engine <b>100</b>, a hardware engine <b>102</b>, and an interactive engine <b>101</b> to deliver biomechanical feedback on human or object motion. As used herein, hardware engine <b>102</b> may comprise one or a plurality of hardware/software devices which function collectively or individually to achieve the described functionality. Similarly, as used herein, interactive engine <b>101</b> may comprise one or a plurality of hardware/software devices which function collectively or individually to achieve the described functionality, typically receiving data from or providing data to a subject.</p>
      <p id="p-00026-en" num="0026">Physics engine <b>100</b> functions as a cross-platform compatible method to transform raw three-dimensional data captured from various hardware data collection devices into usable biomechanical feedback. <figref>FIG. 2</figref> illustrates conceptually an exemplary process flow of the physics engine <b>100</b>. First, the physics engine <b>100</b> is initialized, typically to a startup state with default parameters, as illustrated by process block <b>200</b>. The physics engine <b>100</b> imports, either in a push or a pull manner over any network infrastructure, wireless or otherwise, either two-dimensional or three-dimensional datasets from the hardware engine <b>102</b>, as illustrated by process block <b>201</b>. In one embodiment, hardware engine <b>102</b> may comprise individually or collectively any of optical motion capture device <b>212</b>, inertial measurement units <b>213</b>, infrared from point scanning devices <b>214</b>, and image capture devices <b>215</b>. Data imported from these devices is provided to physics engine <b>100</b> which performs one or more algorithmic processes to compute kinematics and kinetics of the human body and objects, as illustrated by process block <b>202</b>.</p>
      <p id="p-00027-en" num="0027">An pseudocode example of the type of kinematic and kinetic algorithmic computations performed by physics engine <b>11</b> on data from optical motion capture device <b>212</b>, infrared scanning devices <b>214</b> and image and video capture devices <b>215</b>, is as follows: <ul id="ul-00001-en" list-style="none"><li><ul id="ul-00002-en" list-style="none"><li>Define a multisegment or single segment biomechanics model comprising bilateral and/or unilateral segments, such as the hands, forearms, upper arms, upper trunk, neck, head, pelvis, upper thighs, lower shanks, feet, and any objects used in the proximity</li><li>Define long-axis unit vectors of the body or object segments in the model using three-dimensional XYZ marker data</li><li>Define Planar-axis unit vectors of each of the above segments</li><li>Compute a cross product of the long and planar axes to determine the third unit vector of each segment</li><li>Compute the cross product of the third unit-vector and the long-axis unit vector to create three orthonormal unit vectors for each segment in the model</li><li>Compute angular velocity vectors of each model segment using a derivative method of unit vectors of each segment</li><li>Compute relative joint angles (in euler and polar coordinate systems) using each segment's set of unit vectors</li><li>Define human mass model using subject weight and height and anthropometrics of body segments to calculate mass moments of inertia about each segment, and center of mass locations</li><li>Compute accelerations of each marker and of each segment center of mass using a five-point central difference filter</li><li>Compute angular accelerations of each model segment using a five point central difference filter of angular velocity</li><li>Compute an inverse dynamics model of motion using angular velocity components of each segment, angular acceleration components of each segment, and linear acceleration components of each segment's center of mass</li><li>Event detection algorithms are used to determine points of interest during a particular motion</li><li>Kinematics and Kinetic values are extracted at points of interest (examples of kinematics and kinetic values are peak angular velocity of the pelvis segment during a sport motion (in meters per second), or shoulder rotation at foot contact during a throwing motion (in degrees) <br /> To compute kinematics and kinetics from inertial measurement units <b>213</b>, the following steps are undertaken: </li><li>Raw sensor data is fused together using an axis-angle integration method.         <ul id="ul-00003-en" list-style="none"><li>A rotation matrix of the sensor is initialized using the gravity vector during a still point (Gravity can also be detected when there is no still point by measuring integration offset after the axis-angle method is completed)</li><li>Angular rate data from the gyroscope is integrated into a rotation matrix at each sample during the motion</li><li>Each rotation matrix from each sample is multiplied consecutively to the initialized rotation matrix in a body-fixed method</li><li>Acceleration data is transformed into the global frame using the rotation matrix computed from the angular rate data</li><li>Gravity is subtracted from the rotated acceleration data in the global reference frame</li><li>Rotated acceleration data is integrated into velocity and position of the sensor and/or any point on a rigid body to which the sensor is attached</li></ul></li><li>Raw acceleration and angular rate data is transformed into the inertial reference frame at the center of mass of the segment or object to which the sensor is attached</li><li>All processed data is rotated by an offset error that may exist in a system</li><li>All rotated and inertial data is filtered using a fourth order low pass filter</li><li>Inertial reference frame acceleration data and angular rate data are fed into an open-chain inverse dynamics kinetic model to solve for reaction forces and torques about vertices within a human or object mass model defined by anthropometrics, height, weight, or lookup tables of object properties</li><li>Event detection algorithms are used to determine points of interest during a particular motion (An example of an event detection algorithm in baseball pitching encompases the generation of Principal Components of a historical dataset (training data). When a given buffer of data in a hardware device meets the criteria of fitting the principal components, an event (such as a pitch) is detected. Event detection is used to prevent undesired motion from being detected in a given application).</li><li>Kinematics and Kinetic values are extracted at points of interest from integrated positions, velocities, accelerations, angular rates, and reaction force and torque data.</li></ul></li></ul></p>
      <p id="p-00028-en" num="0054">The above identified process for the computation of kinematics and kinetics data are repeated for multiple trials of a subject's or object's motion and compiled in a memory or database, as illustrated by process block <b>203</b>. In one embodiment, similar computed kinematics and kinetics data are compiled across multiple subjects and stored for further analysis, as also illustrated by process block <b>203</b>. Physics engine <b>100</b> then provides relevant portions of the compiled kinematics and kinetics data to mechanical equivalency models, as illustrated by process block <b>204</b>. As part of this process, physics engine <b>100</b> correlates the compiled data to performance and injury data, as illustrated by process block <b>300</b>. Discrete data are correlated to performance and injury data by way of person correlations and Bayesian mixed regression to eliminate non-contributing factors. Physics engine <b>100</b> further correlates discrete kinematics and kinetics data of the body to discrete kinematics and kinetics of the object or other body segments, as illustrated by process block <b>302</b>. Discrete body data are correlated other discrete body and object data <b>302</b> in a similar method to <b>300</b> by way of person correlations and Bayesian mixed regression. Also as part of this process, physics engine <b>100</b> correlates time-series data of the body to time-series data of other body parts or objects using principal component analysis, as illustrated by process block <b>303</b>. Time-series segment data are reduced to multiple, e.g. over 80, principal components. The reduced components are then correlated to principal components of other body or object segments and are used to extract additional motion given raw data from one or more body segments. Discrete and time-series data are compared to a database of motion, as illustrated by process block <b>305</b>, by use of averages and standard deviations of data across multiple subjects. Based upon relationships established during process blocks <b>300</b>, <b>302</b>, <b>303</b>, <b>305</b>, physics engine <b>100</b> further extracts prescriptive feedback using correlation models and the subject's/object's motion, as illustrated by process block <b>304</b>. After the prescriptive feedback information is gathered by the physics engine <b>100</b>, the prescriptive feedback is exported to interactive engine <b>101</b>, as illustrated by process block <b>205</b>, and presented to the subject/object in a matter that facilitates altering the mechanics in a way that leads to increased performance or reduced risk of injury.</p>
      <p id="p-00029-en" num="0055">
        <figref>FIG. 3</figref> illustrates the process flow performed by the interactive engine <b>101</b>. After any initialization of the interactive engine <b>101</b>, information is acquired from the physics engine <b>100</b>, as illustrated by process block <b>206</b>, either manually, or with automated API's in either a push or pull protocol. Such information is then stored on back end servers and databases, as illustrated by process block <b>207</b>, in a logical manner for reference. Next, the information acquired from physics engine <b>100</b> is displayed through user interfaces, as illustrated by process block <b>208</b>. Interactive engine <b>101</b> may display the information acquired from physics engine <b>100</b> through any of mobile devices (iOS/Android) <b>306</b>, spreadsheets <b>307</b>, web-based interfaces (HTML) <b>308</b>, and virtual and/or augmented reality heads-up displays <b>309</b>, as illustrated by process block <b>208</b>. After such information is displayed, the interactive engine <b>101</b> has the further capability to monitor compliance and usability information through the various interface components of interactive engine <b>101</b>, as illustrated by process block <b>209</b>, by storing compliance information entered by a user or collected by one of the data collection devices comprising hardware engine <b>102</b>. Any acquired compliance information may be exported to either the hardware engine <b>102</b> and/or physics engine <b>100</b>, as applicable, to form a continuous feedback loop, as illustrated by process block <b>210</b>, and as illustrated in <figref>FIG. 1</figref>.</p>
      <p id="p-00030-en" num="0056">
        <figref>FIGS. 4A-B</figref> illustrate the process flow performed by the hardware engine <b>102</b>. For the optical motion capture devices <b>212</b>, third party software is utilized to calibrate multiple, e.g. up to 16, near infrared cameras around a centralized volume space, as illustrated by process block <b>310</b>. Retro-reflective markers are then placed on the subject and/or object as seen in <figref>FIGS. 5A-D</figref>. Data are recorded with devices <b>212</b> and transmitted using an Ethernet bus connected to a computer. The calibrated XYZ positions of each marker in the capture volume are recorded into a binary encoded file, as illustrated by process block <b>312</b>. After collection, the acquired data are exported to the physics engine <b>102</b>, for example using Windows/OSX devices, for processing, as illustrated by process block <b>313</b>. For the inertial measurement devices <b>213</b>, accelerometers and gyroscopes are calibrated and interfaced with a microcontroller capable of interfacing with any of memory, e.g. RAM or FLASH, and Bluetooth chipsets by internal firmware operations that control the hardware, as illustrated by process block <b>314</b>. Inertial measurement units are placed on points of interest, such as body segments or objects, as illustrated by process block <b>315</b>. Six axes of data, in addition to time samples, may be recorded continuously. Upon the trigger of a customized event (event detection), as illustrated by process block <b>316</b>, data are stored in RAM and/or FLASH memory for later analysis, as illustrated by process block <b>317</b>. Data are simultaneously compressed using principal component analysis, similar as previously described with reference to process block <b>303</b>, as illustrated by process block <b>318</b>. Data are then exported to a physics engine <b>100</b>, with, for example, Bluetooth low energy SDK processes on mobile devices (iOS/Android), as illustrated by process block <b>319</b>. For the infrared scanning devices <b>214</b>, a scanning laser is calibrated alongside on-board inertial measurement units that may include accelerometers and gyroscopes, or other positional measurement units, such as Red Green Blue (RGB) video or known environmental conditions, as illustrated by process block <b>320</b>. XYZ point cloud data of human and object motion is then captured, as illustrated by process block <b>321</b>, and stored to local memory. Simultaneously, a local skeletal and object tracking algorithm is implemented to track three-dimensional joint and object positions, as illustrated by process block <b>322</b>. The local skeletal and object tracking algorithm may be implemented using known anthropometrics of the human, or known information from the object, alongside edge filtering techniques to identify points of interest on a human or object. Skeletal and object data are then also stored in memory, as illustrated by process block <b>323</b>, and are exported to a physics engine <b>100</b> by, for example, Android/iOS devices. For the image and video capture devices <b>215</b>, consecutive two-dimensional RGB images and videos of human object motion are captured, as illustrated by process block <b>325</b>. A local skeleton and object tracking algorithm is then used to extract joint and object positions similar as previously described with reference to process block to <b>322</b>, as illustrated by process block <b>326</b>. Skeletal and object positional data are then stored to memory, as illustrated by process block <b>327</b>, and are exported to physics engine <b>100</b>, by for example, mobile devices (IDS/Android), as illustrated by process block <b>328</b>.</p>
      <p id="p-00031-en" num="0057">As seen in <figref>FIG. 5A-D</figref>, markerset <b>300</b> comprise a full body markerset <b>400</b> and various object markersets, including baseball/softball bat markerset <b>401</b>, golf clubs markerset <b>402</b>, tennis racket markerset <b>403</b>, and other objects that interact with a human body. <figref>FIG. 5A</figref> illustrates conceptually a full body markerset <b>400</b> comprises markers attached to bony anatomical landmarks, including, but not limited to: front of the head, back of the head, right side of the head, left side of the head, the T2 vertebrate, the T8 vertebrate, the Xiphoid process of the sternum, the left and right sternoclavicular joints, the Acromioclavicular joint of both shoulders, the middle of the bicep of the upper arms, the lateral and medial epicondyle of both humeruses, middle of the forearms, the medial and lateral styloid processes of both wrists, the tip of the third metacarpal in both hands, the right and left posterior superior iliac spines, the right and left anterior superior iliac spines, the right and left greater trochanters, the middle of both thighs, the medial and lateral epicondyles of the femurs, the right calf, the medial and lateral malleouleses of the ankles, the tip of the third metatarsal of both feet, and the tip of the calcaneous in both feet.</p>
      <p id="p-00032-en" num="0058">
        <figref>FIG. 5B</figref> illustrates conceptually a markerset <b>401</b> with five markers attached to a hypothetical baseball bat in the following locations: knob of bat, above the handle of the bat, at the center of mass of the bat, at the tip of the bat forming a line between the knob and center of mass, and on the posterior side of the bat at the tip.</p>
      <p id="p-00033-en" num="0059">
        <figref>FIG. 5C</figref> illustrates conceptually a markerset <b>402</b> with five markers attached to a hypothetical golf club at the following locations: at the base of the handle, above the handle, at the center of mass, at the base of the club, and at the tip of the clubface to form a direct line of the clubface angle.</p>
      <p id="p-00034-en" num="0060">
        <figref>FIG. 5D</figref> illustrates conceptually a markerset <b>403</b> with five markers are placed on a hypothetical tennis racket at the following locations: at the base of the handle, at the intersection of the handle and the racket head, on the left and right sides of the racket head, and at the tip of the racket head.</p>
      <p id="p-00035-en" num="0061">Referring to <figref>FIGS. 6A-H</figref>, wearable technology <b>102</b>, they comprise in embodiments any of a plurality of garments <b>800</b>, which may be made of any material including spandex, Lycra, polyester, in combination with cotton, silk or any other wearable material, to assist with the acquisition of motion data from human subjects and/or accompanying objects. Various embodiments of such garments are described in greater detail below. Each garment <b>800</b> comprises multiple data-gathering mechanisms <b>801</b>, such as MEMS/IMU hardware, which can which can record and transmit motion data wirelessly to an iOS/Android phone or other network enabled device <b>802</b>. Software applications, such as those available from Diamond Kinetics, Pittsburgh, Pa. may be utilized to compute kinematics from MEMS/IMU hardware.</p>
      <p id="p-00036-en" num="0062">In one embodiment, the mThrow sleeve garment <b>803</b>, as illustrated in <figref>FIG. 6A</figref>, is coupled with motion sensors <b>801</b> to monitor workload and performance. The mThrow sleeve garment <b>803</b> is worn on the throwing arm during practices, games, or rehab sessions. Data may be collected throughout workouts and is transmitted wirelessly to an iOS/Android phone or other network enabled device. In such embodiment, garment <b>803</b> may comprise <b>801</b> a six axis accelerometer/gyroscope IMU that interfaces wirelessly via Bluetooth or other protocol with a mobile device <b>802</b>, such as an iOS/Android smartphone. Software applications, either executing on the network enabled device or remotely accessible via a network, perform the functions to interact with the physics engine <b>100</b> and interactive engine <b>101</b>. Such existing functions include computation of throwing forearm motion, kinetic solutions of elbow reaction forces and torques, algorithmic calculation of throwing workload during game/practice/season/career, pitch and throw counting functions, computation of arm and ball speed, and trends for the use of performance and injury forecasting. Principal component analysis of sensor data will also allow for mechanical equivalence models to be created that extract additional segment data that is not directly measured by the sensors.</p>
      <p id="p-00037-en" num="0063">In another embodiment, the mThrow Pro garment <b>804</b>, as illustrated in <figref>FIG. 6B</figref>, is a comprehensive throwing analysis platform designed to allow athletes of all ages and skill level to receive a full-body biomechanical analysis of their throwing mechanics. Data is gathered with four motion sensors. mThrow Pro garment <b>804</b> can be worn during games or practices, and is coupled with interactive applications to provide useful feedback. In such embodiment, garment <b>804</b> may comprise <b>801</b> multiple six axis accelerometer/gyroscope IMU's that interfaces wirelessly via Bluetooth or other protocol with a mobile device <b>802</b>, such as an iOS/Android smartphone. Software applications, either executing on the network enabled device or remotely accessible via a network, perform the functions to interact with the physics engine <b>100</b> and interactive engine <b>101</b>. Such existing functions include computation of throwing forearm, upper arm, pelvis, and upper trunk biomechanical motion, kinetic solution of elbow and shoulder reaction forces and torques, algorithmic calculation of throwing workload during game/practice/season/career, pitch and throw counting functions, computation of ball and arm speed, computation of kinetic chain velocities and accelerations of the pelvis, upper trunk, upper arm, and forearm and their associated temporal reference, and trends for the use of performance and injury forecasting. Principal component analysis of sensor data will also allow for mechanical equivalence models to be created that extract additional segment data that is not directly measured by the sensors.</p>
      <p id="p-00038-en" num="0064">In another embodiment, the mRun ACL Sleeve garment <b>806</b>, as illustrated in <figref>FIG. 6C</figref>, is coupled with motion sensors <b>801</b> and EMG muscle activity sensors <b>805</b> to detect injury risk factors and workload in athletic movements such as running, sprinting, cutting, and jumping. The mRun ACL Sleeve garment <b>806</b> can be used on the field of play during games, practice, or training. The mRun ACL Sleeve garment <b>806</b> may comprise <b>801</b> a six axis accelerometer/gyroscope IMU and <b>805</b> two EMG sensors on the quadriceps and hamstrings that interfaces wirelessly via Bluetooth or other protocol with a mobile device <b>802</b>, such as an iOS/Android smartphone. Software applications, either executing on the network enabled device or remotely accessible via a network, perform the functions to interact with the physics engine <b>100</b> and interactive engine <b>101</b>. Such existing functions include computation of lower extremity kinematics, muscle firing activity, and algorithmic computation of movement workload during game/practice/season/career, and trends for the use of performance and injury forecasting. Principal component analysis of sensor data will also allow for mechanical equivalence models to be created that extract additional segment data that is not directly measured by the sensors.</p>
      <p id="p-00039-en" num="0065">In another embodiment, the mRun ACL Leggings Pro garment <b>807</b>, as illustrated in <figref>FIG. 6D</figref>, give athletes in any sport the ability to reduce their risk of injury and enhance performance. Garment <b>807</b> is a comprehensive movement analysis platform that captures ACL injury risk. The mRun ACL Leggings Pro garment <b>807</b> uses motion sensors <b>801</b> and muscle activity sensors <b>805</b> to monitor the entire lower body during any movement. mRun ACL Leggings Pro garment <b>808</b> may be used during any training platform to allow athletes to undergo an ACL risk screening. In such embodiment, garment <b>808</b> may comprise <b>801</b> two six axis accelerometer/gyroscope IMU's and <b>805</b> six EMG sensors on the quadriceps, hamstrings, and gluteus medius that interfaces wirelessly via Bluetooth or other protocol with a mobile device <b>802</b>, such as an iOS/Android smartphone. Software applications, either executing on the network enabled device or remotely accessible via a network, perform the functions to interact with the physics engine <b>100</b> and interactive engine <b>101</b>. Such existing functions include computation of full lower extremity kinematics and kinetics, computation of full lower extremity muscle firing activity, algorithmic computation of movement workload during game/practice/season/career, and the use of performance and injury forecasting. Principal component analysis of sensor data will also allow for mechanical equivalence models to be created that extract additional segment data that is not directly measured by the sensors.</p>
      <p id="p-00040-en" num="0066">In another embodiment, the SmartSocks garment <b>809</b>, as illustrated in <figref>FIG. 6E</figref>, can be used in any sport to monitor feet movement, running patterns, and weight distribution. Coupled with force sensor arrays and motion sensors, the SmartSocks garment <b>810</b> can interface with any of the garments disclosed herein to provide a more comprehensive experience. In such embodiment, garment <b>810</b> may comprise <b>801</b> a six axis accelerometer/gyroscope IMU that interfaces wirelessly via Bluetooth or other protocol with a mobile device <b>802</b>, such as an iOS/Android smartphone. Software applications, either executing on the network enabled device or remotely accessible via a network, perform the functions to interact with the physics engine <b>100</b> and interactive engine <b>101</b>. Additional sensor technology is to include <b>810</b> cardiac sensor for heart rate and respiration rate measurement, <b>811</b> blood oxygenation sensor, <b>812</b> sweat and hydration sensors, <b>813</b> global positioning system sensors for on field movement monitoring, and <b>814</b> force sensor arrays for weight distribution and lower extremity kinetic computations. Together, these elements will allow for in depth analysis of player workload during practice and games, while offering performance alerts in the form of dashboards for forecasting performance and injury. The embodiment <b>809</b> will also have the capability to pair sensor data with <b>800</b><b>804</b><b>806</b><b>807</b>.</p>
      <p id="p-00041-en" num="0067">In another embodiment, the mSense device <b>801</b>, as illustrated in <figref>FIG. 6F</figref>, comprises <b>801</b> motion sensors, <b>810</b> cardiac heart and breathing monitors, <b>811</b> oxygen sensors, <b>812</b> sweat and hydration sensors, and <b>813</b> GPS to monitor team members during gameplay, practice, and training sessions. The mSense device <b>801</b> is designed for the competitive individual as well as for coaches to monitor their entire team and embeds in on to any garment and interfaces wirelessly with your phone <b>802</b> with a user interface in the form of dashboards to monitor player workload. In such embodiment, device <b>801</b> may comprise of a six axis accelerometer/gyroscope IMU that interfaces wirelessly via Bluetooth or other protocol with <b>802</b> a mobile device, such as an iOS/Android smartphone. Software applications, either executing on the network enabled device or remotely accessible via a network, perform the functions to interact with the physics engine <b>100</b> and interactive engine <b>101</b>.</p>
      <p id="p-00042-en" num="0068">In another embodiment, the mSwing glove garment <b>816</b>, as illustrated in <figref>FIG. 6G</figref>, is coupled with motion sensors <b>801</b> to monitor workload and performance of swinging motions such as but not limited to golf swings, baseball batting swings, softball batting swings, tennis swings, and lacrosse swings. The mSwing glove garment <b>816</b> is worn on the dominant and/or non-dominant arm during practices, games, or rehab sessions. Data may be collected throughout workouts and is transmitted wirelessly to an iOS/Android phone or other network enabled device. In such embodiment, garment <b>816</b> may comprise <b>801</b> a six axis accelerometer/gyroscope IMU that interfaces wirelessly via Bluetooth or other protocol with a mobile device <b>802</b>, such as an iOS/Android smartphone. Software applications, either executing on the network enabled device or remotely accessible via a network, perform the functions to interact with the physics engine <b>100</b> and interactive engine <b>101</b>. Such existing functions include computation of held object motion, kinetic solutions of wrist reaction forces and torques, algorithmic calculation of swinging workload during game/practice/season/career, swing counting functions, computation of held object speed, kinematics and temporal measures, and trends for the use of performance and injury forecasting. Principal component analysis of sensor data will also allow for mechanical equivalence models to be created that extract additional segment data that is not directly measured by the sensors.</p>
      <p id="p-00043-en" num="0069">In another embodiment, the mBand wristband garment <b>817</b>, as illustrated in <figref>FIG. 6H</figref>, is coupled with motion sensors <b>801</b> to monitor workload and performance of swinging motions such as but not limited to golf swings, baseball batting swings, softball batting swings, tennis swings, and lacrosse swings. The mBand wristband garment <b>817</b> is worn on the dominant and/or non-dominant arm during practices, games, or rehab sessions. Data may be collected throughout workouts and is transmitted wirelessly to an iOS/Android phone or other network enabled device. In such embodiment, garment <b>817</b> may comprise <b>801</b> a six axis accelerometer/gyroscope IMU that interfaces wirelessly via Bluetooth or other protocol with a mobile device <b>802</b>, such as an iOS/Android smartphone. Software applications, either executing on the network enabled device or remotely accessible via a network, perform the functions to interact with the physics engine <b>100</b> and interactive engine <b>101</b>. Such existing functions include computation of held object motion, kinetic solutions of wrist reaction forces and torques, algorithmic calculation of swinging workload during game/practice/season/career, swing counting functions, computation of held object speed, kinematics and temporal measures, and trends for the use of performance and injury forecasting. Principal component analysis of sensor data will also allow for mechanical equivalence models to be created that extract additional segment data that is not directly measured by the sensors.</p>
      <p id="p-00044-en" num="0070">The previously described physics engine <b>100</b> and the processes or functions performed thereby may be implemented with computer program code executing under the control of an operating system and running on one or more primary hardware platforms as described with reference to <figref>FIG. 7</figref>. Referring to <figref>FIG. 7</figref>, a computer system <b>500</b> comprises a central processing unit <b>502</b> (CPU), a system memory <b>530</b>, including one or both of a random access memory <b>532</b> (RAM) and a read-only memory <b>534</b> (ROM), and a system bus <b>510</b> that couples the system memory <b>530</b> to the CPU <b>502</b>. An input/output system containing the basic routines that help to transfer information between elements within the computer architecture <b>500</b>, such as during startup, can be stored in the ROM <b>534</b>. The computer architecture <b>500</b> may further include a mass storage device <b>520</b> for storing an operating system <b>522</b>, software, data, and various program modules <b>600</b>, associated with an application <b>580</b> which may include functionality described herein with reference to any of physics engine <b>100</b>, hardware engine <b>102</b> or interactive engine <b>100</b>, one or any combination thereof which are executable by a special-purpose application, such as analytics engine <b>524</b>.</p>
      <p id="p-00045-en" num="0071">The mass storage device <b>520</b> may be connected to the CPU <b>502</b> through a mass storage controller (not illustrated) connected to the bus <b>510</b>. The mass storage device <b>520</b> and its associated computer-readable media can provide non-volatile storage for the computer architecture <b>500</b>. Although the description of computer-readable media contained herein refers to a mass storage device, such as a hard disk or CD-ROM drive, it should be appreciated by those skilled in the art that computer-readable media can be any available computer storage media that can be accessed by the computer architecture <b>500</b>.</p>
      <p id="p-00046-en" num="0072">By way of example, and not limitation, computer-readable media may include volatile and non-volatile, removable and non-removable media implemented in any method or technology for the non-transitory storage of information such as computer-readable instructions, data structures, program modules or other data. For example, computer-readable media includes, but is not limited to, RAM, ROM, EPROM, EEPROM, flash memory or other solid state memory technology, CD-ROM, digital versatile disks (DVD), HD-DVD, BLU-RAY, or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by the computer architecture <b>500</b>.</p>
      <p id="p-00047-en" num="0073">According to various embodiments, the computer architecture <b>500</b> may operate in a networked environment using logical connections to remote physical or virtual entities through a network such as the network <b>599</b>. The computer architecture <b>500</b> may connect to the network <b>599</b> through a network interface unit <b>504</b> connected to the bus <b>510</b>. It will be appreciated that the network interface unit <b>504</b> may also be utilized to connect to other types of networks and remote computer systems. The computer architecture <b>500</b> may also include an input/output controller for receiving and processing input from a number of other devices, including a keyboard, mouse, or electronic stylus (not illustrated). Similarly, an input/output controller may provide output to a video display <b>506</b>, a printer, or other type of output device. A graphics processor unit <b>525</b> may also be connected to the bus <b>510</b>.</p>
      <p id="p-00048-en" num="0074">As mentioned briefly above, a number of program modules and data files may be stored in the mass storage device <b>520</b> and RAM <b>532</b> of the computer architecture <b>500</b>, including an operating system <b>522</b> suitable for controlling the operation of a networked desktop, laptop, server computer, or other computing environment. The mass storage device <b>520</b>, ROM <b>534</b>, and RAM <b>532</b> may also store one or more program modules. In particular, the mass storage device <b>520</b>, the ROM <b>534</b>, and the RAM <b>532</b> may store the engine <b>524</b> for execution by the CPU <b>502</b>. The engine <b>524</b> can include software components for implementing portions of the processes described herein. The mass storage device <b>520</b>, the ROM <b>534</b>, and the RAM <b>532</b> may also store other types of program modules.</p>
      <p id="p-00049-en" num="0075">Software modules, such as the various modules within the engine <b>524</b> may be associated with the system memory <b>530</b>, the mass storage device <b>520</b>, or otherwise. According to embodiments, the analytics engine <b>524</b> may be stored on the network <b>599</b> and executed by any computer within the network <b>599</b>. Databases <b>572</b> and <b>575</b>, which may be used to store any of the acquired or processed data or idealized models, and/or kinematics and kinetic data described herein, such databases being coupled remotely to network <b>599</b> and network interface <b>504</b>.</p>
      <p id="p-00050-en" num="0076">The software modules may include software instructions that, when loaded into the CPU <b>502</b> and executed, transform a general-purpose computing system into a special-purpose computing system customized to facilitate all, or part of, the techniques disclosed herein. As detailed throughout this description, the program modules may provide various tools or techniques by which the computer architecture <b>500</b> may participate within the overall systems or operating environments using the components, logic flows, and/or data structures discussed herein.</p>
      <p id="p-00051-en" num="0077">The CPU <b>502</b> may be constructed from any number of transistors or other circuit elements, which may individually or collectively assume any number of states. More specifically, the CPU <b>502</b> may operate as a state machine or finite-state machine. Such a machine may be transformed to a second machine, or specific machine by loading executable instructions contained within the program modules. These computer-executable instructions may transform the CPU <b>502</b> by specifying how the CPU <b>502</b> transitions between states, thereby transforming the transistors or other circuit elements constituting the CPU <b>502</b> from a first machine to a second machine, wherein the second machine may be specifically configured to manage the generation of indices. The states of either machine may also be transformed by receiving input from one or more user input devices associated with the input/output controller, the network interface unit <b>504</b>, other peripherals, other interfaces, or one or more users or other actors. Either machine may also transform states, or various physical characteristics of various output devices such as printers, speakers, video displays, or otherwise.</p>
      <p id="p-00052-en" num="0078">Encoding of executable computer program code modules may also transform the physical structure of the storage media. The specific transformation of physical structure may depend on various factors, in different implementations of this description. Examples of such factors may include, but are not limited to: the technology used to implement the storage media, whether the storage media are characterized as primary or secondary storage, and the like. For example, if the storage media are implemented as semiconductor-based memory, the program modules may transform the physical state of the system memory <b>530</b> when the software is encoded therein. For example, the software may transform the state of transistors, capacitors, or other discrete circuit elements constituting the system memory <b>530</b>.</p>
      <p id="p-00053-en" num="0079">As another example, the storage media may be implemented using magnetic or optical technology. In such implementations, the program modules may transform the physical state of magnetic or optical media, when the software is encoded therein. These transformations may include altering the magnetic characteristics of particular locations within given magnetic media. These transformations may also include altering the physical features or characteristics of particular locations within given optical media, to change the optical characteristics of those locations. It should be appreciated that various other transformations of physical media are possible without departing from the scope of and spirit of the present description.</p>
      <p id="p-00054-en" num="0080">
        <figref>FIG. 8</figref> illustrates conceptually a network topology in which the components of the disclosed system may be implemented. Any of the engines <b>100</b>, <b>102</b> or <b>103</b> and the data processing systems <b>110</b>, <b>113</b>A-C, <b>112</b>-<b>118</b>, illustrated in the network topology of <figref>FIG. 8</figref> may be implemented with a processing architecture similar to that illustrated in <figref>FIG. 7</figref>, including any of a desktop computer, laptop computer, tablet computer or smart phone/personal digital assistant device such as an iPhone or android operating system based device. Note that any of the systems illustrated in <figref>FIG. 8</figref> may be interoperably connected either through a wide area network (WAN) <b>125</b> or local area network (LAN) <b>115</b> or both, or any hybrid combination thereof using known network infrastructure and components, protocols, and/or topologies. For example, any of devices <b>212</b>-<b>215</b> may be connected to hardware engine <b>102</b> either through a physical network physically or wirelessly, or any combination thereof. Similarly any of devices <b>306</b>-<b>309</b> may be coupled to interface engine <b>103</b> either through a physical network or wirelessly, or any combination thereof. Similarly any of engines <b>100</b>, <b>102</b>, <b>103</b> may be connected either via WAN <b>125</b> or LAN <b>115</b>, any portion of which may be wireless connection between respective network nodes, the entirety of the system being indicated as system <b>110</b>. Systems <b>112</b>-<b>118</b> may comprise additional databases more users capable of interacting remotely with system <b>110</b>.</p>
      <p id="p-00055-en" num="0081">The reader can appreciate that the various systems and elements and methods described herein enables delivery of near real-time feedback to motions of human subjects and any objects in their possession and proximity.</p>
      <p id="p-00056-en" num="0082">Although the various embodiments of the system and techniques disclosed herein have been described with reference to specific sports and/or garments related to such activities, it will be obvious to those reasonably skilled in the art that modifications to the systems and processes disclosed herein may occur, without departing from the true spirit and scope of the disclosure. For example, any type of wearable garment which is capable of transmitting motion data useful for analysis may be utilized with the systems and techniques described herein. Further, notwithstanding the network implementation described, any existing or future network or communications infrastructure technologies may be utilized, including any combination of public and private networks. In addition, although specific algorithmic flow diagrams or data structures may have been illustrated, these are for exemplary purposes only, other processes which achieve the same functions or utilized different data structures or formats are contemplated to be within the scope of the concepts described herein. As such, the exemplary embodiments described herein are for illustrative purposes and are not meant to be limiting</p>
    </detailed-desc>
  </description>
  <us-claim-statement>What is claimed is:</us-claim-statement>
  <claims id="claims_eng" lang="eng" format="original" date-changed="20150917">
    <claim num="1" id="clm-00001-en" independent="true">
      <claim-text>
        <b>1</b>. A system for providing prescriptive feedback based on motion of a human subject or object comprising:
<claim-text>A) a hardware engine for gathering data about the motion of a subject or object,</claim-text><claim-text>B) a physics engine operatively coupled to the hardware engine and configured for processing the gathered motion data and correlating the processed motion data with previously stored biomechanical data representing idealized motion models, and</claim-text><claim-text>C) an interactive interface operatively coupled to the data gathering devices and the physics engine and configured for providing prescriptive feedback on flaws identified in the subject's or object's motion.</claim-text></claim-text>
    </claim>
    <claim num="2" id="clm-00002-en">
      <claim-text>
        <b>2</b>. The system of <claim-ref idref="clm-00002-en">claim 2</claim-ref> wherein the interactive interface is further configured for enabling compliance monitoring of the subject's or object's motion.</claim-text>
    </claim>
    <claim num="3" id="clm-00003-en">
      <claim-text>
        <b>3</b>. The system of <claim-ref idref="clm-00001-en">claim 1</claim-ref> wherein the hardware engine comprises a plurality of data gathering devices configured for gathering data about the motion of a subject or object.</claim-text>
    </claim>
    <claim num="4" id="clm-00004-en">
      <claim-text>
        <b>4</b>. The system of <claim-ref idref="clm-00003-en">claim 3</claim-ref> wherein at least one of the plurality of data gathering devices comprises an optical motion capture device to collect three-dimensional positional data from reflective markers placed on the subject or object.</claim-text>
    </claim>
    <claim num="5" id="clm-00005-en">
      <claim-text>
        <b>5</b>. The system of <claim-ref idref="clm-00003-en">claim 3</claim-ref> wherein at least one of the plurality of data gathering devices comprises an inertial measurement unit disposed on the subject or object to collect three-dimensional translation and rotation motion data.</claim-text>
    </claim>
    <claim num="6" id="clm-00006-en">
      <claim-text>
        <b>6</b>. The system of <claim-ref idref="clm-00003-en">claim 3</claim-ref> wherein at least one of the plurality of data gathering devices comprises an infrared scanning device configured to capture three-dimensional point clouds of movement data.</claim-text>
    </claim>
    <claim num="7" id="clm-00007-en">
      <claim-text>
        <b>7</b>. The system of <claim-ref idref="clm-00003-en">claim 3</claim-ref> wherein the physics engine is configured to perform one or more algorithmic processes of the gathered motion data to compute kinematics and kinetics of the subject or object.</claim-text>
    </claim>
    <claim num="8" id="clm-00008-en">
      <claim-text>
        <b>8</b>. The system of <claim-ref idref="clm-00007-en">claim 7</claim-ref> wherein the physics engine is configured to determine points of interest during a particular motion using event detection algorithms.</claim-text>
    </claim>
    <claim num="9" id="clm-00009-en">
      <claim-text>
        <b>9</b>. The system of <claim-ref idref="clm-00007-en">claim 7</claim-ref> wherein the physics engine is configured to define a model of the subject having a plurality of segments.</claim-text>
    </claim>
    <claim num="10" id="clm-00010-en">
      <claim-text>
        <b>10</b>. The system of <claim-ref idref="clm-00009-en">claim 9</claim-ref> wherein the physics engine is further configured to compute an inverse dynamics model of motion of the subject using angular velocity components of each segment, angular acceleration components of each segment, and linear acceleration components of each segment center of mass.</claim-text>
    </claim>
    <claim num="11" id="clm-00011-en" independent="true">
      <claim-text>
        <b>11</b>. A method for providing prescriptive feedback based on motion of a human subject or object comprising:
<claim-text>A) gathering data about the motion of a subject or object,</claim-text><claim-text>B) processing the gathered motion data and correlating the processed motion data with previously stored biomechanical data representing idealized motion models, and</claim-text><claim-text>C) providing prescriptive feedback on flaws identified in the subject's or object's motion.</claim-text></claim-text>
    </claim>
    <claim num="12" id="clm-00012-en">
      <claim-text>
        <b>12</b>. The method of <claim-ref idref="clm-00011-en">claim 11</claim-ref> further comprising:
<claim-text>E) enabling compliance monitoring of motion of the subject or object.</claim-text></claim-text>
    </claim>
    <claim num="13" id="clm-00013-en">
      <claim-text>
        <b>13</b>. The method of <claim-ref idref="clm-00011-en">claim 11</claim-ref> wherein the interactive interface is further configured for enabling compliance monitoring of the subject's or object's motion.</claim-text>
    </claim>
    <claim num="14" id="clm-00014-en">
      <claim-text>
        <b>14</b>. The method of <claim-ref idref="clm-00011-en">claim 11</claim-ref> wherein A) gathering data about the motion of a subject or object comprises:
<claim-text>A1) gathering data about the motion of a subject or object with an optical motion capture device to collect three-dimensional positional data from reflective markers placed on the subject or object.</claim-text></claim-text>
    </claim>
    <claim num="15" id="clm-00015-en">
      <claim-text>
        <b>15</b>. The method of <claim-ref idref="clm-00011-en">claim 11</claim-ref> wherein A) gathering data about the motion of a subject or object comprises:
<claim-text>A1) gathering data about the motion of a subject or object with an inertial measurement unit disposed on the subject or object to collect three-dimensional translation and rotation motion data.</claim-text></claim-text>
    </claim>
    <claim num="16" id="clm-00016-en">
      <claim-text>
        <b>16</b>. The method of <claim-ref idref="clm-00011-en">claim 11</claim-ref> wherein A) gathering data about the motion of a subject or object comprises:
<claim-text>A1) gathering data about the motion of a subject or object with an infrared scanning device configured to capture three-dimensional point clouds of movement data.</claim-text></claim-text>
    </claim>
    <claim num="17" id="clm-00017-en">
      <claim-text>
        <b>17</b>. The method of <claim-ref idref="clm-00011-en">claim 11</claim-ref> wherein B) comprises:
<claim-text>B1) performing one or more algorithmic processes of the gathered motion data to compute kinematics and kinetics of the subject or object.</claim-text></claim-text>
    </claim>
    <claim num="18" id="clm-00018-en">
      <claim-text>
        <b>18</b>. The method of <claim-ref idref="clm-00017-en">claim 17</claim-ref> wherein B) comprises:
<claim-text>B1) determining points of interest during a particular motion using event detection algorithms.</claim-text></claim-text>
    </claim>
    <claim num="19" id="clm-00019-en">
      <claim-text>
        <b>19</b>. The method of <claim-ref idref="clm-00017-en">claim 17</claim-ref> wherein B) comprises:
<claim-text>B1) defining a model of the subject having a plurality of segments.</claim-text></claim-text>
    </claim>
    <claim num="20" id="clm-00020-en">
      <claim-text>
        <b>20</b>. The method of <claim-ref idref="clm-00017-en">claim 17</claim-ref> wherein B) comprises:
<claim-text>B1) computing an inverse dynamics model of motion of the subject using angular velocity components of each segment, angular acceleration components of each segment, and linear acceleration components of each segment center of mass.</claim-text></claim-text>
    </claim>
    <claim num="21" id="clm-00021-en" independent="true">
      <claim-text>
        <b>21</b>. A method for providing prescriptive feedback based on motion of a human subject or object comprising:
<claim-text>A) gathering data about the motion of a subject or object,</claim-text><claim-text>B) processing motion data,</claim-text><claim-text>C) correlating the process motion data with previously stored biomechanical data representing idealized motion models, and</claim-text><claim-text>D) providing prescriptive feedback on flaws identified in the subject's and object's motion based on the motion data gathered from either the wearable technology or interactive platform. </claim-text></claim-text>
    </claim>
  </claims>
  <drawings id="drawings" format="original">
    <figure num="1">
      <img he="N/A" wi="N/A" file="US20150257682A1_00001.PNG" alt="clipped image" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00000.TIF" />
    </figure>
    <figure num="2">
      <img he="N/A" wi="N/A" file="US20150257682A1_00002.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00001.TIF" />
    </figure>
    <figure num="3">
      <img he="N/A" wi="N/A" file="US20150257682A1_00003.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00002.TIF" />
    </figure>
    <figure num="4">
      <img he="N/A" wi="N/A" file="US20150257682A1_00004.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00003.TIF" />
    </figure>
    <figure num="5">
      <img he="N/A" wi="N/A" file="US20150257682A1_00005.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00004.TIF" />
    </figure>
    <figure num="6">
      <img he="N/A" wi="N/A" file="US20150257682A1_00006.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00005.TIF" />
    </figure>
    <figure num="7">
      <img he="N/A" wi="N/A" file="US20150257682A1_00007.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00006.TIF" />
    </figure>
    <figure num="8">
      <img he="N/A" wi="N/A" file="US20150257682A1_00008.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00007.TIF" />
    </figure>
    <figure num="9">
      <img he="N/A" wi="N/A" file="US20150257682A1_00009.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00008.TIF" />
    </figure>
    <figure num="10">
      <img he="N/A" wi="N/A" file="US20150257682A1_00010.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00009.TIF" />
    </figure>
    <figure num="11">
      <img he="N/A" wi="N/A" file="US20150257682A1_00011.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00010.TIF" />
    </figure>
    <figure num="12">
      <img he="N/A" wi="N/A" file="US20150257682A1_00012.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00011.TIF" />
    </figure>
    <figure num="13">
      <img he="N/A" wi="N/A" file="US20150257682A1_00013.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00012.TIF" />
    </figure>
    <figure num="14">
      <img he="N/A" wi="N/A" file="US20150257682A1_00014.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00013.TIF" />
    </figure>
    <figure num="15">
      <img he="N/A" wi="N/A" file="US20150257682A1_00015.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00014.TIF" />
    </figure>
    <figure num="16">
      <img he="N/A" wi="N/A" file="US20150257682A1_00016.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00015.TIF" />
    </figure>
    <figure num="17">
      <img he="N/A" wi="N/A" file="US20150257682A1_00017.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00016.TIF" />
    </figure>
    <figure num="18">
      <img he="N/A" wi="N/A" file="US20150257682A1_00018.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00017.TIF" />
    </figure>
    <figure num="19">
      <img he="N/A" wi="N/A" file="US20150257682A1_00019.PNG" alt="thumbnail image" img-content="drawing" img-format="png" original="US20150257682A1-20150917-D00000.TIF" />
    </figure>
  </drawings>
  <image file="US20150257682A1.PDF" type="pdf" size="1008587" pages="27" />
</lexisnexis-patent-document>