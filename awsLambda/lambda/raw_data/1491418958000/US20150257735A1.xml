<?xml version="1.0" encoding="utf-8"?>
<!-- Copyright ©2016 LexisNexis Univentio, The Netherlands. -->
<lexisnexis-patent-document schema-version="1.13" date-produced="20160127" file="US20150257735A1.xml" produced-by="LexisNexis-Univentio" lang="eng" date-inserted="20150917" time-inserted="030139" date-changed="20151208" time-changed="150652">
  <bibliographic-data lang="eng">
    <publication-reference publ-type="Application" publ-desc="Patent Application Publication">
      <document-id id="121315001">
        <country>US</country>
        <doc-number>20150257735</doc-number>
        <kind>A1</kind>
        <date>20150917</date>
      </document-id>
    </publication-reference>
    <application-reference appl-type="utility">
      <document-id>
        <country>US</country>
        <doc-number>14522188</doc-number>
        <date>20141023</date>
      </document-id>
    </application-reference>
    <application-series-code>14</application-series-code>
    <language-of-filing>eng</language-of-filing>
    <language-of-publication>eng</language-of-publication>
    <dates-of-public-availability date-changed="20150924">
      <unexamined-printed-without-grant>
        <date>20150917</date>
      </unexamined-printed-without-grant>
    </dates-of-public-availability>
    <classifications-ipcr date-changed="20151208">
      <classification-ipcr sequence="1">
        <text>A61B   8/00        20060101AFI20150917BHUS        </text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>8</main-group>
        <subgroup>00</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>US</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>A61B   8/08        20060101ALI20150917BHUS        </text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>8</main-group>
        <subgroup>08</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>US</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-ipcr>
      <classification-ipcr sequence="3">
        <text>A61B   8/14        20060101ALI20150917BHUS        </text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>8</main-group>
        <subgroup>14</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>US</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-ipcr>
    </classifications-ipcr>
    <classifications-cpc date-changed="20151208">
      <classification-cpc sequence="1">
        <text>A61B   8/462       20130101 FI20150917BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>8</main-group>
        <subgroup>462</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="2">
        <text>A61B   5/0035      20130101 LI20151201BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>5</main-group>
        <subgroup>0035</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20151201</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="3">
        <text>A61B   5/0077      20130101 LI20151201BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>5</main-group>
        <subgroup>0077</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20151201</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="4">
        <text>A61B   8/4416      20130101 LA20151130BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>8</main-group>
        <subgroup>4416</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <action-date>
          <date>20151130</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="5">
        <text>A61B   8/464       20130101 LA20150917BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>8</main-group>
        <subgroup>464</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="6">
        <text>A61B   8/5207      20130101 LA20150917BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>8</main-group>
        <subgroup>5207</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="7">
        <text>A61B   8/5261      20130101 LI20151130BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>8</main-group>
        <subgroup>5261</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20151130</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="8">
        <text>A61B   8/565       20130101 LA20150917BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>8</main-group>
        <subgroup>565</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
    </classifications-cpc>
    <number-of-claims calculated="yes">21</number-of-claims>
    <invention-title id="title_eng" date-changed="20150917" lang="eng" format="original">SYSTEMS AND METHODS FOR DISPLAYING MEDICAL IMAGES</invention-title>
    <related-documents date-changed="20150917">
      <provisional-application>
        <document-id>
          <country>US</country>
          <doc-number>61895302</doc-number>
          <date>20131024</date>
        </document-id>
      </provisional-application>
      <provisional-application>
        <document-id>
          <country>US</country>
          <doc-number>61988098</doc-number>
          <date>20140502</date>
        </document-id>
      </provisional-application>
    </related-documents>
    <parties date-changed="20150917">
      <applicants>
        <applicant sequence="1" app-type="applicant" designation="us-only">
          <addressbook lang="eng">
            <orgname>Evena Medical, Inc.</orgname>
            <address>
              <city>Los Altos</city>
              <state>CA</state>
              <country>US</country>
            </address>
          </addressbook>
          <residence>
            <country>US</country>
          </residence>
        </applicant>
      </applicants>
      <inventors>
        <inventor sequence="1" designation="us-only">
          <addressbook lang="eng">
            <last-name>Ball</last-name>
            <first-name>Frank J.</first-name>
            <address>
              <city>Roseville</city>
              <state>CA</state>
              <country>US</country>
            </address>
          </addressbook>
        </inventor>
        <inventor sequence="2" designation="us-only">
          <addressbook lang="eng">
            <last-name>Cespedes</last-name>
            <first-name>Ignacio E.</first-name>
            <address>
              <city>Folsom</city>
              <state>CA</state>
              <country>US</country>
            </address>
          </addressbook>
        </inventor>
        <inventor sequence="3" designation="us-only">
          <addressbook lang="eng">
            <last-name>Harris</last-name>
            <first-name>Melvyn L.</first-name>
            <address>
              <city>Folsom</city>
              <state>CA</state>
              <country>US</country>
            </address>
          </addressbook>
        </inventor>
        <inventor sequence="4" designation="us-only">
          <addressbook lang="eng">
            <last-name>Gruebele</last-name>
            <first-name>David J.</first-name>
            <address>
              <city>Folsom</city>
              <state>CA</state>
              <country>US</country>
            </address>
          </addressbook>
        </inventor>
        <inventor sequence="5" designation="us-only">
          <addressbook lang="eng">
            <last-name>Hoseit</last-name>
            <first-name>Paul M.</first-name>
            <address>
              <city>El Dorado Hills</city>
              <state>CA</state>
              <country>US</country>
            </address>
          </addressbook>
        </inventor>
      </inventors>
    </parties>
    <patent-family date-changed="20150917">
      <main-family family-id="173668124">
        <family-member>
          <document-id>
            <country>US</country>
            <doc-number>20150257735</doc-number>
            <kind>A1</kind>
            <date>20150917</date>
          </document-id>
          <application-date>
            <date>20141023</date>
          </application-date>
        </family-member>
      </main-family>
      <complete-family family-id="173668123">
        <family-member>
          <document-id>
            <country>US</country>
            <doc-number>20150257735</doc-number>
            <kind>A1</kind>
            <date>20150917</date>
          </document-id>
          <application-date>
            <date>20141023</date>
          </application-date>
        </family-member>
      </complete-family>
    </patent-family>
  </bibliographic-data>
  <abstract id="abstr_eng" date-changed="20150917" lang="eng" format="original">
    <p id="p-a-00001-en" num="0000">Various embodiments relate to methods and systems for displaying an ultrasound image of a patient's target body portion using a wearable display module. A wearable display module can be mounted on a user's head. A wearable display module can display ultrasound images as a heads up display so that a user can view an ultrasound image while maintaining lines of sight with other viewpoints. Various embodiments relate to methods and systems for displaying both an ultrasound image and a light image (e.g., a near infrared image). A single display can be configured to display the light image and the ultrasound image on a single surface, in close proximity, and at the same time or in sequence. This disclosure provides in some embodiments for the use of wireless communication between various imaging probes, sensors, image processors, and display modules in order to facilitate more effective viewing of images.</p>
  </abstract>
  <description id="descr_eng" lang="eng" format="original" date-changed="20150917">
    <related-apps>
      <heading id="h-00001-en" level="1">RELATED APPLICATIONS</heading>
      <p id="p-00001-en" num="0001">This application claims the benefit under 35 U.S.C. §119(e) of U.S. Provisional Patent Application No. 61/895,302, filed Oct. 24, 2013, and titled SYSTEMS AND METHODS FOR DISPLAYING MEDICAL IMAGES, and U.S. Provisional Patent Application No. 61/988,098, filed May 2, 2014, and titled SYSTEMS AND METHODS FOR DISPLAYING MEDICAL IMAGES. The entire contents of each of the above-identified applications are hereby incorporated by reference herein and made a part of this specification.</p>
    </related-apps>
    <summary>
      <heading id="h-00002-en" level="1">INCORPORATION BY REFERENCE</heading>
      <p id="p-00002-en" num="0002">U.S. patent application Ser. No. 13/802,604 (the '604 Application), titled VEIN IMAGING SYSTEMS AND METHODS, filed on Mar. 13, 2013, and published as U.S. Patent Publication No. US 2014/0039309 on Feb. 6, 2014, is hereby incorporated by reference in its entirety.</p>
      <heading id="h-00003-en" level="1">BACKGROUND</heading>
      <p id="p-00003-en" num="0003">1. Field of the Disclosure</p>
      <p id="p-00004-en" num="0004">Some embodiments of this disclosure relate to systems and methods for displaying medical images, such as subcutaneous images taken of a patient with medical imaging technology.</p>
      <p id="p-00005-en" num="0005">2. Description of the Related Art</p>
      <p id="p-00006-en" num="0006">Certain imaging technologies such as near infrared (NIR) light imaging and ultrasound imaging can allow a doctor or healthcare professional to examine subcutaneous tissue of a patient. Access to such images can be useful for effective treatment and/or diagnosis, particularly in emergency response settings. However, many methods and systems for displaying medical images require consulting stationary screens adapted only for displaying a single type of image. Such limitations can cause delay in treatment and/or diagnosis.</p>
      <heading id="h-00004-en" level="1">SUMMARY OF SOME EMBODIMENTS</heading>
      <p id="p-00007-en" num="0007">A method for displaying an ultrasound image can include identifying a target body portion of a patient to be imaged; emitting ultrasound signals from an ultrasound probe, wherein the ultrasound signals can cause echo signals to be reflected from tissue within the target body portion; receiving the echo signals; generating an ultrasound image of the target body portion based at least in part on the echo signals; and displaying the ultrasound image on a head-mounted display.</p>
      <p id="p-00008-en" num="0008">The method can include transmitting the ultrasound image to a remote display accessible to a remote healthcare professional; receiving treatment instructions from the remote healthcare professional; and administering treatment based at least in part on the instructions from the remote healthcare professional.</p>
      <p id="p-00009-en" num="0009">The method can include generating echo signal data based at least in part on the echo signals; and transmitting the echo signal data wirelessly to an image processor that generates the ultrasound image.</p>
      <p id="p-00010-en" num="0010">The method can include transmitting the ultrasound image wirelessly to the head-mounted display.</p>
      <p id="p-00011-en" num="0011">The method can include attaching the head-mounted display to a user. The attaching can be achieved by the user wearing a helmet that supports the head-mounted display such that the ultrasound image displayed on the head-mounted display is configured to be visible to the user wearing the helmet. The attaching can be achieved by the user wearing an eyewear frame that supports the head-mounted display such that the ultrasound image displayed on the head-mounted display is configured to be visible to the user wearing the eyewear frame. The attaching can be achieved by the user wearing a headband that supports the head-mounted display such that the ultrasound image displayed on the head-mounted display is configured to be visible to the user wearing the headband.</p>
      <p id="p-00012-en" num="0012">A system for displaying an ultrasound image can include an ultrasound probe configured to emit ultrasound signals and detect echo signals reflected from tissue in a target body portion of a patient; an image processor configured to receive echo signal data that is based at least in part on the echo signals received by the ultrasound probe and generate an ultrasound image of the target body portion based at least in part on the echo signal data; and a wearable display configured to receive the ultrasound image of the target body portion from the image processor and display the ultrasound image.</p>
      <p id="p-00013-en" num="0013">The system can include a remote display accessible to a remote healthcare professional and configured to receive and display the image of the target body portion.</p>
      <p id="p-00014-en" num="0014">The system can include a communications module that is configured to deliver the ultrasound image wirelessly to a remote display accessible to a remote healthcare professional.</p>
      <p id="p-00015-en" num="0015">The system can include a communications module that is configured to deliver the ultrasound image wirelessly to the wearable display module.</p>
      <p id="p-00016-en" num="0016">The wearable display can be configured to display the image of the target body portion on a heads-up display.</p>
      <p id="p-00017-en" num="0017">The wearable display can include an at least partially transparent surface.</p>
      <p id="p-00018-en" num="0018">The system can include a helmet that supports the wearable display such that the ultrasound image displayed by the wearable display is configured to be visible to a person wearing the helmet. The system can include an eyewear frame that supports the wearable display such that the ultrasound image displayed by the wearable display is configured to be visible to a person wearing the eyewear frame. The system can include a headband that supports the wearable display such that the ultrasound image displayed by the wearable display is configured to be visible to a person wearing the headband.</p>
      <p id="p-00019-en" num="0019">An image display system can include an ultrasound probe; a processor configured to generate an image based at least in part on data from the ultrasound probe; and a wearable display configured to display the image.</p>
      <p id="p-00020-en" num="0020">The wearable display can be configured to display the image as a heads-up display image.</p>
      <p id="p-00021-en" num="0021">The processor can be capable of wireless communication with at least one of the ultrasound probe and/or the wearable display.</p>
      <p id="p-00022-en" num="0022">The system can include a remote display configured to display the image.</p>
      <p id="p-00023-en" num="0023">The system can include a communication interface configured to wirelessly transmit the ultrasound image to a remote display.</p>
      <p id="p-00024-en" num="0024">The wearable display module can be a head-mounted display configured to display the image to the wearer regardless of the orientation of the wearer's head.</p>
      <p id="p-00025-en" num="0025">A method for displaying medical images of a target body portion of a patient can include identifying a target body portion to be imaged; emitting ultrasound signals from an ultrasound probe that cause echo signals to be reflected from the target body portion; providing echo signal data based at least in part on the reflected echo signals; generating an ultrasound image of the target body portion from the echo signal data that provides a subcutaneous cross-sectional view of the target body portion; displaying the ultrasound image on a display; illuminating the target body portion with near infrared light; receiving near infrared light from the target body portion onto a light sensor; generating a near infrared image of the target body portion based at least in part on the near infrared light received by the light sensor; and displaying the near infrared image on the same display that displays the ultrasound image.</p>
      <p id="p-00026-en" num="0026">The ultrasound image and the near infrared light image can be displayed simultaneously.</p>
      <p id="p-00027-en" num="0027">The ultrasound image and the near infrared image can be displayed at different times on the same display.</p>
      <p id="p-00028-en" num="0028">The method can include displaying one of the ultrasound image and the near infrared image on the display at a first time; and in response to user input received by a user interface, displaying the other of the near infrared image and the ultrasound image on the display at a second time.</p>
      <p id="p-00029-en" num="0029">The display can be a head-mounted display. The display can include a heads-up display.</p>
      <p id="p-00030-en" num="0030">Illuminating the target body portion with near infrared light can include illuminating the target body portion with multiple lines of near infrared light emitted from multiple near infrared light sources. Illuminating the target body portion with near infrared light can include illuminating the target body portion with different lines of near infrared light at different times.</p>
      <p id="p-00031-en" num="0031">A system for displaying medical images of a target body portion of a patient can include an ultrasound probe configured to emit ultrasound signals and receive echo signals reflected from a target body portion of a patient; an ultrasound image processor configured to generate an ultrasound image of the target body portion based at least in part on the echo signals received by the ultrasound probe; a near infrared light source configured to direct near infrared light onto the target body portion; a light sensor configured to receive near infrared light reflected from the target body portion; a near infrared light image processor configured to generate a near infrared image of the target body portion based at least in part on the near infrared light received by the light sensor; and a display configured to receive both the ultrasound image and the near infrared image and display both the ultrasound image and the near infrared image.</p>
      <p id="p-00032-en" num="0032">The ultrasound image processor and the near infrared processor are implemented using a single computer processor.</p>
      <p id="p-00033-en" num="0033">The display can be configured to display both the ultrasound image and the near infrared image simultaneously.</p>
      <p id="p-00034-en" num="0034">The system can include a user interface configured to receive input from a user to select the ultrasound image or the near infrared image, and the display can be configured to display the ultrasound image in response to a user selection of the ultrasound image, and the display can be configured to display the near infrared image in response to a user selection of the near infrared image.</p>
      <p id="p-00035-en" num="0035">The system can include a remote display configured to be accessible to a remote healthcare professional and configured to receive both the ultrasound image and the near infrared image and display the two images.</p>
      <p id="p-00036-en" num="0036">The display can be configured to display both the ultrasound image and the near infrared image as heads-up display images.</p>
      <p id="p-00037-en" num="0037">The display can be capable of being physically mounted on a user.</p>
      <p id="p-00038-en" num="0038">The near infrared light source can include multiple light emitters configured to emit different lines of near infrared light. The system can be configured to illuminate the target body portion with light from each of the multiple light emitters simultaneously. The system can include a user interface that includes one or more user input elements, wherein the multiple light emitters are configured to change the relative intensity of the emitted lines of near infrared light in response to input received by the one or more user input elements. The system can be configured to pulse the multiple light emitters to illuminate the target body portion with different wavelengths of near infrared light at different times.</p>
      <p id="p-00039-en" num="0039">A system for displaying medical images of a target area can include an ultrasound probe configured to emit ultrasound signals into a target area and receive echo signals from the target area; a light emitter configured to direct light onto the target area; a light sensor configured to receive light from the target area; a controller configured to produce an ultrasound image based at least in part on the echo signals and configured to produce a light image based at least in part on the light received by the light sensor; and a display configured to display both the ultrasound image and the light image.</p>
      <p id="p-00040-en" num="0040">The light emitter can be configured to emit near infrared (NIR) light.</p>
      <p id="p-00041-en" num="0041">The display can be configured to display both the ultrasound image and the light image simultaneously.</p>
      <p id="p-00042-en" num="0042">The display can be capable of being physically mounted on a user.</p>
      <p id="p-00043-en" num="0043">The display can be configured to display both the ultrasound image and the light image as heads-up display images.</p>
      <p id="p-00044-en" num="0044">The light emitter can include at least first and second light emitters each having different near infrared emission spectra. The system can be configured to illuminate the target area with light from each of the multiple light emitters simultaneously.</p>
      <p id="p-00045-en" num="0045">The system can include a user interface that includes one or more user input elements, wherein the multiple light emitters are configured to change the relative intensity of the emitted light from the respective first and second light emitters in response to input received by the one or more user input elements.</p>
      <p id="p-00046-en" num="0046">The system can be configured to pulse the multiple light emitters to illuminate the target area with different wavelengths of near infrared light at different times.</p>
      <p id="p-00047-en" num="0047">A method of displaying an ultrasound image can include emitting ultrasound signals from an ultrasound probe, wherein the ultrasound signals cause echo signals to be reflected from tissue within a target body portion of a patient; receiving the echo signals reflected from the tissue within the target body portion; generating an ultrasound image of the target body portion based at least in part on the echo signals; determining a position of the ultrasound probe relative to a head-mounted display; and displaying the ultrasound image on the head-mounted display at a location that is based at least in part on the position of the ultrasound probe relative to the head-mounted display, wherein the location of the ultrasound image on the head-mounted display is configured such that the ultrasound image is overlaid over the target body portion.</p>
      <p id="p-00048-en" num="0048">The method can include determining that the position of the ultrasound probe relative to the head-mounted display has changed; and moving the location of the ultrasound image on the head-mounted display such that the ultrasound image remains overlaid over the target body portion.</p>
      <p id="p-00049-en" num="0049">Determining the position of the head-mounted display relative to the ultrasound probe can include receiving light onto a light sensor and analyzing image data that is based on the light received by the light sensor.</p>
      <p id="p-00050-en" num="0050">The light sensor can be coupled to the head-mounted display, and the ultrasound probe can include one or more position identifiers, and analyzing the image data can include identifying the one or more position identifiers in the image data.</p>
      <p id="p-00051-en" num="0051">A system for displaying an ultrasound image can include an ultrasound probe configured to emit ultrasound signals and to receive echo signals; a display; and a controller configured to generate an ultrasound image based at least in part on the echo signals, wherein the controller is configured to display the ultrasound image on the display at a location that is based at least in part on a position of the ultrasound probe.</p>
      <p id="p-00052-en" num="0052">The display can be configured to be coupled to a user. The system can include a head-mounted unit that supports the display.</p>
      <p id="p-00053-en" num="0053">The controller can be configured to display the ultrasound image at the location based at least in part on a position of the ultrasound probe relative to the display.</p>
      <p id="p-00054-en" num="0054">The controller can be configured to display the ultrasound image at the location based at least in part on a position of an operator.</p>
      <p id="p-00055-en" num="0055">The controller can be configured to display the ultrasound image at the location such that the ultrasound image is overlaid over a target body portion being imaged.</p>
      <p id="p-00056-en" num="0056">The controller can be configured to move the location of the ultrasound image on the display in response to a change in the position of the ultrasound probe. The controller can be configured to move the location of the ultrasound image on the display in response to a change in the position of the ultrasound probe relative to the display. The controller can be configured to move the location of the ultrasound image on the display in response to a change in the position of the ultrasound probe relative to an operator.</p>
      <p id="p-00057-en" num="0057">The display can include a transparent stationary display configured to be positioned between an operator and the target body portion.</p>
      <p id="p-00058-en" num="0058">Various display disclosed herein can be configured to be mounted on an articulated arm slidably attached to a vertical support.</p>
      <p id="p-00059-en" num="0059">A system for displaying medical images of a target body portion of a patient can include a near infrared light source configured to direct near infrared light onto the target body portion, the near infrared light source comprising multiple light emitters having different respective emission spectra; a user interface configured to responsively adjust the relative intensities of the light emitters based on user input; at least one light sensor configured to receive near infrared light from the target body portion; a near infrared light image processor configured to generate a near infrared image of the target body portion based at least in part on the near infrared light received by the light sensor; and a display configured to display the near infrared image.</p>
      <p id="p-00060-en" num="0060">The near infrared light source can include a first light emitter configured to emit light having a peak in the wavelength range of about 700 nm to about 800 nm; a second light emitter configured to emit light having a peak in the wavelength range of about 800 nm to about 900 nm; and a third light emitter configured to emit light having a peak in the wavelength range of about 900 nm to about 1100 nm.</p>
      <p id="p-00061-en" num="0061">Various near infrared light sources disclosed herein can include multiple light emitters configured to emit different lines of near infrared light, and the systems disclosed herein can include a user interface configured to responsively adjust the relative intensities of the light emitters based on user input received by the user interface.</p>
      <p id="p-00062-en" num="0062">Various light emitters disclosed herein can include at least first and second light emitters having different near infrared spectral outputs, and the systems disclosed herein can include a user interface configured to responsively adjust the relative intensities of the first and second light emitters based on user input received by the user interface.</p>
      <p id="p-00063-en" num="0063">A medical imaging system can include a display; a housing supporting the display; a near infrared light source configured to emit near infrared light onto a target area; a light sensor configured to receive near infrared light from the target area; one or more input modules configured to receive input from one or more of an ultrasound probe, a spectrometer, an electrocardiogram (EKG) device, an X-ray device; a magnetic resonance imaging (MRI) device, a pulse oximeter, a blood pressure monitor, a digital stethoscope, a thermometer, an otoscope, and an examination camera; and a controller comprising one or more hardware processors inside the housing, the controller configured to produce an image for the display based at least in part on the near infrared light received by the light sensor, and wherein the controller is configured to produce an image and/or data for the display based at least in part on input received from the one or more input modules.</p>
      <p id="p-00064-en" num="0064">The near infrared light source can include multiple light emitters configured to emit light having different near infrared spectra.</p>
      <p id="p-00065-en" num="0065">The system can be configured to illuminate the target area with light from each of the multiple light emitters simultaneously.</p>
      <p id="p-00066-en" num="0066">The system can include a user interface that includes one or more user input elements, and the multiple light emitters can be configured to change the intensity of the emitted light in response to input received by the one or more user input elements.</p>
      <p id="p-00067-en" num="0067">The system can be configured to pulse the multiple light emitters to illuminate the target area with different wavelengths of near infrared light at different times.</p>
      <p id="p-00068-en" num="0068">The system can include a user interface, and the controller can be responsive to user input received via the user interface to toggle the display between a near infrared image and an image that is based on input received from the one or more input modules.</p>
      <p id="p-00069-en" num="0069">The input module can include a wireless input module configured to receive a wireless signal from one or more of the ultrasound probe, the spectrometer, the electrocardiogram (EKG) device, the X-ray device; the magnetic resonance imaging (Mill) device, the pulse oximeter, the blood pressure monitor, the digital stethoscope, the thermometer, the otoscope, and the examination camera</p>
      <p id="p-00070-en" num="0070">The input module can include an electrical port for a wired connection.</p>
      <p id="p-00071-en" num="0071">The controller can be configured to display a near infrared image and an image that is based on input received from the one or more input modules simultaneously.</p>
      <p id="p-00072-en" num="0072">The system of can include an ultrasound probe coupled to the one or more input modules.</p>
      <p id="p-00073-en" num="0073">A system for displaying medical images of a target area can include an ultrasound probe configured to emit ultrasound signals into a target area and receive echo signals from the target area; a light emitter configured to direct light onto the target area; a light sensor configured to receive light from the target area; a set of electrodes configured to detect electrical activity; a controller configured to produce an ultrasound image based at least in part on the echo signals, a light image based at least in part on the light received by the light sensor, and electrocardiogram information based at least in part on the electrical activity detected by the electrodes; and a display configured to display the ultrasound image, the light image, and the electrocardiogram information.</p>
      <p id="p-00074-en" num="0074">The set of electrodes can be configured to detect electrical activity indicative that the tip of a peripheral inserted central catheter has reached a location within a patient.</p>
      <p id="p-00075-en" num="0075">A system for displaying medical images of a target area can include an ultrasound probe configured to emit ultrasound signals into a target area and receive echo signals from the target area; a light emitter configured to direct light onto the target area; a light sensor configured to receive light from the target area; a spectrometer configured to receive light from the target area; a controller configured to produce an ultrasound image based at least in part on the echo signals, a light image based at least in part on the light received by the light sensor, and spectroscopic information based at least in part on the light detected by the spectrometer; and a display configured to display the ultrasound image, the light image, and the spectroscopic information.</p>
      <p id="p-00076-en" num="0076">Various systems disclosed herein can include a thermometer configured to detect a temperature associated with the target area, wherein in the controller is further configured to produce a temperature reading based at least in part on the temperature detected by the thermometer and wherein the display is further configured to display the temperature reading.</p>
      <p id="p-00077-en" num="0077">Various displays disclosed herein can be configured to display the ultrasound image and the near infrared light image at least partially overlaid one on top of the other.</p>
      <p id="p-00078-en" num="0078">Various displays disclosed herein can be configured to display the ultrasound image and the light image at least partially overlaid one on top of the other.</p>
      <p id="p-00079-en" num="0079">Various systems disclosed herein can include a visible light source configured to emit visible light onto the target body portion.</p>
      <p id="p-00080-en" num="0080">Various systems disclosed herein can include a visible light source configured to emit visible light onto the target area, and wherein the light emitter is configured to emit near infrared light.</p>
      <p id="p-00081-en" num="0081">A medical imaging system can include a near infrared light source configured to emit near infrared light onto a target area; a light sensor configured to receive light from the target area; a display; a controller configured to produce an image for the display based on the near infrared light received by the light sensor; and a visible light source configured to emit visible light onto the target area.</p>
      <p id="p-00082-en" num="0082">The visible light source can be a red light source. The visible light source is an orange light source.</p>
      <p id="p-00083-en" num="0083">A method of recording attempts to locate a vein can include emitting near infrared light on a target area; detecting near infrared light received from the target area; displaying a near infrared image based on the near infrared light received from the target area; receiving an input indicating that a vein with a satisfactory venous access site was not identified in the near infrared image; storing an indication of an attempt to identify a vein with a suitable venous access site using near infrared imaging; emitting an ultrasound signal into the target area; receiving echo signals reflected from the target area; and displaying an ultrasound image based on the echo signals.</p>
      <p id="p-00084-en" num="0084">The indication of the attempt can include a near infrared image based at least in part on the near infrared light received from the target area.</p>
      <p id="p-00085-en" num="0085">The indication of the attempt can include a patient identifier.</p>
      <p id="p-00086-en" num="0086">A system for performing and recording attempts to locate a vein can include a near infrared light emitter configured to emit near infrared light onto a target area; a near infrared light detector configured to detect near infrared light received from the target area; an ultrasound probe configured to emit an ultrasound signal into the target area and receive echo signals from the target area; and a controller comprising one or more hardware processors, the controller configured to store an indication of an attempt to identify a vein with a suitable venous access site using near infrared imaging.</p>
      <p id="p-00087-en" num="0087">The indication of the attempt can include a near infrared image based at least in part on the near infrared light received from the target area.</p>
      <p id="p-00088-en" num="0088">The indication of the attempt can include a patient identifier.</p>
      <p id="p-00089-en" num="0089">The system can include a display configured to display both a near infrared image based on the light received by the near infrared light detector and an ultrasound image based on the echo signals received by the ultrasound probe.</p>
      <p id="p-00090-en" num="0090">A method of accessing a patient's vasculature can include illuminating a target area with near infrared light; positioning a light sensor to receive near infrared light from the target area; viewing a near infrared image on a display, the near infrared image based on the near infrared light received by the light sensor; emitting an ultrasound signal into the target area using an ultrasound probe; receiving echo signals reflected from the target area using an ultrasound probe; viewing an ultrasound image on the same display used for viewing the near infrared image, the ultrasound image based on the echo signals received by the ultrasound probe; identifying or confirming a vein in the target area using the ultrasound image; and inserting a medical implement into the vein.</p>
      <p id="p-00091-en" num="0091">The method can include using the near infrared image and/or the ultrasound image to facilitate the inserting of the medical implement into the vein.</p>
      <p id="p-00092-en" num="0092">The method can include identifying a vein using the near infrared image; and confirming the vein using the ultrasound image.</p>
      <p id="p-00093-en" num="0093">The method can include illuminating the target area with near infrared light to produce a near infrared image on the display after confirming the vein using the ultrasound image; and using the near infrared image to facilitate the inserting of the medical implement into the vein.</p>
      <p id="p-00094-en" num="0094">The method can include using the ultrasound image to facilitate the inserting of the medical implement into the vein.</p>
      <p id="p-00095-en" num="0095">A method for assessing the usefulness of near infrared imaging for identifying a vein having a suitable venous access site on a patient can include accessing patient information comprising one or more of the patient' weight, Body Mass Index (BMI), blood pressure, temperature, skin color, past medical history, arm dominance, and kidney function; determining, using one or more computer processors, a score indicative of the likelihood that near infrared imaging would be useful for identifying a vein having a suitable venous access site, the determination based at least in part on the patient information; and outputting the score.</p>
      <p id="p-00096-en" num="0096">Accessing patient information can include accessing a hospital information system (HIS) and reading the patient information from the hospital information system (HIS).</p>
      <p id="p-00097-en" num="0097">Accessing patient information can include performing one or more measurements with one or more medical instruments to produce at least some of the patient information.</p>
      <p id="p-00098-en" num="0098">A system for assessing the usefulness of near infrared imaging for identifying a vein having a suitable venous access site on a patient can include computer-readable storage configured to store patient information comprising one or more of the patient′ weight, Body Mass Index (BMI), blood pressure, temperature, skin color, past medical history, arm dominance, and kidney function; at least one processor configured to determine a score indicative of the likelihood that near infrared imaging would be useful for identifying a vein having a suitable venous access site, the determination based at least in part on the patient information; and an output configured to output the score.</p>
      <p id="p-00099-en" num="0099">The system can include a near infrared light emitter configured to emit near infrared light onto a target area; and a light sensor configured to receive near infrared light from the target area; wherein the at least one processor is configured to produce a near infrared image for display, the near infrared image based in the near infrared light received by the light sensor.</p>
      <p id="p-00100-en" num="0100">The output can be a display configured to display the score.</p>
      <p id="p-00101-en" num="0101">The system can include one or more medical instruments configured to perform one or more measurements to produce at least some of the patient information.</p>
      <p id="p-00102-en" num="0102">The system can include a communication module configured to receive data from one or more medical instruments to produce at least some of the patient information.</p>
    </summary>
    <description-of-drawings>
      <heading id="h-00005-en" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
      <p id="p-00103-en" num="0103">
        <figref>FIG. 1</figref> shows a schematic example of an ultrasound system used to generate and display ultrasound images.</p>
      <p id="p-00104-en" num="0104">
        <figref>FIG. 2A</figref> shows an example of an embodiment where a display module is worn as part of a helmet.</p>
      <p id="p-00105-en" num="0105">
        <figref>FIG. 2B</figref> shows an example of an embodiment where a display module is worn as part of a frame as for glasses.</p>
      <p id="p-00106-en" num="0106">
        <figref>FIG. 2C</figref> shows an example of an embodiment where a display module is worn as part of a headband.</p>
      <p id="p-00107-en" num="0107">
        <figref>FIG. 2D</figref> shows an example of an embodiment where a display module is attached to an articulated arm.</p>
      <p id="p-00108-en" num="0108">
        <figref>FIG. 2E</figref> shows an example of an embodiment where a display module is attached to an articulated arm.</p>
      <p id="p-00109-en" num="0109">
        <figref>FIG. 3</figref> shows an example embodiment of an ultrasound system that includes a heads up display module.</p>
      <p id="p-00110-en" num="0110">
        <figref>FIG. 3A</figref> shows an example of a view of a heads up display module with an ultrasound probe where the displayed image is configured to overlay the imaging target.</p>
      <p id="p-00111-en" num="0111">
        <figref>FIG. 4A</figref> shows a schematic example of an ultrasound system where an ultrasound probe wirelessly communicates data with an image processor.</p>
      <p id="p-00112-en" num="0112">
        <figref>FIG. 4B</figref> shows a schematic example of an ultrasound system where an image processor wirelessly communicates ultrasound images with a display module.</p>
      <p id="p-00113-en" num="0113">
        <figref>FIG. 5A</figref> shows a schematic example of an ultrasound system where an ultrasound probe wirelessly communicates data with both a local image processor and a remote image processor.</p>
      <p id="p-00114-en" num="0114">
        <figref>FIG. 5B</figref> shows a schematic example of an ultrasound system where an image processor wirelessly communicates with both a local display module and a remote display module.</p>
      <p id="p-00115-en" num="0115">
        <figref>FIG. 6A</figref> shows a schematic example of a medical display system where an ultrasound probe and a light sensor both produce images of a single target and both images are displayed on a single display module.</p>
      <p id="p-00116-en" num="0116">
        <figref>FIG. 6B</figref> shows a schematic example of a medical display system where an ultrasound probe, a light sensor, and/or a third sensor can provide input regarding a single target and the information from the three inputs can be displayed on a single multi-input display module.</p>
      <p id="p-00117-en" num="0117">
        <figref>FIG. 6C</figref> shows an example embodiment of a medical imaging system.</p>
      <p id="p-00118-en" num="0118">
        <figref>FIG. 7</figref> shows a flow chart of an example embodiment of a method for displaying one or more images on a display module.</p>
      <p id="p-00119-en" num="0119">
        <figref>FIG. 8</figref> shows a flow chart of an example embodiment of a method for recording an attempt to locate a vein with NIR Light.</p>
      <p id="p-00120-en" num="0120">
        <figref>FIG. 9</figref> is a flowchart of an example embodiment of a method for accessing a patient's vasculature.</p>
      <p id="p-00121-en" num="0121">
        <figref>FIG. 10</figref> illustrates and example embodiment of a near infrared imaging assessment.</p>
    </description-of-drawings>
    <detailed-desc>
      <heading id="h-00006-en" level="1">DETAILED DESCRIPTION OF CERTAIN EMBODIMENTS</heading>
      <p id="p-00122-en" num="0122">Medical imaging technology can enable doctors, emergency medical technicians, nurses, and any other healthcare professionals to examine patients. Ultrasound imaging can be useful to a healthcare professional desiring to know the subcutaneous features in a target area or body portion of a patient. Once a healthcare professional identifies an area to be imaged with an ultrasound system it can be beneficial for the healthcare professional to have quick and convenient access to the ultrasound image. Ultrasound systems can in some embodiments provide a subcutaneous cross sectional view of a target area, which can be a target body portion of a patient. Additionally, in many situations a healthcare professional may need to devote attention to a variety of needs for a single patient, including performing an ultrasound. Some embodiments of this disclosure allow a healthcare professional to quickly and effectively view an ultrasound image while being able to maintain attention and focus to a variety of patient needs.</p>
      <p id="p-00123-en" num="0123">
        <figref>FIG. 1</figref> shows a schematic example of an ultrasound system. In order to display an ultrasound image, an ultrasound probe <b>102</b> can send ultrasound signals to an imaging target <b>106</b>. Imaging target <b>106</b> can be a target body portion of a patient or in some embodiments can be any target area. The ultrasound signals' interaction with the imaging target <b>106</b> can create reflected echo signals, which can be detected by ultrasound probe <b>102</b>. Based on the reflected echo signals and initial ultrasound signals the image processor <b>104</b> can generate an ultrasound image. This ultrasound image can then be displayed on a display (e.g., display module <b>100</b>). In some embodiments a doctor or healthcare professional can use the ultrasound image to assist in making a diagnosis or implementing a treatment.</p>
      <p id="p-00124-en" num="0124">Image processor <b>104</b> can develop an ultrasound image in a variety of methods. In some embodiments image processor <b>104</b> can use the difference between the ultrasound signal emitted by ultrasound probe <b>102</b> and the reflected echo signal to determine the location and density of features (e.g., body tissue) within imaging target <b>106</b>. Imaging processor <b>104</b> can compile reflected echo signal data from a plurality of lines or planes to generate an ultrasound image. In some embodiments, the image processor <b>104</b> can be a processor that handles multiple aspects of the ultrasound image system. Image processor <b>104</b> can be, in some embodiments, part of the ultrasound probe <b>102</b>. In some embodiments image processor <b>104</b> can also be part of the display module <b>100</b>. Image processor <b>104</b> can be a hardware processor, in some embodiments. The image processor <b>104</b> can be implemented as a software program or algorithm, which in some cases can be implemented on a general purpose computer processor.</p>
      <p id="p-00125-en" num="0125">The display module <b>100</b> can be wearable, in some embodiments. A wearable display module <b>100</b> can produce an image visible to a user (e.g., an operator of the ultrasound system) wearing the display module <b>100</b> or wearing structure that supports the display module <b>100</b>. <figref>FIGS. 2A</figref>, <b>2</b>B and <b>2</b>C show some example embodiments of wearable display modules <b>100</b>. A wearable display module <b>100</b> can be worn on any part of a user <b>110</b> such as a head, arm, shoulder or chest. <figref>FIG. 2A</figref> shows an example of some embodiments where a display module <b>100</b> can be worn as or with a helmet <b>118</b>. <figref>FIG. 2B</figref> shows an example of some embodiments where a display module <b>100</b> can be worn as or with a frame <b>120</b> as for glasses or other eyewear. <figref>FIG. 2C</figref> shows an example of some embodiments where display module <b>100</b> can be worn as or with a headband <b>122</b>. When worn as or with a helmet <b>118</b>, eyewear frame <b>120</b>, or headband <b>122</b>, display module <b>100</b> can, in some embodiments, be configured to display an image appearing to be on surface <b>116</b>. Surface <b>116</b> can be configured to be in front of the eye of a user <b>110</b> when the display module <b>100</b> is worn by a user <b>110</b>. Surface <b>116</b> can in some embodiments be at least partially transparent. Surface <b>116</b> can be non-transparent. Many types of displays can be used to present the ultrasound image to the user. For example, in some embodiments, the display <b>100</b> can be a direct retinal display.</p>
      <p id="p-00126-en" num="0126">When wearing a wearable display module <b>100</b> in connection with an ultrasound system as shown in <figref>FIG. 1</figref>, a healthcare professional wearing the display module <b>100</b> can locate or adjust the position of the ultrasound image more quickly and easily, compared to having the ultrasound image on a stationary screen that does not move with changes in the user's field of view. For example, in some embodiments a healthcare professional can be the user <b>110</b> wearing the display module <b>100</b>, as shown in <figref>FIGS. 2A</figref>, <b>2</b>B, and/or <b>2</b>C. The display module <b>100</b> can be configured to present the ultrasound image in the user's field of view. The display module <b>100</b> can be a head-mounted display module <b>100</b> or otherwise coupled to the user <b>110</b> such that the ultrasound image is maintained in the user's field of view as the wearer's head moves. For example, an image can be displayed on surface <b>116</b>, which can be maintained immediately in front of the eye of the healthcare professional user <b>110</b>. Maintaining the ultrasound image in the user's field of view can enable the user <b>110</b> to observe the ultrasound image while viewing the target area being imaged by the ultrasound system. The heads up display <b>100</b> can also enable the user <b>110</b> to observe the ultrasound image while attending to other matters (e.g., looking at another medical device, preparing medication, talking to another medical professional, looking at a body portion of the patient that is not being imaged, etc.). In some embodiments, the ultrasound system can enable the medical professional to attend to multiple needs of a patient in less time, which can be particularly important in an emergency response situation.</p>
      <p id="p-00127-en" num="0127">As shown in <figref>FIGS. 2D and 2E</figref>, in some embodiments, the display module <b>100</b> can be mountable onto an articulating arm <b>125</b>, which in some implementations can be slidably coupled to a vertical support member <b>127</b>. In some embodiments, the height of the articulated arm <b>125</b> (and the display module <b>100</b>) may be vertically adjusted (e.g., by sliding on the vertical support member <b>127</b>). The display module <b>100</b> may be positioned in a wide variety of positions depending on the patient's orientation, the medical practitioner's position, the portion of the patient's body being imaged, etc. In some embodiments, the display module <b>100</b> can be mounted onto a point of care cart, onto a clinic utility cart, or onto a variety of other surfaces in various other configurations, for example as disclosed in the '604 Application. In some embodiments, the display module <b>100</b> and the support member (e.g., articulated arm <b>125</b>) can be coupled together by a quick release mechanism that allows a user to quickly release the display module <b>100</b> from the support member (e.g., articulated arm <b>125</b>). Many variations are possible.</p>
      <p id="p-00128-en" num="0128">In some embodiments display module <b>100</b> can be configured to display images as heads up display images. A heads up display can be any display that presents images that can be viewed without the viewer having to look away from some usual viewpoints. For example, a heads up display can project an image onto an at least partially transparent surface <b>116</b>. A heads up display can also include a direct retinal display, where an image is projected onto the retina of the eye of a user. <figref>FIG. 3</figref> shows an ultrasound system with a heads up display module <b>100</b> configured to display an ultrasound image <b>106</b>A as a heads up display image. In some embodiments, a heads up display module <b>100</b> displays an image <b>106</b>A on a transparent or at least partially transparent surface <b>116</b>. The surface <b>116</b> (e.g., as shown in <figref>FIGS. 2A</figref>, <b>2</b>B, and <b>2</b>C) can be transparent or at least partially transparent.</p>
      <p id="p-00129-en" num="0129">As shown in <figref>FIG. 3</figref>, a user <b>110</b> can view an ultrasound image <b>106</b>A of imaging target <b>106</b> on the heads up display module <b>100</b> while still being able to maintain visual contact with a viewing target <b>107</b>. The viewing target <b>107</b> can be the imaging target <b>106</b>, the ultrasound probe <b>102</b>, a different medical device (not shown), another medical professional, etc. When viewing a heads up display module <b>100</b> showing an ultrasound image <b>106</b>A, a healthcare professional user <b>110</b> can have the advantage of being able to maintain visual contact with an imaging target <b>106</b> on a patient (or other visual targets important to treating the patient) while still viewing the ultrasound image <b>106</b>A. This can facilitate both more effective use of ultrasound probe <b>102</b>, and improved ability to respond to multiple healthcare needs of a patient.</p>
      <p id="p-00130-en" num="0130">In some embodiments, the display module <b>100</b> can be attachable to the user <b>100</b> at a variety of places, such as the arm, chest, head, or shoulder of the user <b>110</b>. A wearable heads up display module <b>100</b> can be configured to present an ultrasound image <b>106</b>A for a user <b>110</b> to view on a transparent or partially transparent surface <b>116</b>. A wearable heads up display module <b>100</b> can be head-mounted. In some embodiments, the heads up display module <b>100</b> can be worn as part of a helmet <b>118</b>, as part of a frame <b>120</b> as for glasses, or as part of a headband <b>122</b>. The display module <b>100</b> as shown in <figref>FIGS. 2A</figref>, <b>2</b>B, and <b>2</b>C can in some embodiments be a heads up display module <b>100</b>. If a heads up display module <b>100</b> is worn on the head of a user <b>110</b>, the user can maintain a view of the image <b>106</b>A regardless of the orientation of the user's head.</p>
      <p id="p-00131-en" num="0131">In some embodiments, a head-mounted heads up display module <b>100</b> can be configured to display an image appearing to be on an at least partially transparent surface <b>116</b> positioned such that when the heads up display module <b>100</b> is worn by a user <b>110</b> the surface <b>116</b> is near the eye of a user <b>110</b> and intersecting a straight ahead line of sight of the user <b>110</b>. In this way, a user <b>110</b> would be able to continuously monitor both the image <b>106</b>A and other important aspects of a patient during examination. This is particularly beneficial during emergency response patient treatment. In some embodiments, the surface <b>116</b> can be positioned to be substantially geometrically normal to a straight ahead line of sight for a person <b>110</b> wearing the wearable heads up display module <b>100</b>. In some embodiments, the angle between a line normal to the surface <b>116</b> and the straight ahead line of sight can be 30 degrees or less, 15 degrees or less, or 5 degrees or less.</p>
      <p id="p-00132-en" num="0132">The heads up display module <b>100</b> can display images visible at a variety of distances. In some embodiments the heads up display module <b>100</b> can be configured to display an image onto a surface <b>116</b> that is located near the eye of a user <b>110</b>. To be located near the eye of a user <b>110</b> can mean to be located within the distance that a lens in a frame for glasses would be located relative to the eye of a person wearing the glasses. In some embodiments, the display <b>100</b> can produce a virtual image viewable to the user (e.g., by projecting light directly on to the retina of the user's eye or by projecting light on the surface <b>116</b> to produce the virtual image). A retinal projector can be positioned near the user's eye. For example, the surface <b>116</b> or retinal projector can be positioned at a distance that is at least about 10 mm, at least about 25 mm, at least about 50 mm, or at least about 100 mm from the eye, and/or the surface <b>116</b> or retinal projector can be positioned at a distance that is less than or equal to about 300 mm, less than or equal to about 150 mm, less than or equal to about 100 mm, less than or equal to about mm, less than or equal to about 25 mm, or less than or equal to about 15 mm from the eye.</p>
      <p id="p-00133-en" num="0133">Heads up display module <b>100</b> can display the image <b>106</b>A in a variety of ways. In some embodiments, image <b>106</b>A can be produced by the projection of light on to an at least partially transparent surface <b>116</b> of the heads up display module <b>100</b>. In some embodiments an image <b>106</b>A can be scanned directly onto the retina of a user <b>110</b>.</p>
      <p id="p-00134-en" num="0134">The ultrasound system can be configured to provide the image <b>106</b>A as an overlay of imaging target <b>106</b>. For example, the image <b>106</b>A can be positioned such that it appears to be positioned over the imaging target <b>106</b>. <figref>FIG. 3A</figref> shows an example of some embodiments where displayed image <b>106</b>A is oriented to overlay the imaging target <b>106</b>. In some embodiments, image <b>106</b>A is an ultrasound image, and imaging target <b>106</b> is a body portion of a patient. A controller <b>104</b> (e.g., the image processor) can be configured to generate the image <b>106</b>A based at least in part on the position of the ultrasound probe <b>102</b> (e.g., based at least in part on the position of the ultrasound probe <b>102</b> relative to the display module <b>100</b>). If the user moves the ultrasound probe <b>102</b>, a different portion of the target area can be imaged by the ultrasound probe <b>102</b>, and the controller <b>104</b> can be configured to adjust the location of the image <b>106</b>A on the display <b>100</b> such that the image <b>106</b>A remains positioned over the target area <b>106</b> being imaged. If the user moves the display <b>100</b> (e.g., by moving user's head while the head-mounted display <b>100</b> is attached thereto), the controller can adjust the location of the image <b>106</b>A on the display <b>100</b> such that the image <b>106</b>A remains positioned over the target area <b>106</b> being imaged. The controller <b>104</b> can increase or decrease the size of the image <b>106</b>A to compensate for the display being moved closer to or further from the imaging target <b>106</b>.</p>
      <p id="p-00135-en" num="0135">The system can include a position sensor <b>115</b>, which can be configured to detect the position of the ultrasound probe <b>102</b> (e.g., relative to the display <b>100</b>). The position sensor <b>115</b> can include a light sensor (e.g., a visible light sensor or camera). For example, with reference to <figref>FIGS. 2A</figref>, <b>2</b>B, and <b>2</b>C a light sensor <b>115</b> can be supported by the head-wearable article (e.g., the helmet <b>118</b>, the eyewear frame <b>120</b>, or the headband <b>122</b>). The light sensor <b>115</b> can be configured to image an area in front of the user's eyes and/or in front of the display module <b>100</b>. The light sensor <b>115</b> can be coupled to the user <b>110</b> such that the light sensor <b>115</b> moves with the user <b>110</b> (e.g., with the user's head). One or more position indicators <b>117</b> can be located on the ultrasound probe <b>102</b>. The position sensor <b>115</b> can detect the one or more positions of the position indicators to determine the position of the probe <b>102</b> (e.g., relative to the light sensor <b>115</b> and/or display <b>100</b>). In some embodiments, the position indicators <b>117</b> can include dots, colored markings, lights, or other visual indicators. The light sensor <b>115</b> can detect the visual position indicators <b>117</b> for determining the position of the probe <b>102</b> (e.g., relative to the light sensor <b>115</b> and/or display <b>100</b>). In some embodiments, image data from the light sensor <b>115</b> can be transmitted (e.g., wirelessly or via a wire) to a controller <b>104</b>, and the controller can analyze the image data to identify the visual position indicators <b>117</b>. By comparing the relative positions of the visual position indicators in the image data from the light sensor <b>115</b>, the position of the ultrasound probe <b>102</b> relative to the light sensor <b>115</b> and/or display <b>100</b> can be determined. For example, if the visual position indicators <b>117</b> are closer together that can be an indication that the probe <b>102</b> is further away from the light sensor <b>115</b>, and if the visual position indicators <b>117</b> are further apart that can be an indication that the probe <b>102</b> is closer to the light sensor <b>102</b>. In some embodiments, two, three, or more position indicators <b>117</b> can be used. In some embodiments, the orientation of the probe <b>102</b> can be determined by comparing the relative positions of the visual indicators. The position of the probe <b>102</b> can include the location of the probe, the distance of the probe from another object (e.g., the display <b>100</b> or the user or the light sensor <b>115</b>), the orientation of the probe <b>102</b>, etc.</p>
      <p id="p-00136-en" num="0136">Many variations and additional features are possible. For example, the light sensor can be coupled to the ultrasound probe and the position indicators can be coupled to the display or to the wearable article. In some embodiments, the ultrasound probe <b>102</b> can include an accelerometer or other positioning sensor for determining the orientation or position of the ultrasound probe <b>102</b>. The system (e.g., the controller <b>104</b> or sensor <b>115</b>) can be configured to determine the position of the ultrasound probe <b>102</b> relative to the display, relative to a light sensor <b>115</b>, relative to a user, etc. In some embodiments, the dedicated position indicators <b>117</b> can be omitted and the position of the ultrasound probe <b>102</b> itself can be determined directly from the image data (e.g., using image or video analysis techniques).</p>
      <p id="p-00137-en" num="0137">In some embodiments, the display <b>100</b> is not a wearable (e.g., head-mounted) display. For example, the display <b>100</b> can be a transparent display that is configured to be positioned between the user and the image target <b>106</b>. The user can view the image target <b>106</b> by looking through the display <b>100</b>, and the display <b>100</b> can display an image overlaid over the image target <b>106</b>. In some embodiments, the controller <b>104</b> can generate the image and/or adjust the image based on the relative positions of the ultrasound probe <b>102</b>, the display <b>100</b>, and/or the user. For example, if any one of the probe <b>102</b>, the display <b>100</b>, and the user moves, the controller <b>104</b> can change the image <b>106</b>A on the display <b>100</b> such that the image <b>106</b>A remains overlaid over the image target <b>106</b>. The relative positions of the ultrasound probe <b>102</b>, the display <b>100</b>, and/or the user can be determined in a manner similar to the discussion of <figref>FIG. 3A</figref>. For example, a light sensor can be coupled to the user, and the display <b>100</b> and/or probe <b>102</b> can include position indicators.</p>
      <p id="p-00138-en" num="0138">
        <figref>FIG. 4A</figref> shows a schematic example of an ultrasound system where an ultrasound probe <b>102</b> can send data regarding an ultrasound signal and reflected echo signal from the imaging target <b>106</b> wirelessly to an image processor <b>104</b>. Wireless communication can be achieved through a communication module <b>108</b>, which in some embodiments may be part of the ultrasound probe <b>102</b>. The image processor <b>104</b> can then generate an ultrasound image of the imaging target <b>106</b> to be displayed by display module <b>100</b>. In some embodiments, the display module <b>100</b> and the image processor <b>104</b> can be supported by the wearable article. In some embodiments, the wearable article can include a communication module (not shown in <figref>FIG. 4A</figref>) that can be configured to receive information from the communication module <b>108</b>. <figref>FIG. 4B</figref> shows a schematic example of an ultrasound system where an image processor <b>104</b> receives data from the ultrasound probe <b>102</b> and wirelessly communicates the image to be displayed to display module <b>100</b>. Wireless communication is achieved through a communication module <b>108</b>, which can be part of the ultrasound probe <b>102</b> and/or part of the image processor <b>104</b>. The display module <b>100</b> may in some embodiments be wearable. Display module <b>100</b> may in some embodiments be a heads up display module <b>100</b>. Although not pictured, in some embodiments, wireless communication can be utilized both between the ultrasound probe <b>102</b> and the image processor <b>104</b>, and between the image processor <b>104</b> and the display module <b>100</b>. In some embodiments, the display module <b>100</b> can include or be coupled to a communication module (not shown in <figref>FIG. 4B</figref>) that can be configured to receive information from the communication module <b>108</b>. In some embodiments, wires or cables can be used to transfer information between the ultrasound probe <b>102</b>, the image processor <b>104</b>, and the display module <b>100</b>.</p>
      <p id="p-00139-en" num="0139">The communication modules <b>108</b> described herein can provide wireless communication in a variety of ways. In some embodiments, wireless communication can be achieved through a Bluetooth wireless communication link, a Wi-Fi or a wireless local area network (WLAN) communication link, a wireless connection to a cellular system, a commercial communications radio link, a military radio link, or combinations thereof. In some embodiments communication module <b>108</b> can include a separate processor from image processor <b>104</b>. In some embodiments image processor <b>104</b> and communications module <b>108</b> can involve the same hardware processor. In some embodiments communication module <b>108</b> is capable of both sending and receiving information wirelessly. In some embodiments communications module <b>108</b> can be a software component. In some embodiments, though not pictured in any figures, additional communication modules may be needed to achieve wireless communication.</p>
      <p id="p-00140-en" num="0140">Wireless communication at some stage between the ultrasound probe <b>102</b> and the display module <b>100</b> can allow more freedom to position the display module <b>100</b> relative to the imaging target <b>106</b>, as compared to an ultrasound system that communicates information from the ultrasound probe <b>102</b> to the display <b>100</b> via wires or cables. A healthcare professional using a display module <b>100</b> with wireless communication at some stage between the ultrasound probe <b>102</b> and the display module <b>100</b> can position the display module <b>100</b> (e.g., at a convenient location for effective use of the ultrasound probe <b>102</b> and analysis of imaging target <b>106</b>) without the restrictions or inconvenience of wired connections. In some embodiments, the ultrasound system can utilize both a wearable display module <b>100</b> and wireless communication at some stage between the display module <b>100</b> and the ultrasound probe <b>102</b>. In some embodiments an ultrasound system can utilize a heads up display module <b>100</b>, which may be wearable, as well as wireless communication at some stage between the display module <b>100</b> and the ultrasound probe <b>102</b>. Combining the advantages of a wearable display module <b>100</b>, a heads up display module <b>100</b>, and the use of wireless communications can further increase the convenience and flexibility for a user viewing an ultrasound image.</p>
      <p id="p-00141-en" num="0141">
        <figref>FIG. 5A</figref> shows a schematic example of an ultrasound system where information from an ultrasound probe <b>102</b> (e.g., the ultrasound signal and reflected echo signal from the imaging target <b>106</b>) can be transmitted to both a local display <b>100</b> and a remote display <b>101</b>. For example, the information can be transmitted wirelessly to both a local image processor <b>104</b> and a remote image processor <b>105</b>. Wireless communication can be achieved through a communication module <b>108</b>, which in some embodiments may be part of the ultrasound probe <b>102</b>. Local image processor <b>104</b> can generate an image to be displayed by local display module <b>100</b>, and remote image processor <b>105</b> can generate an image to be displayed on a remote display module <b>101</b>, which can be accessible to a remote medical professional (e.g., a doctor). Accordingly, the system can enable a local medical professional (e.g., an emergency medical technician) on site with the patient (e.g., at a scene of an accident or emergency) to view the ultrasound image on the local display <b>100</b> (e.g., for performing the ultrasound), and can enable a remote medical professional (e.g., a doctor) at a remote location (e.g., a hospital or doctor's office) to view the ultrasound image on the remote display <b>101</b> (e.g., for analyzing the ultrasound image, such as for diagnosis).</p>
      <p id="p-00142-en" num="0142">Local image processor <b>104</b> and remote image processor <b>105</b> can in some embodiments provide identical images for display in the respective display modules. In some embodiments remote image processor <b>105</b> can be configure to deliver a lower quality or higher quality image compared to local image processor <b>104</b>. If the local display module <b>100</b> has different specifications for image display than remote display module <b>101</b>, then the local image processor <b>104</b> and remote image processor <b>105</b> can, in some embodiments, provide different images of imaging target <b>106</b> to each respective display module in order to meet the different specifications of the display modules. Local image processor <b>104</b> may in some embodiments be less sophisticated or powerful than the remote image processor <b>105</b>. Different images can also be displayed in response to either different processing power or different viewing needs associated with local display module <b>100</b> compared with remote display module <b>101</b>. Although not pictured in <figref>FIG. 5A</figref>, wireless communication can in some embodiments be used between local image processor <b>104</b> and local display module <b>100</b>, between remote image processor <b>105</b> and remote display module <b>101</b>, or between both.</p>
      <p id="p-00143-en" num="0143">
        <figref>FIG. 5B</figref> shows a schematic example of an ultrasound system where image processor <b>104</b> generates an ultrasound image of imaging target <b>106</b>, and the ultrasound image can be wirelessly communicated by communication module <b>108</b> to local display module <b>100</b> and to remote display module <b>101</b>. Although not pictured in <figref>FIG. 5B</figref>, in some embodiments wireless communication can be utilized between the ultrasound probe <b>102</b> and image processor <b>104</b>. In some embodiments image processor <b>104</b> can deliver a single image to both local display module <b>100</b> and remote display module <b>101</b>. In some embodiments image processor <b>104</b> can deliver different images of imaging target <b>106</b> to the local display module <b>100</b> and the remote display module <b>101</b>. Different images may be delivered in order to satisfy different image display specifications or to meet different viewing needs at the remote display module <b>101</b> compared with the local display module <b>100</b>. For example, if a remote healthcare profession is attempting to provide treatment or diagnostic instructions to a local user imaging the imaging target <b>106</b>, then the remote healthcare professional may need to view an image in greater detail than the local user. Accordingly, in some embodiments, the image processor <b>104</b> can generate a first image for the local display <b>100</b> and a second image for the remote display, and the second image can have higher quality (e.g., higher resolution) than the first image.</p>
      <p id="p-00144-en" num="0144">In the embodiments described in connection with <figref>FIGS. 5A and 5B</figref>, local display module <b>100</b>, remote display module <b>101</b>, or both can be a wearable display module. In some embodiments local display module <b>100</b>, remote display module <b>101</b>, or both can be configured to display an ultrasound image as a heads up display, similar to the above discussed heads up display module <b>100</b>. A local display module <b>100</b> may be configured to be a wearable heads up display module <b>100</b> in order to facilitate quick and convenient viewing of an ultrasound image, while remote display module <b>101</b> may be a non-wearable display (e.g., a desktop display, mobile device display, laptop computer display, etc.). The remote display <b>101</b> can be larger than the local display <b>100</b>, e.g., in order to facilitate more detailed and thorough examination of the ultrasound image in a context where convenience of display is less important.</p>
      <p id="p-00145-en" num="0145">A remote healthcare professional can access the remote display module <b>101</b> and can provide diagnostic or treatment information to a local healthcare professional (e.g., the user wearing the local display <b>100</b>). By displaying the image in two locations better collaboration and medical decision making can be achieved. In the context of emergency response treatment, a remote doctor viewing an ultrasound image of imaging target <b>106</b> can provide medical or diagnostic information to a healthcare professional at the scene of the emergency that may not otherwise be available, an a patient can receive treatment at the scene of the emergency without being transported to a hospital or doctor's office.</p>
      <p id="p-00146-en" num="0146">In some embodiments a display module <b>100</b> may be configured to display images from multiple imaging sources. The <figref>FIG. 6A</figref> shows a schematic example of a system where a multi-image display module <b>100</b> can display images of imaging target <b>106</b> from a light sensor <b>114</b> and an ultrasound probe <b>102</b>. Imaging target <b>106</b> can be a target body portion of a patient or any target area.</p>
      <p id="p-00147-en" num="0147">Ultrasound probe <b>102</b> can emit ultrasound signals and can receive reflected echo signals from imaging target <b>106</b>. Based at least in part on data from ultrasound probe <b>102</b>, an ultrasound image processor <b>104</b> can generate an ultrasound image, which can be displayed on a display <b>100</b>. Light emitter <b>112</b> can emit light, which can be of a specific wavelength, onto the imaging target <b>112</b>. A light sensor <b>114</b> can be configured to detect light emitted by light emitter <b>112</b> that is reflected and/or scattered from imaging target <b>106</b>. Based on data from the light sensor <b>114</b>, a light image processor <b>103</b> can generate a light image indicating the features of imaging target <b>106</b> based on the light reflected and/or scattered by the imaging target <b>106</b>. In some embodiments a light image can be a near infrared light image (NIR image). Display module <b>100</b> can then display both the light image and the ultrasound image. In some embodiments, the display <b>100</b> can display the ultrasound image and the light image simultaneously (e.g., side-by-side or one above the other). In some embodiments, the display <b>100</b> can display the ultrasound image and the light image overlaid one image over the other, or a single image produced from both the ultrasound image and the light image. In some embodiments, a user interface <b>124</b> enables a user to select whether the display <b>100</b> shows the ultrasound image or the light image.</p>
      <p id="p-00148-en" num="0148">A light emitter <b>112</b> can in some embodiments be configured to emit near infrared (NIR) light. Reflection or scattering of NIR light can be used to show the locations of veins in the body portion of the patient. The light emitter <b>112</b> can be configured to emit light between about 600 nm and about 1000 nm, in some embodiments. The light emitter <b>112</b> can emit light that is configured to be absorbed by hemoglobin. The light sensor <b>114</b> can detect light that was reflected or scattered from imaging target <b>106</b> (and thus not absorbed by hemoglobin), and in some embodiments the NIR light image produced by light image processor <b>103</b> can illustrate distinct locations of hemoglobin in blood and the surrounding tissue. The NIR light image can be used to identify veins, to check patency of a vein, and/or or to identify infiltration or extravasation, for example as discussed in the '604 Application.</p>
      <p id="p-00149-en" num="0149">In some embodiments, the light emitter <b>112</b> can emit multiple wavelengths of light (e.g., as discussed in the '604 Application). For example, the light emitter <b>112</b> can comprise multiple different light emitters of varying types (e.g., LEDs) that are configured to emit different wavelengths (e.g., different lines) of light (e.g., see FIG. 3 of the '604 Application and the accompanying text). Although some embodiments are discussed as having three different light emitter types with three different wavelengths (e.g., three different lines) that produce three different image contributions, any number of light emitter types, wavelengths, and image contributions can be used (e.g., 2, 4, 5, 6, etc.). For example, 2, 3, or 4 types of LED sets can be used to emit light of different wavelengths ranging from about 700 nm to about 1000 nm, and in some embodiments, the LEDs can be pulsed or sequenced, as discussed herein. Various spectral outputs can be used. For example, the light emitters can have nominal wavelengths (e.g., lines or peaks) of about 740 nm, about 850 nm, and about 950 nm respectively. In some embodiments, a first light emitter can emit light having a peak at about 700 nm to about 800 nm (e.g., about 750 nm to about 760 nm). A second light emitter can emit light having a peak at about 800 nm to about 900 nm (e.g., about 850 nm to about 870 nm). A third light emitter can emit light having a peak at about 900 nm to about 1100 nm (e.g., about 940 nm to about 950 nm). In some embodiments, the spectral output of the light emitters can have bell curve (e.g., Gaussian) shapes. In some embodiments, the spectral output curves for the different light emitters can overlap each other. Light from the first light emitter can be used to produce a first image contribution of high quality but that reaches only a short distance into the tissue depth. Light from the second light emitter can be used to produce a second image contribution that has lower quality than the first image but reaches deeper into the tissue than the first image contribution. Light from the third light emitter can be used to produce a third image contribution that is able to reach deeper into the tissue than the first and second image contributions but has a lower quality than the first and second image contributions. In some embodiments some or all of the multiple light emitters can emit light with lines at wavelengths between about 1000 nm and about 2500 nm. In some embodiments, a single broadband NIR light source can be used instead of multiple distinct light source types. Various embodiments discussed herein relate to multiple light sources that emit light having different NIR spectral outputs (e.g., different wavelengths, different lines, and/or different peaks). A spectral line as described herein can relate to a portion of a spectral output that includes a peak and a trough, wherein the intensity of the peak is 2 times greater than the trough, 3 times greater than the trough, 4 times greater than the trough, or more.</p>
      <p id="p-00150-en" num="0150">In some embodiments, all light emitters comprising the light emitter <b>112</b> can be turned on at the same time so that the light from all three light emitters illuminates the target area simultaneously. Light of multiple wavelengths (e.g., multiple lines) can be reflected or scattered by the target area to the light sensor <b>114</b> to produce a single composite image that is a combination of the different image contributions. A user can in some embodiments adjust the relative intensity of the different light emitters. If light emitters of different wavelengths are being used in the light emitter <b>112</b>, a user may opt to increase the relative intensity of a certain light emitter in order to adjust the quality of the image versus the depth of penetration for the image. For example, a user may opt to increase the relative intensity of the light emitter having the largest wavelength in order to produce an image reaching deeper into tissue. Conversely, for example, a user may opt to increase the relative intensity of an emitter having a lower wavelength in order to produce an image with greater quality but lower penetration depth. A user can in some embodiments change the relative intensities through a user interface such as the user interface <b>124</b>. Increasing or decreasing the relative intensity of a light emitter can include turning one light emitter completely off.</p>
      <p id="p-00151-en" num="0151">
        <figref>FIG. 6C</figref> shows an example embodiment of a medical imaging system.</p>
      <p id="p-00152-en" num="0152">The medical imaging system can include a display module <b>100</b> and user interface <b>124</b>. In some embodiments, the system can include a touchscreen that is configured to display images and operate as the display module <b>100</b> and also is configured to receive touch input from a user to operate as the user interface <b>124</b>. The user interface can include various user input elements. The user interface <b>124</b> can include one or more user input elements <b>602</b> for adjusting settings of the NIR light source(s) that make up the light emitter <b>112</b>. For example, if the light emitter <b>112</b> includes three NIR light sources, the user interface <b>124</b> can include three user input elements <b>602</b> (e.g., sliders) for adjusting the intensity of light output by the three light sources. The user interface <b>124</b> can include a locking user input element <b>604</b> which can receive input to lock or unlock the user input elements <b>602</b> (e.g., the sliders) that control the intensity of the light sources. When the locking user input element is unlocked, the relative intensity of the light sources can be adjusted (e.g., by moving the slider user input elements <b>602</b>). When the locking user input element is locked, in some embodiments, the intensity of the NIR light sources of the light emitter <b>112</b> can be adjusted together as a group (e.g., such that adjusting one of the slider user input elements <b>602</b> causes all the slider user input elements <b>602</b> to move together). The one or more user input elements <b>602</b> can be used to adjust various parameters (e.g., duty cycle, frequency modulation, amount of time that each light source is on, and/or brightness) of the NIR light sources to vary the optical illumination provide by the different light sources. In some embodiments, the user interface can include a visible light adjustment input element <b>606</b>, which can enable the user to adjust settings of the visible light source <b>113</b> (e.g., the color and/or intensity). The user interface <b>124</b> can include an activation user input element <b>608</b>, which can cause NIR imaging to start. The user interface <b>124</b> can include a pause user input element <b>610</b>, which can stop the NIR imaging and/or hold the last image captured on the display <b>100</b>. Various other user input elements can be included to enable the user to perform the various operations disclosed herein and in the '604 Application. By way of one example, the user interface <b>124</b> can include a store image input element (not shown in <figref>FIG. 6C</figref>), which can store an NIR image, as discussed herein. The user interface <b>124</b> can include a home or return user input element <b>612</b>, which can cause the user interface to change to a different interface (e.g., an interface with user input elements for selecting NIR and/or ultrasound imaging). The user interface <b>124</b> can include one or more display adjustment input elements <b>614</b> for adjusting settings of the display <b>100</b> (e.g., contrast, brightness, color, etc.).</p>
      <p id="p-00153-en" num="0153">The medical imaging system can include a housing <b>616</b>, which can support the display <b>100</b> and/or other components of the system. The system can include one or more light sources <b>618</b> (e.g., the NIR light emitter(s) <b>112</b> and/or the visible light source <b>113</b>) and a light sensor <b>620</b> for NIR imaging. The system can include one or more input modules <b>622</b> (e.g., a wireless input module or an electrical port such as a USB port) configured to receive input from a medical instrument (e.g., an ultrasound probe <b>624</b>, spectrometer, etc.) as discussed herein. The system can include a coupling mechanism <b>126</b> configured to mechanically couple the medical instrument (e.g., the ultrasound probe <b>624</b>) to the housing <b>616</b>. The coupling mechanism <b>126</b> can be a clip, a pocket, a slide engagement, etc. The coupling mechanism <b>126</b> facilitate rapid and easy transition from using the NIR imaging system (e.g., the light source <b>618</b> and sensor <b>620</b>) to using the medical instrument (e.g., the ultrasound probe <b>624</b>).</p>
      <p id="p-00154-en" num="0154">In some embodiments, the light emitters can be pulsed in sequence with the light sensor <b>114</b> (e.g., synchronized with a shutter of the camera), so that the light emitters are turned off when the light sensor <b>114</b> is not generating an image and so that the light emitters are turned on when the light sensor <b>114</b> is generating an image. In some cases, the pulsing of the light emitters can be synchronized with the shutter of the camera so that the light emitters are turned on when the shutter is open and turned off when the shutter is closed. Turning the light emitters off when not needed can reduce power usage and heat buildup. In some embodiments, a light emitter <b>112</b> that includes only a single light emitter, or light emitters of all substantially the same wavelength, or of different wavelengths, can be pulsed at a rate that corresponds to an imaging rate of the light sensor <b>114</b>.</p>
      <p id="p-00155-en" num="0155">In some embodiments, the light emitters can be pulsed sequentially. For example, at a first time, the first light emitter can be turned on while the second and third light emitters are turned off, and the light sensor <b>114</b> can generate a first image at the first time using the light from the first light emitter. At a second time, the second light emitter can be turned on while the first and third light emitters are turned off, and the light sensor <b>114</b> can generate a second image at the second time using the light from the second light emitter. At a third time, the third light emitter can be turned on while the first and second light emitters are turned off, and the light sensor <b>114</b> can generate a third image at the third time using the light from the third light emitter. Additional images can be generated by additional light emitters of different wavelengths, depending on the number of different wavelengths utilized. In some embodiments, the system can perform different image processing on the different images that correspond to the respective different wavelength lines (e.g., from the different light sources). For example, in some embodiments, a first image processing procedure (e.g., a filter or look-up-table conversion) can be performed on the first image that is optimized or otherwise configured for use with the spectrum of light output by the first light source. A second image processing procedure (e.g., a filter or look-up-table conversion) can be performed on the second image that is optimized or otherwise configured for use with the spectrum of light output by the second light source. A third image processing procedure (e.g., a filter or look-up-table conversion) can be performed on the third image that is optimized or otherwise configured for use with the spectrum of light output by the third light source. By way of example, the look-up-table conversion can convert the raw pixel values from the imaging sensor into a different set of values (e.g., for use in the displayed image). The look-up-table can remap the grayscale levels of the pixels or colorize the image, for example. The different images can be displayed on the display module <b>100</b> in rapid succession (e.g., interlaced) so that the images combine to form a composite image of all three images to the human eye. Similarly, the different images can be stored in memory and then combined by the imaging system to form a composite image, which may be displayed on the display module <b>100</b> to the user. Optionally, a control may be provided enabling the user to instruct the imaging system via a user interface <b>124</b> to display each image individually and/or to display a composite image including images selected by the user.</p>
      <p id="p-00156-en" num="0156">Pulsing the light emitters sequentially can allow for more light of each wavelength to be used. For example, if all three light emitters are turned on together, the amount of light emitted by each light emitter may need to be limited or reduced to avoid overpowering the light sensor <b>114</b>. However, if the light emitters are pulsed sequentially, more light of each wavelength can be used since the light is not combined with the other wavelengths of light from the other light emitters. By illuminating the target area with more light of each of the three light emitters, the quality and/or imaging depth of the produced image can be improved. In some sequentially pulsing embodiments, the light sensor <b>114</b> can be configured to capture images at a faster rate (e.g., 60 hz or 90 hz) than would be needed in embodiments in which the light emitters are turned on together, since the different image portions are captured separately. In some embodiments, the light sensor <b>112</b> can include multiple light sensor portions (e.g., as subpixels of the light sensor <b>112</b>) configured to synchronize with the multiple light emitters that are pulsed in sequence. In some embodiments, different light sensors can be used for the different wavelengths of light and can be configured to synchronize with the pulsing of the multiple light emitters.</p>
      <p id="p-00157-en" num="0157">The composite image that includes the three image portions can provide the benefits of all three image portions to the user simultaneously, without requiring that the user toggle between the different wavelengths of light. When the user wants to observe a feature that is relatively deep in the tissue, the user can focus on the third image portion of the composite image, which is produced using the longer wavelength NIR light. When the user wants to observe high quality detail of a feature that is relatively shallow in the tissue, the user can focus on the first image portion of the composite image, which is produced using the shorter wavelength NIR light. Although the presence of the third image portion can degrade the quality of the first image portion to some degree, it is expected that the human mind is able to focus on the desired portions of the image while deemphasizing the other portions of the image. Various embodiments disclosed herein can utilize a light emitter <b>112</b> that is configured to pulse, as discussed herein, and can include multiple light emitters for producing images with different wavelengths of light, even where not specifically mentioned in connection with the specific embodiments.</p>
      <p id="p-00158-en" num="0158">In some embodiments a visible light source <b>113</b> can be incorporated in the imaging system. The visible light source <b>113</b> can produce visible light that illuminates the same general area as the light emitter <b>112</b> (e.g., the imaging target <b>106</b>). In some embodiments the visible light source <b>113</b> provides visible illumination of the imaging target <b>106</b> to better facilitate inspection or interaction with the imaging target <b>106</b>, such as insertion of a needle at the imaging target <b>106</b>. The visible light source <b>113</b> can also provide a visible cue that other potentially non-visible light is being emitted by light emitter <b>112</b>. Absent any visible light, non-visible light (e.g., near infrared light) from the light emitter <b>112</b> could potentially shine continuously in a person's eye without that person noticing the exposure. Thus, a visible light source <b>113</b> could provide a desirable safety feature. The visible light from the visible light source <b>113</b> can be of any color or wavelength, and in some embodiments the light is green, blue, red, white, etc. In some embodiments the visible light source <b>113</b> can emit broadband (e.g., white) visible light. In some embodiments the visible light source <b>113</b> can be a colored light source (e.g., a red or orange light source), which can emit visible light having a wavelength (e.g., red or orange light) that is configured to facilitate the identification of blood vessels (e.g., veins) in the imaging target <b>106</b> (e.g., in the body tissue of a patient) with the naked eye. For example, the visible light can have a wavelength (e.g., red or orange light) that is absorbed more by the blood vessels (e.g., by the hemoglobin in the blood) than by the surrounding tissue, such that illumination of the target area with the visible light can facilitate the identification of the blood vessels (e.g., veins), for example, with the naked eye.</p>
      <p id="p-00159-en" num="0159">Although various embodiments are disclosed in connection with accessing a patient's vein (e.g., by inserting a needle or other medical implement into the vein) and/or imaging a patient's veins, the systems and method can also be applied to imaging and accessing other blood vessels as well.</p>
      <p id="p-00160-en" num="0160">The capability in some embodiments to present multiple medical images for viewing on a single display module <b>100</b> can provide a healthcare professional with improved ability to analyze an imaging target <b>106</b>. A light image utilizing near infrared (NIR) light can provide a healthcare professional with a view of veins in imaging target <b>106</b>. An ultrasound image can show a subcutaneous cross-sectional view of imaging target <b>106</b>. When viewed together or in sequence, the NIR light image and the subcutaneous cross-sectional view (e.g., ultrasound image) of the imaging target <b>106</b> can provide complimentary information to a healthcare professional. By providing healthcare professionals with improved access to multiple medical images from different sources, a medical imaging system can facilitate quicker action by healthcare professionals. Accordingly, some embodiments can be particularly valuable in an emergency response context where time is of the essence.</p>
      <p id="p-00161-en" num="0161">In some embodiments, the ultrasound image can be used to calibrate and/or validate the NIR light image. The combined use of an ultrasound image in addition to a light image (e.g., an NIR light image) can in some embodiments be used to determine the inner diameter and outer diameter of a blood vessel. The light image can show the transverse path of a blood vessel (e.g., along a generally planar region that is generally perpendicular to the direction of the light emitted from the light emitter <b>112</b> onto the imaging target <b>106</b>). One or more ultrasound images can provide a cross sectional image of the blood vessel (e.g., along a direction that can be generally perpendicular to the generally planar region shown in the light image). The inner diameter and/or outer diameter of the blood vessel can then be determined from one or more ultrasound images. For example, in some embodiments, the system can enable the user to identify two or more locations associated with the vein (e.g., the outer boundaries defining the outer diameter of the vein or the inner boundaries defining the inner diameter of the vein) on the ultrasound image, and the system can be configured to determine the physical diameter based at least in part by the two or more locations identified by the user. In some embodiments, the system can be configured to automatically identify the inner and/or outer boundaries of the vein in the ultrasound image (e.g., using edge detection), and the system can automatically determine the inner diameter and/or outer diameter of the vein. Knowing a blood vessel's inner and outer diameters can be useful to a medical professional to assess the vein and to select the proper catheter. In some embodiments, the system can be configured to provide a recommendation of an appropriate catheter or appropriate catheter size based at least in part on the diameter of the vein.</p>
      <p id="p-00162-en" num="0162">In some embodiments light imaging (e.g., the NIR imaging) can be used to determine the blood vessel's inner diameter and/or outer diameter without using an ultrasound image. For example, the system can enable the user to identify the inner and/or outer boundary of the vein. In some embodiments, edge detection algorithms can be used on the light image to identify the inner and/or outer boundaries of the vein in the NIR image. The system can be configured to determine the inner diameter and/or outer diameter of the vein from the identified inner and/or outer boundaries identified in the NIR image. In some embodiments, a reference object (e.g., a needle or other object) can be used to facilitate the size determinations (e.g., vein size) made by the system. The reference object can have a predetermined size, or it can have markers that are space apart by a predetermined distance, and the reference object can be positioned such that it will be imaged by the system so that the image of the reference object can be used by the system to make size determinations of other objects that are being imaged. In some embodiments, multiple imaging methods can be used together to determine size measurements (e.g., inner diameter and/or outer diameter of a vein), such as NIR imaging and/or ultrasound imaging.</p>
      <p id="p-00163-en" num="0163">The display module <b>100</b> can display more than one image at a single time. In some embodiments, the multi-image display module <b>100</b> may display both a light image (e.g., an NIR image) and an ultrasound image of a single imaging target <b>106</b> in close proximity at a single time. A user viewing an image on the multi-image display module <b>100</b> can, in some embodiments, alter the portion (e.g., the ratio) of the display <b>100</b> attributed to any displayed image (e.g., by proving input via the user interface <b>124</b>). For example, the system can be configured to respond to user input received by the user interface <b>124</b> to change the output of the display <b>100</b> from a relatively large NIR image to a relatively small NIR image and/or from a relatively small ultrasound image to a relatively large ultrasound image. In some embodiments multiple images can be viewed in sequence on the multi-image display module <b>100</b>, and in some embodiments a user can control what image is displayed at a given time (e.g., via input provided to the user interface <b>124</b>). In some embodiments, the multi-image display module <b>100</b> can display a single image at a time. The display module <b>100</b> can display a light image and an ultrasound image side-by-side or one above the other. The display module <b>100</b> can display the light image and the ultrasound image overlaid one over the other, or a single composite image generated from both the light image and the ultrasound image. Multiple images of an imaging target <b>106</b> from multiple inputs such as from a light sensor <b>114</b> and an ultrasound <b>102</b> may be overlaid to display a composite image of target <b>106</b>. A user can in some embodiments select through user interface <b>124</b> whether to display input from multiple sources as a single overlaid image or as separate images in close proximity (i.e. side-by-side). In some embodiments a user can select the focus or balance between overlaid images.</p>
      <p id="p-00164-en" num="0164">The display module <b>100</b> can, in some embodiments, display other types of medical images (e.g., at a single time) such as X-Ray images or MRI images, or other types of medical data such as a patient's temperature, heart rate, oxygen saturation, blood pressure, etc. Additional images or medical data can in some embodiments be displayed in combination with an ultrasound image, a light image, both, or neither. In some embodiments, a multi-image display module <b>100</b> can display three or four or more medical images in close proximity at a single time, or at different times (e.g. in response to user input). As shown in <figref>FIG. 6B</figref>, in some embodiments a sensor <b>126</b> can be incorporated in a medical imaging system. The sensor <b>126</b> can be connected to a sensor processor <b>128</b>, which transforms the information collected by sensor <b>126</b> into an image or readable data to be displayed by the display module <b>100</b>. While only a single additional sensor <b>126</b> is pictured in <figref>FIG. 6B</figref>, in some embodiments two or more sensors <b>126</b> can be incorporated in the imaging system in combination with a light reflection sensor, an ultrasound probe, both, or neither.</p>
      <p id="p-00165-en" num="0165">The sensor <b>126</b> can be any sensor that provides medical data or imaging. In some embodiments sensor <b>126</b> can be the leads for an electrocardiogram (EKG or ECG). In such embodiments, the output from the EKG leads can be processed by the sensor processor <b>128</b> and provided on the display module <b>100</b> (e.g., in combination with a light reflection image, an ultrasound image, both, or neither). In some embodiments the sensor <b>126</b> can be a spectrometer, which can detect information from a target area based on the spectrum of light reflected from the target area. For example, the spectrometer can provide information regarding the chemical composition of the target area <b>106</b> based on the spectrum of light that is returned to the spectrometer. In some embodiments, the spectrometer can include one or more light sources configured to emit light (e.g., broadband white light) onto the imaging target <b>106</b> such that the emitted light can be reflected from the imaging target to be received by the sensor <b>126</b> of the spectrometer. The system can automatically turn off the light emitter <b>112</b> and/or the visible light source <b>113</b> during operation of the spectrometer that includes its own light source. In some embodiments, the spectrometer can be configured to operate by receiving light from the light emitter <b>112</b> (e.g., NIR light) and/or light from the visible light source <b>113</b> that is reflected from the imaging source <b>106</b> onto the sensor <b>126</b>. In some embodiments, the system can be configured to use different settings (e.g., intensities) for the light emitter <b>112</b> and/or visible light source <b>113</b> during NIR imaging than during operation of the spectrometer, and the system can be configured to automatically change the settings (e.g., intensities) of the light emitter <b>112</b> and/or visible light source <b>113</b> depending on whether the system is operating the NIR imaging system or the spectrometer. In some embodiments, the data provided by the spectrometer can depend on the output of the light emitter <b>112</b> and/or visible light source <b>113</b>, which can be adjustable by the user. In some embodiments, the system can be configured to override user commands relating to the light emitter <b>112</b> and/or the visible light source <b>113</b> when the sensor <b>126</b> of the spectrometer is collecting light. For example, if the user sets custom intensities for NIR imaging using the three light sources that make up the light emitter <b>112</b>, the system can be configured to use the custom intensities for the three light sources during NIR imaging, and the system can change one or more of the intensities of the three light sources to values adapted for use with the spectrometer during operation of the spectrometer. In some instances, the NIR imaging and the spectrometer operation can be performed in rapid succession with the light intensities changing rapidly between the NIR imaging settings and the spectrometer settings, such that the user perceives that the NIR imaging and spectrometer operation occur simultaneously. In some embodiments, the spectrometer can operate while the light from the light emitter <b>112</b> and/or the visible light source <b>113</b> are on, and the processing of the data from the spectrometer can compensate for the light from the light emitter <b>112</b> and/or the visible light source <b>113</b>. In various embodiments, the spectrometer can comprise a wavelength dispersion element such as a diffraction grating or a prism that disperses light into different wavelengths that can be detected by a light sensor or light sensor array to determine the spectral composition of the received light. Further example embodiments can include the one or more sensors <b>126</b> comprising part of: an X-Ray emitter and detector; a magnetic resonance imaging (MRI) device; a pulse oximeter; a blood pressure monitor; a digital stethoscope; a thermometer; an otoscope; and/or an examination camera.</p>
      <p id="p-00166-en" num="0166">In some embodiments wireless communication can occur between the sensor <b>126</b> and the sensor processor <b>128</b>, or among the sensor <b>126</b>, the sensor processor <b>128</b> and any other component of the imaging system. The system can include a wireless input module configured to receive input wirelessly from the sensor <b>126</b>. In some embodiments, the ultrasound probe <b>102</b> and/or the sensor <b>126</b> (e.g., the EKG or ECG device, the spectrometer, the X-Ray device, the MRI device, the pulse oximeter, the blood pressure monitor, the digital stethoscope, the thermometer, and/or the otoscope) can be coupled to the system by a detachable cable (e.g., via an input module such as a USB port), for example, as disclosed in the '604 Application. Accordingly, the system can provide a single, portable, modular system that can be a platform for various types of medical imaging and medical data collection and display.</p>
      <p id="p-00167-en" num="0167">In some embodiments the system can be used to facilitate insertion of a peripherally inserted central catheter (PICC). A PICC can be inserted in a patient's peripheral vein and then advanced toward the patient's heart until the tip rests in the distal superior vana cava, cavoatrial junction, or other suitable location. The location of the tip can be confirmed by X-Ray imaging or by a signal from an EKG. An ultrasound image, a light reflection image (e.g., NIR image), or both can be used to facilitate location and insertion of a PICC into a peripheral vein. In some embodiments a sensor <b>126</b> can be configured to confirm that the tip of the PICC has reached the desired location, for example the sensor <b>126</b> can be a set of EKG leads or an X-Ray emitter and detector such as used in fluoroscopy. The system can provide a single imaging system to facilitate both the initial insertion of the catheter into the peripheral vein and the advancement of the catheter to the desired location in the patient's body.</p>
      <p id="p-00168-en" num="0168">The display module <b>100</b> can display medical images or data from multiple sources in a variety of manners, such as: in sequence, in close proximity on a single screen or on multiple screens, substantially overlaid onto a single image, or any combination thereof. In some embodiments a user can select through the user interface <b>124</b> which medical images or data to display on display module <b>100</b>, and in which manner. For example, a user could select to display an overlaid ultrasound image and light reflection image (e.g., NIR image) with the patient's temperature and readings from an EKG both displayed in close proximity to the overlaid image.</p>
      <p id="p-00169-en" num="0169">In some embodiments the display module <b>100</b> can be coupled to or integrated within a single housing with some or all other components of the system. The housing can in some embodiments encompass one or more of: a display module <b>100</b>; a user interface <b>124</b>; a light reflection image processor <b>103</b>; a light sensor <b>114</b>; a light emitter <b>112</b>; a visible light source <b>113</b>; an ultrasound image processor <b>104</b>; a port for coupling an ultrasound probe <b>102</b>; one or more sensor processor <b>128</b> such as a processor for an EKG, a spectrometer, or other sensor; and/or one or more ports for receiving one or more medical devices, such as one or more sensors <b>126</b> such as an EKG lead input, a spectrometer, or other sensors. In some embodiments the display module <b>100</b> can be a touchscreen capable of both facilitating the user interface <b>124</b> and providing images of the imaging target <b>106</b>. In some embodiments one or more hardware processors serving as any of the ultrasound processor <b>104</b>, the light reflection image processor <b>103</b>, or the additional sensor processor <b>128</b> can be integrated with the display module <b>100</b> and the user interface <b>124</b> in a single housing. In some embodiments, a single hardware processor can be used to perform the operations of the processors <b>103</b>, <b>104</b>, and <b>128</b>. Multiple components can be integrated within the same housing in some embodiments through permanent connections within the housing or in some embodiments through detachable wired connections such as through a USB port. In some embodiments light sensor <b>114</b> and light emitter <b>112</b> can be permanently integrated into a housing with the display module <b>100</b> and the user interface <b>124</b>, while an ultrasound probe <b>102</b> is removably connected to the housing and other components through a wired connection such as a USB port. Accordingly, the NIR imaging system, the ultrasound imaging system, and/or the other medical imaging and data collection devices discussed herein can be implemented into a single platform (e.g., utilizing the same processor, the same display, the same user interface, the same memory, the same communication module, and/or the same file system). A housing containing the display module <b>100</b> can be mountable onto an articulating arm <b>125</b>, which may or may not be slidably coupled to a vertical support member <b>127</b>.</p>
      <p id="p-00170-en" num="0170">The multi-image display module <b>100</b> can be wearable. A wearable multi-image display module <b>100</b> can in some embodiments be mounted or attached to any part of a user, such as to the user's arm, chest, head, or shoulders. In some embodiments, a multi-image display module <b>100</b> can also be configure to display images as a heads up display. A multi-image heads up display module <b>100</b> can display multiple images on an at least partially transparent surface or the images can be projected directly onto the retina of the user's eye. The wearable display <b>100</b> can maintain the displayed image(s) in the field of view of the user regardless of movement of the user's head. In some embodiments a display module <b>100</b> can be any combination of one or more of the following: wearable, a heads up display module <b>100</b>, or a multi-image display module <b>100</b>. In some embodiments the multi-image display module <b>100</b> can be mountable onto an articulating arm <b>125</b>, which may or may not be slidably coupled to a vertical support member <b>127</b>.</p>
      <p id="p-00171-en" num="0171">In some embodiments, the ultrasound probe <b>102</b> is in wireless communication with the ultrasound image processor <b>104</b>, and/or the light sensor <b>114</b> is in wireless communication with the light image processor <b>103</b>. In some embodiments, wireless communication can occur between the ultrasound probe <b>102</b> and ultrasound image processor <b>104</b>, and wired communication can occur between the light sensor <b>114</b> and the light image processor <b>103</b>. In some embodiments, wireless communication can occur between the light sensor <b>114</b> and the light image processor <b>103</b>, and wired communication can occur between the ultrasound probe <b>102</b> and ultrasound image processor <b>104</b>.</p>
      <p id="p-00172-en" num="0172">In some embodiments, the light sensor <b>114</b> and/or the ultrasound probe <b>102</b> can in some embodiments wirelessly communicate with one or more remote image processors configured to display the image(s) on a remote multi-image display module (e.g., in a manner similar to <figref>FIGS. 5A</figref> and/or <b>5</b>B). A remote image processor can be configured to generate an identical image or images of imaging target <b>106</b> as displayed on a local multi-image display module <b>100</b>. In some embodiments a remote image processor can generate an image or images meeting different parameters from the image or images displayed on a local multi-image display module <b>100</b>.</p>
      <p id="p-00173-en" num="0173">In some embodiments, an ultrasound image processor <b>104</b> and/or a light image processor <b>103</b> are in wireless communication with a multi-image display module <b>100</b>. In some embodiments, only one of ultrasound image processor <b>104</b> or light image processor <b>103</b> are in wireless communication with the multi-image display module <b>100</b>. Ultrasound image processor <b>104</b> and light image processor <b>104</b> can wirelessly communicate with a remote multi-image display module (e.g., similar to <figref>FIGS. 5A</figref> and/or <b>5</b>B). In some embodiments a remote multi-image display module can display identical images as a local multi-image display module <b>100</b>. A remote multi-image display module can be configured to display a more or less detailed set of images compared to a local multi-image display module <b>100</b>.</p>
      <p id="p-00174-en" num="0174">Although pictured as separate processors in <figref>FIG. 6A</figref>, ultrasound image processor <b>104</b> and light image processor <b>103</b> can be either partially or entirely the same processor, in some embodiments. A single processor can act as both the ultrasound image processor <b>104</b> and the light image processor <b>103</b>. Ultrasound image processor <b>104</b> and light image processor <b>103</b> can be separate hardware processors, in some embodiments. In some embodiments, two separate communication modules or a single communication module can facilitate the wireless communication of the ultrasound image processor <b>103</b> and the light image processor <b>103</b>. In some embodiments, additional communication modules may be utilized for wireless communications. In some embodiments at least one communication module can be integrated into any of the following: the ultrasound probe <b>102</b>, the light sensor <b>114</b>, the ultrasound image processor <b>104</b>, the light reflection image processor <b>103</b>, or the multi-image display module <b>100</b>. Similarly, the additional sensor processor <b>128</b> pictured in <figref>FIG. 6B</figref> can in some embodiments be incorporated partially or entirely in the same processor as ultrasound image processor <b>104</b>, light image processor <b>103</b>, or both. Wireless communication with an additional sensor processor <b>128</b> and/or sensor <b>126</b> can be facilitated by one or more communication modules.</p>
      <p id="p-00175-en" num="0175">In some embodiments, a user can select what image or images to display on a multi-image display module <b>100</b> through user interface <b>124</b>. In some embodiments a user can direct multi-image display module <b>100</b> to display a light image (e.g., an NIR image), an ultrasound image, or both. The user interface can provide a user input elements for selecting to view both images at once on the same display, for selecting to view the ultrasound image, and/or for selecting to view the light image (e.g., the NIR image). User interface <b>124</b> can provide a user with additional options such as altering the size or position of the image or images. A user interface <b>124</b> can be utilized in combination with any other discussed image display module <b>100</b>, including: a wearable image display module such as shown in <figref>FIG. 2A</figref>, <b>2</b>B, or <b>2</b>C; a heads up image display module <b>100</b>; or a remote image display module <b>100</b>. In some embodiments, wireless communication can be utilized between user interface <b>124</b> and any other component. The user interface <b>124</b> can include buttons, switches, a touchscreen, or various other types of user input elements. In some cases the display <b>100</b> can include the user interface <b>124</b>.</p>
      <p id="p-00176-en" num="0176">
        <figref>FIG. 7</figref> shows a flowchart of an example for how a multi-image display module <b>100</b> can display an image or images in accordance with user instructions (e.g., received by user interface <b>124</b>). The multi-image display module <b>100</b>, or a controller configured to send one or more images to the display module <b>100</b>, can receive an ultrasound image, a light reflection image, or both. The, multi-image display module <b>100</b>, or a controller, can determine what user input options are available and display the available user input options. A user can supply image parameter input and/or an image selection (e.g., to the user interface <b>124</b> or to the multi-image display module <b>100</b>). Multi-image display module <b>100</b> can display the selected image or images in conformance with the image parameter input. If the user selects to display both images, then user can in some embodiments select whether to display the images side-by-side or overlaid. After displaying the selected image or images, the process can be continued so that a user can input new parameters or selections.</p>
      <p id="p-00177-en" num="0177">The order of the steps shown in <figref>FIG. 7</figref> can be changed. For example, in some embodiments, either determining what user input options are available, or receiving user image selections can be performed before receiving the ultrasound image or the light reflection image. Also, some or all of the steps shown in <figref>FIG. 7</figref> can in some embodiments be performed by user interface <b>124</b> or any other processor instead of by multi-image display module <b>100</b>. In addition to the other steps show, one or more separate steps of communicating instructions to an image processor, the light sensor, the light emitter, or the ultrasound probe may also be performed. In some embodiments, portions of the method discussed in connection with <figref>FIG. 7</figref> can be omitted. For example, in some embodiments, the user interface can be static and the determination of the available user input options can be omitted. In some embodiments, the user interface can enable the user to select one or more images but does not enable the user to provide input relating to image parameters. Although the flowchart shown in <figref>FIG. 7</figref> illustrates the user selecting between the light image (NIR), the ultrasound image, or both, the user interface can similarly enable the user to toggle between or select various other images and/or data for display.</p>
      <p id="p-00178-en" num="0178">Information relating to attempts to locate a vein using NIR light can be recorded by the imaging system automatically or in response to a user command received by the user interface <b>124</b>. For Example, the system can be configured to store NIR images (e.g., on non-transitory computer-readable storage, in a patient file, in a hospital information system (HIS), electronic medical record (EMR), or electronic health record (HER)), as discussed, for example, in the '604 Application. For example, the system can include a communication system that can communicate with a database (e.g., on the HIS) for storing NIR images and associations between the NIR images and the associated patients and medical procedures.</p>
      <p id="p-00179-en" num="0179">
        <figref>FIG. 8</figref> shows a flowchart of an example of a process for performing and recording attempts to locate a vein or venous access site using NIR imaging and ultrasound imaging. A medical practitioner can image the target area using NIR light, such as disclosed herein and/or in the '604 Application. For example, in some embodiments, multispectral NIR imaging can be used (e.g., using a plurality of NIR light sources having different NIR emission spectra) to image the target area. If a satisfactory vein is identified using the NIR imaging system, the medical practitioner can use the identified vein to access the patient's vasculature (e.g., to insert a needle, IV, or catheter). In some embodiments, the system can store one or more images from the NIR imaging system. For example, the medical practitioner can provide a user command (e.g., via the user interface <b>124</b>) to capture an NIR image of the identified vein before and/or after accessing the vein (e.g., with the needle, IV, or catheter). The system can document the condition of the identified vein before and/or after being accessed, which can facilitate evaluation and training of medical practitioners and can provide evidence that can facilitate defense of a claim of medical misconduct or mistake. If a satisfactory vein is not identified using the NIR imaging system, the system can store an indication that an attempt was made to identify a suitable vein using NIR imaging. For example, the system can store an NIR image, which can be indicative of the medical practitioner's “best effort” to identify a suitable vein using NIR imaging. The medical practitioner can provide a command (e.g., via the user interface <b>124</b>) to store an NIR image that shows the target area and does not show a vein or a venous access site sufficient for the needle, IV, catheter, etc. In some cases, multiple images can be stored. For example, the system can store multiple images (e.g., a video) that document the medical practitioner's attempts to use NIR imaging to identify a satisfactory vein. The NIR image(s) can be stored in local or remote non-transitory computer-readable storage, in a patient file, in a hospital information system (HIS)), etc. such as described in the '604 Application. In some embodiments, the system can store meta data with the NIR image, and the meta data can include information regarding the settings of the system during the imaging attempt, the date and time of the imaging attempt, a patient identifier, a medical practitioner identifier, etc. The system can enable the user to add notes (e.g., via the user interface <b>124</b>) that are associated with the imaging attempt and/or the stored NIR image(s). If NIR imaging is not successful, the medical practitioner can transition to using ultrasound imaging to identify a suitable vein. Ultrasound imaging can be more costly than NIR imaging and can require patient contact (e.g., using the ultrasound probe), whereas the NIR imaging can be performed without patient contact. Once a suitable venous access site is identified using ultrasound imaging, the medical practitioner can use the identified vein to access the patient's vasculature (e.g., using a needle, IV, catheter, etc.). The system can store an association between the NIR image and the patient or the ultrasound procedure. In some embodiments, the documentation can be performed automatically instead of in response to a user command.</p>
      <p id="p-00180-en" num="0180">The process of <figref>FIG. 8</figref> can in some embodiments be useful for medical professionals to demonstrate an attempt to use NIR imaging to locate a vein before resorting to ultrasound imaging. An insurance company may be less likely to cover the more expensive ultrasound imaging costs if there is no record of previous attempts to locate a vein using less expensive methods (e.g., NIR imaging). In some embodiments, the ultrasound probe may not activate unless a record of an attempt to use NIR imaging has been recorded. In some embodiments, the system will automatically activate or prompt a user to activate the ultrasound probe in response to a detected failure to locate a vein with NIR light. The process of <figref>FIG. 8</figref> can in some embodiments be performed automatically with no additional input from a user, other than the activation of the NIR light. In some embodiments, not all steps of <figref>FIG. 8</figref> need to be performed, some steps can be omitted, some steps can be combined or divided into multiple steps, the order of some steps can be changed, and additional steps can be added. For example, in some embodiments, the storing of the indication of the attempt to use NIR imaging to identify a vein (e.g., storing an NIR image) can be omitted. The medical practitioner can attempt to use NIR imaging to identify a suitable venous access site and, if not successful, the medical practitioner can use ultrasound imaging (e.g., on the same display used for the NIR imaging) to identify a suitable venous access site. In some embodiments, records of all attempts to locate a vein with NIR imaging can be recorded. In some cases, the NIR imaging can be used to image a larger portion of target area than the ultrasound imaging. In some embodiments, the medical practitioner can use NIR imaging to identify a specific portion of the target area to use for the ultrasound imaging. For example, NIR imaging can be used to locate one or more veins, and the ultrasound imaging can be used to perform a more detailed analysis of the one or more veins (e.g., to determine the condition of the one or more veins and/or to confirm that the vein is suitable for accessing the patient's vasculature). The system can be used to document various components of imaging target <b>106</b>, such as the patient's vasculature, the blood flow, the blood flow rate, IV patency, infiltration and/or extravasation, and/or areas that should be avoided when placing an IV (e.g., the valves and branches in the vasculature and scar tissue).</p>
      <p id="p-00181-en" num="0181">In some embodiments, the near infrared image and the ultrasound image can be displayed on the same display, as described herein. A medical imaging instrument that includes both a near infrared imaging system and an ultrasound imaging system can be used. The same processor(s) can be used to produce the near infrared image and the ultrasound image.</p>
      <p id="p-00182-en" num="0182">
        <figref>FIG. 9</figref> shows a flowchart of an example embodiment of a method for accessing a patient's vasculature. A medical practitioner can use an NIR imaging system to image a target area on a patient. For example, the medical practitioner can illuminate the target area with near infrared light (e.g., as described herein) and can position a light sensor to receive near infrared light from the target area (e.g., reflected or scattered from the target area). The medical practitioner can position the light sensor at a distance from the target area, and the distance can at least about 1 inch, at least about 5 inches, at least about 7 inches, at least about 9 inches, at least about 10 inches, at least about 15 inches, etc. The distance can be less than or equal to about 30 inches, less than or equal to about 20 inches, less than or equal to about 15 inches, less than or equal to about 12 inches, less than or equal to about 10 inches, less than or equal to about 7 inches, less than or equal to about 5 inches, etc. If a satisfactory venous access site is identified using the NIR imaging, the medical practitioner can proceed to access the patient's vasculature using the identified vein (e.g., at the venous access site). For example, the medical practitioner can insert a needle or other medical implement into the vein at the venous access site.</p>
      <p id="p-00183-en" num="0183">If the medical practitioner was able to identify a possible venous access site using the NIR imaging, the medical practitioner can use ultrasound imaging to confirm the venous access site. For example, in some instances, the medical practitioner may see an object in the NIR image that might or might not be a vein, or the NIR image might show a vein but without sufficient image quality for the medical practitioner to determine whether the vein presents a suitable venous access site. The medical practitioner can then use an ultrasound probe to emit ultrasound signals into the target area. Echo signals can be received by the ultrasound probe and an ultrasound image can be displayed on the display, as discussed herein. The same display can be used for displaying the NIR images and the ultrasound images. The medical practitioner can use the ultrasound image to confirm the presence of a vein (e.g., that presents a suitable venous access site). Once the vein is confirmed using ultrasound imaging, the medical practitioner can use the vein to access the patient's vasculature. For example, the medical practitioner can insert a needle or other medical implement into the identified vein (e.g., at the identified venous access site). The medical practitioner can use ultrasound imaging and/or NIR imaging to facilitate inserting the needle or other medical implement, for example, depending on the preference of the medical practitioner. By way of example, the medical practitioner might see an object (e.g., a dark area) in the NIR image that could possibly be a vein. The medical practitioner can use ultrasound imaging to image the identified object to confirm that the object is indeed a vein. Then the medical practitioner can revert back to NIR imaging and can use the NIR image of the object, which was confirmed to be a vein, to facilitate insertion of a needle into the vein.</p>
      <p id="p-00184-en" num="0184">If the NIR image is not sufficient to identify a possible vein or venous access site, the medical practitioner can use ultrasound imaging to image the target area and to identify a suitable venous access site. The medical practitioner can then use the ultrasound imaging to facilitate insertion of the medical implement into the identified vein (e.g., at the venous access site).</p>
      <p id="p-00185-en" num="0185">In some embodiments, the system can be configured to automatically evaluate the NIR. If the evaluation of the NIR image determines that the NIR image is insufficient for identifying a suitable vein or vascular access site, the system can output an alert (e.g., a sound or a visual alert on the display <b>100</b>). For example, the alert can be an instruction to use ultrasound imaging. In some embodiments, the system can use image contrast analysis and/or edge detection algorithms to evaluate the NIR image.</p>
      <p id="p-00186-en" num="0186">With reference to <figref>FIG. 10</figref>, in some embodiments, a system (e.g., the medical imaging systems disclosed herein) can be configured to perform an assessment of the likelihood that NIR imaging would be successful for identifying a suitable vein to provide a venous access site. A processor can be used to consider one or more factors to perform the NIR imaging assessment. The processor can be an application specific integrated circuit (ASIC), or a general purpose processor configured to execute instructions stored on computer-readable memory to implement the assessment. The system can use a formula, algorithm, or look-up-table to generate a score (e.g., a ranking value) indicative of how likely it is that NIR imaging will be able to identify a suitable venous access site. Various factors can be considered by the assessment, including one or more of the following: the patients weight, Body Mass Index (BMI), blood pressure, temperature, skin color, past medical history (e.g., received from a patient file or hospital information system, such as using a wireless or wired connection), arm dominance (right or left handed), kidney function, etc. The system can include a user interface (e.g., user interface <b>124</b>) that is configured to enable a medical practitioner to input patient information to be used in the assessment. For example, the user interface can enable input of the patient's weight, BMI, blood pressure, temperature, skin color, medical history, kidney function, etc. In some embodiments, the system can be configured to measure one or more of the factors to be used by the assessment. For example, the system can include one or more medical devices, or one or more communication modules (e.g., electrical ports or wireless communication modules) configured to receive data from one or more medical devices, that are configured to measure one or more of the factors. For example, a scale, blood pressure monitor, a thermometer, a spectrometer, etc. can provide data to the system to be used in making the NIR imaging assessment. In some embodiments, the system can provide a single platform for performing NIR imaging and for measuring one or more factors for use in assessing the likelihood of success of NIR imaging for facilitating vascular access.</p>
      <p id="p-00187-en" num="0187">The system can output (e.g., using the user interface <b>124</b>) information relating to the assessment. For example the system can output (e.g., as an image on the display <b>100</b>) a raking indicating how likely it is that NIR imaging will be able to identify a suitable venous access site. For example, a Likert-type scale can be used with provide a ranking between 1 to 5 (e.g., with 1 indicating that a vein is likely to be easily identifiable using NIR imaging and with 5 indicating that a vein is very unlikely to be identifiable using NIR imaging). A medical practitioner can use the system to decide whether to skip NIR imaging, for example, and use ultrasound imaging to identify a vein. The system can be configured to document the NIR imaging assessment. For example, information regarding the assessment (e.g., the ranking and/or the factors using making the assessment) can be stored in local or remote non-transitory computer readable storage such as a patient file or hospital information system. A association can be stored between the assessment information and the patient and/or the ultrasound procedure. The documentation can be useful to demonstrate (e.g., to an insurance company) why ultrasound imaging was used instead of NIR imaging to facilitate venous access.</p>
      <p id="p-00188-en" num="0188">The systems and methods disclosed herein can be implemented in hardware, software, firmware, or a combination thereof. Software can include computer-readable instructions stored in memory (e.g., non-transitory, tangible memory, such as solid state memory (e.g., ROM, EEPROM, FLASH, RAM), optical memory (e.g., a CD, DVD, Blu-ray disc, etc.), magnetic memory (e.g., a hard disc drive), etc.), configured to implement the algorithms on a general purpose computer, special purpose processors, or combinations thereof. For example, one or more computing devices, such as a processor, may execute program instructions stored in computer readable memory to carry out processes disclosed herein. Hardware may include state machines, one or more general purpose computers, and/or one or more special purpose processors. In some embodiment, multiple processors can be used, and in some implementations the processors can be at different locations (e.g., coupled via a network). While certain types of user interfaces and controls are described herein for illustrative purposes, other types of user interfaces and controls may be used.</p>
      <p id="p-00189-en" num="0189">The embodiments discussed herein are provided by way of example, and various modifications can be made to the embodiments described herein. Certain features that are described in this disclosure in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can be implemented in multiple embodiments separately or in various suitable subcombinations. Also, features described in connection with one combination can be excised from that combination and can be combined with other features in various combinations and subcombinations. Various features can be added to the example embodiments disclosed herein. Also, various features can be omitted from the example embodiments disclosed herein.</p>
      <p id="p-00190-en" num="0190">Similarly, while operations are depicted in the drawings or described in a particular order, the operations can be performed in a different order than shown or described. Other operations not depicted can be incorporated before, after, or simultaneously with the operations shown or described. In certain circumstances, parallel processing or multitasking can be used. Also, in some cases, the operations shown or discussed can be omitted or recombined to form various combinations and subcombinations.</p>
    </detailed-desc>
  </description>
  <claims id="claims_eng" lang="eng" format="original" date-changed="20150917">
    <claim num="1" id="clm-00001-en">
      <claim-text>
        <b>1</b>. A method for displaying an ultrasound image, the method comprising:
<claim-text>identifying a target body portion of a patient to be imaged;</claim-text><claim-text>emitting ultrasound signals from an ultrasound probe, wherein the ultrasound signals cause echo signals to be reflected from tissue within the target body portion;</claim-text><claim-text>receiving the echo signals;</claim-text><claim-text>generating an ultrasound image of the target body portion based at least in part on the echo signals; and</claim-text><claim-text>displaying the ultrasound image on a head-mounted display.</claim-text></claim-text>
    </claim>
    <claim num="2" id="clm-00002-en">
      <claim-text>
        <b>2</b>. The method of <claim-ref idref="clm-00001-en">claim 1</claim-ref>, further comprising:
<claim-text>transmitting the ultrasound image to a remote display accessible to a remote healthcare professional;</claim-text><claim-text>receiving treatment instructions from the remote healthcare professional; and</claim-text><claim-text>administering treatment based at least in part on the instructions from the remote healthcare professional.</claim-text></claim-text>
    </claim>
    <claim num="3" id="clm-00003-en">
      <claim-text>
        <b>3</b>. The method of <claim-ref idref="clm-00001-en">claim 1</claim-ref>, further comprising:
<claim-text>generating echo signal data based at least in part on the echo signals;</claim-text><claim-text>transmitting the echo signal data wirelessly to an image processor that generates the ultrasound image.</claim-text></claim-text>
    </claim>
    <claim num="4" id="clm-00004-en">
      <claim-text>
        <b>4</b>. The method of <claim-ref idref="clm-00001-en">claim 1</claim-ref>, further comprising transmitting the ultrasound image wirelessly to the head-mounted display.</claim-text>
    </claim>
    <claim num="5" id="clm-00005-en">
      <claim-text>
        <b>5</b>. The method of <claim-ref idref="clm-00001-en">claim 1</claim-ref>, further comprising attaching the head-mounted display to a user.</claim-text>
    </claim>
    <claim num="6" id="clm-00006-en">
      <claim-text>
        <b>6</b>. The method of <claim-ref idref="clm-00005-en">claim 5</claim-ref>, wherein the attaching is achieved by the user wearing a helmet that supports the head-mounted display such that the ultrasound image displayed on the head-mounted display is configured to be visible to the user wearing the helmet.</claim-text>
    </claim>
    <claim num="7" id="clm-00007-en">
      <claim-text>
        <b>7</b>. The method of <claim-ref idref="clm-00005-en">claim 5</claim-ref>, wherein the attaching is achieved by the user wearing an eyewear frame that supports the head-mounted display such that the ultrasound image displayed on the head-mounted display is configured to be visible to the user wearing the eyewear frame.</claim-text>
    </claim>
    <claim num="8" id="clm-00008-en">
      <claim-text>
        <b>8</b>. The method of <claim-ref idref="clm-00005-en">claim 5</claim-ref>, wherein the attaching is achieved by the user wearing a headband that supports the head-mounted display such that the ultrasound image displayed on the head-mounted display is configured to be visible to the user wearing the headband.</claim-text>
    </claim>
    <claim num="9" id="clm-00009-en">
      <claim-text>
        <b>9</b>. A system for displaying an ultrasound image, the system comprising:
<claim-text>an ultrasound probe configured to emit ultrasound signals and detect echo signals reflected from tissue in a target body portion of a patient;</claim-text><claim-text>an image processor configured to receive echo signal data that is based at least in part on the echo signals received by the ultrasound probe and generate an ultrasound image of the target body portion based at least in part on the echo signal data; and</claim-text><claim-text>a wearable display configured to receive the ultrasound image of the target body portion from the image processor and display the ultrasound image.</claim-text></claim-text>
    </claim>
    <claim num="10" id="clm-00010-en">
      <claim-text>
        <b>10</b>. The system of <claim-ref idref="clm-00009-en">claim 9</claim-ref>, further comprising a remote display accessible to a remote healthcare professional and configured to receive and display the image of the target body portion.</claim-text>
    </claim>
    <claim num="11" id="clm-00011-en">
      <claim-text>
        <b>11</b>. The system of <claim-ref idref="clm-00009-en">claim 9</claim-ref>, further comprising a communications module that is configured to deliver the ultrasound image wirelessly to a remote display accessible to a remote healthcare professional.</claim-text>
    </claim>
    <claim num="12" id="clm-00012-en">
      <claim-text>
        <b>12</b>. The system of <claim-ref idref="clm-00009-en">claim 9</claim-ref>, further comprising a communications module that is configured to deliver the ultrasound image wirelessly to the wearable display module.</claim-text>
    </claim>
    <claim num="13" id="clm-00013-en">
      <claim-text>
        <b>13</b>. The system of <claim-ref idref="clm-00009-en">claim 9</claim-ref>, wherein the wearable display is configured to display the image of the target body portion on a heads-up display.</claim-text>
    </claim>
    <claim num="14" id="clm-00014-en">
      <claim-text>
        <b>14</b>. The system of <claim-ref idref="clm-00009-en">claim 9</claim-ref>, wherein the wearable display comprises an at least partially transparent surface.</claim-text>
    </claim>
    <claim num="15" id="clm-00015-en">
      <claim-text>
        <b>15</b>. The system of <claim-ref idref="clm-00009-en">claim 9</claim-ref>, further comprising a helmet that supports the wearable display such that the ultrasound image displayed by the wearable display is configured to be visible to a person wearing the helmet.</claim-text>
    </claim>
    <claim num="16" id="clm-00016-en">
      <claim-text>
        <b>16</b>. The system of <claim-ref idref="clm-00009-en">claim 9</claim-ref>, further comprising an eyewear frame that supports the wearable display such that the ultrasound image displayed by the wearable display is configured to be visible to a person wearing the eyewear frame.</claim-text>
    </claim>
    <claim num="17" id="clm-00017-en">
      <claim-text>
        <b>17</b>. The system of <claim-ref idref="clm-00009-en">claim 9</claim-ref>, further comprising a headband that supports the wearable display such that the ultrasound image displayed by the wearable display is configured to be visible to a person wearing the headband.</claim-text>
    </claim>
    <claim num="18" id="clm-00018-en">
      <claim-text>
        <b>18</b>. An image display system comprising:
<claim-text>an ultrasound probe;</claim-text><claim-text>a processor configured to generate an image based at least in part on data from the ultrasound probe; and</claim-text><claim-text>a wearable display configured to display the image.</claim-text></claim-text>
    </claim>
    <claim num="19" id="clm-00019-en">
      <claim-text>
        <b>19</b>. The system of <claim-ref idref="clm-00018-en">claim 18</claim-ref>, wherein the wearable display is further configured to display the image as a heads-up display image.</claim-text>
    </claim>
    <claim num="20" id="clm-00020-en">
      <claim-text>
        <b>20</b>. The system of <claim-ref idref="clm-00018-en">claim 18</claim-ref>, wherein the processor is capable of wireless communication with at least one of the ultrasound probe and/or the wearable display.</claim-text>
    </claim>
    <claim num="21" id="clm-00021-en">
      <claim-text>
        <b>21</b>.-<b>114</b>. (canceled)</claim-text>
    </claim>
  </claims>
  <drawings id="drawings" format="original">
    <figure num="1">
      <img he="N/A" wi="N/A" file="US20150257735A1_00001.PNG" alt="clipped image" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00000.TIF" />
    </figure>
    <figure num="2">
      <img he="N/A" wi="N/A" file="US20150257735A1_00002.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00001.TIF" />
    </figure>
    <figure num="3">
      <img he="N/A" wi="N/A" file="US20150257735A1_00003.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00002.TIF" />
    </figure>
    <figure num="4">
      <img he="N/A" wi="N/A" file="US20150257735A1_00004.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00003.TIF" />
    </figure>
    <figure num="5">
      <img he="N/A" wi="N/A" file="US20150257735A1_00005.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00004.TIF" />
    </figure>
    <figure num="6">
      <img he="N/A" wi="N/A" file="US20150257735A1_00006.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00005.TIF" />
    </figure>
    <figure num="7">
      <img he="N/A" wi="N/A" file="US20150257735A1_00007.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00006.TIF" />
    </figure>
    <figure num="8">
      <img he="N/A" wi="N/A" file="US20150257735A1_00008.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00007.TIF" />
    </figure>
    <figure num="9">
      <img he="N/A" wi="N/A" file="US20150257735A1_00009.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00008.TIF" />
    </figure>
    <figure num="10">
      <img he="N/A" wi="N/A" file="US20150257735A1_00010.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00009.TIF" />
    </figure>
    <figure num="11">
      <img he="N/A" wi="N/A" file="US20150257735A1_00011.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00010.TIF" />
    </figure>
    <figure num="12">
      <img he="N/A" wi="N/A" file="US20150257735A1_00012.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00011.TIF" />
    </figure>
    <figure num="13">
      <img he="N/A" wi="N/A" file="US20150257735A1_00013.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00012.TIF" />
    </figure>
    <figure num="14">
      <img he="N/A" wi="N/A" file="US20150257735A1_00014.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00013.TIF" />
    </figure>
    <figure num="15">
      <img he="N/A" wi="N/A" file="US20150257735A1_00015.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00014.TIF" />
    </figure>
    <figure num="16">
      <img he="N/A" wi="N/A" file="US20150257735A1_00016.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00015.TIF" />
    </figure>
    <figure num="17">
      <img he="N/A" wi="N/A" file="US20150257735A1_00017.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00016.TIF" />
    </figure>
    <figure num="18">
      <img he="N/A" wi="N/A" file="US20150257735A1_00018.PNG" alt="thumbnail image" img-content="drawing" img-format="png" original="US20150257735A1-20150917-D00000.TIF" />
    </figure>
  </drawings>
  <image file="US20150257735A1.PDF" type="pdf" size="1219032" pages="35" />
</lexisnexis-patent-document>