<?xml version="1.0" encoding="utf-8"?>
<!-- Copyright ©2016 LexisNexis Univentio, The Netherlands. -->
<lexisnexis-patent-document schema-version="1.13" date-produced="20160127" file="US20150257635A1.xml" produced-by="LexisNexis-Univentio" lang="eng" date-inserted="20150917" time-inserted="030130" date-changed="20151103" time-changed="063351">
  <bibliographic-data lang="eng">
    <publication-reference publ-type="Application" publ-desc="Patent Application Publication">
      <document-id id="121314772">
        <country>US</country>
        <doc-number>20150257635</doc-number>
        <kind>A1</kind>
        <date>20150917</date>
      </document-id>
    </publication-reference>
    <application-reference appl-type="utility">
      <document-id>
        <country>US</country>
        <doc-number>14725667</doc-number>
        <date>20150529</date>
      </document-id>
    </application-reference>
    <application-series-code>14</application-series-code>
    <language-of-filing>eng</language-of-filing>
    <language-of-publication>eng</language-of-publication>
    <priority-claims date-changed="20150917">
      <priority-claim sequence="1" kind="national">
        <country>JP</country>
        <doc-number>2012262498</doc-number>
        <date>20121130</date>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability date-changed="20150924">
      <unexamined-printed-without-grant>
        <date>20150917</date>
      </unexamined-printed-without-grant>
    </dates-of-public-availability>
    <classifications-ipcr date-changed="20151103">
      <classification-ipcr sequence="1">
        <text>A61B   1/06        20060101AFI20150917BHUS        </text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>1</main-group>
        <subgroup>06</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>US</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>A61B   1/00        20060101ALI20150917BHUS        </text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>1</main-group>
        <subgroup>00</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>US</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-ipcr>
      <classification-ipcr sequence="3">
        <text>A61B   1/04        20060101ALI20150917BHUS        </text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>1</main-group>
        <subgroup>04</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>US</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-ipcr>
    </classifications-ipcr>
    <classifications-cpc date-changed="20151103">
      <classification-cpc sequence="1">
        <text>A61B   1/0638      20130101 FI20150918BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>1</main-group>
        <subgroup>0638</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150918</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="2">
        <text>A61B   1/00009     20130101 LI20150917BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>1</main-group>
        <subgroup>00009</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="3">
        <text>A61B   1/00186     20130101 LI20151028BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>1</main-group>
        <subgroup>00186</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20151028</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="4">
        <text>A61B   1/043       20130101 LI20150917BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>1</main-group>
        <subgroup>043</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20150917</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="5">
        <text>G02B  23/2484      20130101 LI20140716BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>G</section>
        <class>02</class>
        <subclass>B</subclass>
        <main-group>23</main-group>
        <subgroup>2484</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20140716</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="6">
        <text>G02B  23/26        20130101 LI20140627BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>G</section>
        <class>02</class>
        <subclass>B</subclass>
        <main-group>23</main-group>
        <subgroup>26</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <action-date>
          <date>20140627</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
      <classification-cpc sequence="7">
        <text>H04N2005/2255      20130101 LA20150119BHEP        </text>
        <cpc-version-indicator>
          <date>20130101</date>
        </cpc-version-indicator>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>2005</main-group>
        <subgroup>2255</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <action-date>
          <date>20150119</date>
        </action-date>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
      </classification-cpc>
    </classifications-cpc>
    <number-of-claims calculated="yes">12</number-of-claims>
    <invention-title id="title_eng" date-changed="20150917" lang="eng" format="original">OBSERVATION APPARATUS</invention-title>
    <related-documents date-changed="20150917">
      <continuation>
        <relation>
          <parent-doc>
            <document-id>
              <country>WO</country>
              <doc-number>/JP2013/081496</doc-number>
              <date>20131122</date>
            </document-id>
            <parent-status>PENDING</parent-status>
            <parent-pct-document>
              <document-id>
                <country>US</country>
                <doc-number>PCT/JP2013/081496</doc-number>
                <date>20131122</date>
              </document-id>
            </parent-pct-document>
          </parent-doc>
          <child-doc>
            <document-id>
              <country>US</country>
              <doc-number>14007256</doc-number>
            </document-id>
          </child-doc>
        </relation>
      </continuation>
    </related-documents>
    <parties date-changed="20150917">
      <applicants>
        <applicant sequence="1" app-type="applicant">
          <addressbook lang="eng">
            <orgname>OLYMPUS CORPORATION</orgname>
            <role>03</role>
            <address>
              <city>Tokyo</city>
              <country>JP</country>
            </address>
          </addressbook>
        </applicant>
      </applicants>
      <inventors>
        <inventor sequence="1" designation="us-only">
          <addressbook lang="eng">
            <last-name>KUBO</last-name>
            <first-name>Kei</first-name>
            <address>
              <city>Tokyo</city>
              <country>JP</country>
            </address>
          </addressbook>
        </inventor>
        <inventor sequence="2" designation="us-only">
          <addressbook lang="eng">
            <last-name>ISHIHARA</last-name>
            <first-name>Yasushige</first-name>
            <address>
              <city>Tokyo</city>
              <country>JP</country>
            </address>
          </addressbook>
        </inventor>
        <inventor sequence="3" designation="us-only">
          <addressbook lang="eng">
            <last-name>SHIDA</last-name>
            <first-name>Hiromi</first-name>
            <address>
              <city>Tokyo</city>
              <country>JP</country>
            </address>
          </addressbook>
        </inventor>
      </inventors>
    </parties>
    <pct-or-regional-filing-data>
      <document-id>
        <country>WO</country>
        <doc-number>JP13081496</doc-number>
        <date>20131122</date>
      </document-id>
    </pct-or-regional-filing-data>
    <pct-or-regional-publishing-data>
      <document-id>
        <country>WO</country>
        <doc-number>2014084134</doc-number>
        <date>20140605</date>
      </document-id>
    </pct-or-regional-publishing-data>
    <patent-family date-changed="20151007">
      <main-family family-id="165995909">
        <family-member>
          <document-id>
            <country>US</country>
            <doc-number>20150257635</doc-number>
            <kind>A1</kind>
            <date>20150917</date>
          </document-id>
          <application-date>
            <date>20150529</date>
          </application-date>
        </family-member>
        <family-member>
          <document-id>
            <country>EP</country>
            <doc-number>2926713</doc-number>
            <kind>A1</kind>
            <date>20151007</date>
          </document-id>
          <application-date>
            <date>20131122</date>
          </application-date>
        </family-member>
        <family-member>
          <document-id>
            <country>CN</country>
            <doc-number>104736036</doc-number>
            <kind>A</kind>
            <date>20150624</date>
          </document-id>
          <application-date>
            <date>20131122</date>
          </application-date>
        </family-member>
        <family-member>
          <document-id>
            <country>WO</country>
            <doc-number>2014084134</doc-number>
            <kind>A1</kind>
            <date>20140605</date>
          </document-id>
          <application-date>
            <date>20131122</date>
          </application-date>
        </family-member>
      </main-family>
      <complete-family family-id="165995906">
        <family-member>
          <document-id>
            <country>US</country>
            <doc-number>20150257635</doc-number>
            <kind>A1</kind>
            <date>20150917</date>
          </document-id>
          <application-date>
            <date>20150529</date>
          </application-date>
        </family-member>
        <family-member>
          <document-id>
            <country>EP</country>
            <doc-number>2926713</doc-number>
            <kind>A1</kind>
            <date>20151007</date>
          </document-id>
          <application-date>
            <date>20131122</date>
          </application-date>
        </family-member>
        <family-member>
          <document-id>
            <country>CN</country>
            <doc-number>104736036</doc-number>
            <kind>A</kind>
            <date>20150624</date>
          </document-id>
          <application-date>
            <date>20131122</date>
          </application-date>
        </family-member>
        <family-member>
          <document-id>
            <country>WO</country>
            <doc-number>2014084134</doc-number>
            <kind>A1</kind>
            <date>20140605</date>
          </document-id>
          <application-date>
            <date>20131122</date>
          </application-date>
        </family-member>
      </complete-family>
    </patent-family>
  </bibliographic-data>
  <abstract id="abstr_eng" date-changed="20150917" lang="eng" format="original">
    <p id="p-a-00001-en" num="0000">An observation apparatus including a light source that irradiates a subject with illumination light and special light that acts on a specific region of the subject; and a processor comprising hardware, wherein the processor is configured to implement: a return-light-image generating portion that generates a return-light image based on captured return light coming from the subject due to irradiation with the illumination light; a special-light-image generating portion that generates a special-light image based on captured signal light coming from the subject due to irradiation with the special light; an extraction portion that extracts the specific region from the special-light image; and an enhancement processing portion that performs enhancement processing, which is based on return-light image information, on the return-light image, in a region corresponding to the extracted specific region.</p>
  </abstract>
  <legal-data date-changed="20151002">
    <legal-event sequence="1">
      <publication-date>
        <date>20150529</date>
      </publication-date>
      <event-code-1>AS</event-code-1>
      <legal-description>ASSIGNMENT</legal-description>
      <status-identifier>N</status-identifier>
      <docdb-publication-number> US  2015257635A1</docdb-publication-number>
      <docdb-application-id>444510952</docdb-application-id>
      <new-owner>OLYMPUS CORPORATION, JAPAN</new-owner>
      <free-text-description>ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:KUBO, KEI;ISHIHARA, YASUSHIGE;SHIDA, HIROMI;SIGNING DATES FROM 20150525 TO 20150526;REEL/FRAME:035745/0460</free-text-description>
    </legal-event>
  </legal-data>
  <description id="descr_eng" lang="eng" format="original" date-changed="20150917">
    <related-apps>
      <heading id="h-00001-en" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
      <p id="p-00001-en" num="0001">This is a continuation of International Application PCT/JP2013/081496, with an international filing date of Nov. 22, 2013, which is hereby incorporated by reference herein in its entirety. This application claims the benefit of Japanese Patent Application No. 2012-262498, filed on Nov. 30, 2012, the content of which is incorporated herein by reference.</p>
    </related-apps>
    <summary>
      <heading id="h-00002-en" level="1">TECHNICAL FIELD</heading>
      <p id="p-00002-en" num="0002">The present invention relates to an observation apparatus.</p>
      <heading id="h-00003-en" level="1">BACKGROUND ART</heading>
      <p id="p-00003-en" num="0003">In the related art, there are known observation apparatuses that selectively capture images of a region-of-interest, such as a lesion, in a subject by using light of a specific wavelength, that identify the position of the region-of-interest by using an obtained special-light image, and that label the identified position in a white-light image with a marker (for example, see Patent Literature 1). With the marker which is displayed at the region-of-interest in the white-light image, the user can easily recognize the region-of-interest that exists in the observation field of view.</p>
      <heading id="h-00004-en" level="1">CITATION LIST</heading>
      <heading id="h-00005-en" level="1">Patent Literature</heading>
      <p id="p-00004-en" num="0004">{PTL 1} Japanese Unexamined Patent Application, Publication No. 2011-104011</p>
      <heading id="h-00006-en" level="1">SUMMARY OF INVENTION</heading>
      <p id="p-00005-en" num="0005">The present invention provides an observation apparatus including a light source that irradiates a subject with illumination light and special light in a wavelength band different from that of the illumination light, which acts on a specific region of the subject; and a processor comprising hardware, wherein the processor is configured to implement: a return-light-image generating portion that generates a return-light image based on captured return light emitted from the subject due to irradiation with the illumination light from the light source; a special-light-image generating portion that generates a special-light image based on captured signal light emitted from the subject due to irradiation with the special light from the light source; an extraction portion that extracts the specific region from the special-light image generated by the special-light-image generating portion; and an enhancement processing portion that performs enhancement processing, which is based on return-light image information, on the return-light image generated by the return-light-image generating portion, in a region corresponding to the specific region extracted by the extraction portion.</p>
    </summary>
    <description-of-drawings>
      <heading id="h-00007-en" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading>
      <p id="p-00006-en" num="0006">
        <figref>FIG. 1</figref> is a diagram showing the overall configuration of an observation apparatus according to a first embodiment of the present invention.</p>
      <p id="p-00007-en" num="0007">
        <figref>FIG. 2</figref> is a flowchart showing image processing performed by the observation apparatus in <figref>FIG. 1</figref>.</p>
      <p id="p-00008-en" num="0008">
        <figref>FIG. 3</figref> is a diagram showing the configuration of an image processor provided in an observation apparatus according to a second modification of the first embodiment.</p>
      <p id="p-00009-en" num="0009">
        <figref>FIG. 4</figref> is a diagram showing the configuration of an image processor provided in an observation apparatus according to a third modification of the first embodiment.</p>
      <p id="p-00010-en" num="0010">
        <figref>FIG. 5</figref> is a graph showing a function relating a mean gradation value and a degree of enhancement processing, which is used in an enhancement-level setting portion in <figref>FIG. 4</figref>.</p>
      <p id="p-00011-en" num="0011">
        <figref>FIG. 6</figref> is a flowchart for explaining image processing performed by the image processor in <figref>FIG. 4</figref>.</p>
      <p id="p-00012-en" num="0012">
        <figref>FIG. 7</figref> is a diagram showing the configuration of an image processor provided in an observation apparatus according to a fourth modification of the first embodiment.</p>
      <p id="p-00013-en" num="0013">
        <figref>FIG. 8</figref> is a flowchart for explaining image processing performed by the image processor in <figref>FIG. 7</figref>.</p>
      <p id="p-00014-en" num="0014">
        <figref>FIG. 9</figref> is a diagram showing the overall configuration of an observation apparatus according to a second embodiment of the present invention.</p>
      <p id="p-00015-en" num="0015">
        <figref>FIG. 10</figref> is a diagram showing the configuration of an image processor provided in an observation apparatus according to a modification of the second embodiment.</p>
      <p id="p-00016-en" num="0016">
        <figref>FIG. 11</figref> is a diagram showing the overall configuration of an observation apparatus according to a third embodiment of the present invention.</p>
      <p id="p-00017-en" num="0017">
        <figref>FIG. 12</figref> is a diagram showing the overall configuration of an observation apparatus according to a fourth embodiment of the present invention.</p>
    </description-of-drawings>
    <detailed-desc>
      <heading id="h-00008-en" level="1">DESCRIPTION OF EMBODIMENTS</heading>
      <heading id="h-00009-en" level="1">First Embodiment</heading>
      <p id="p-00018-en" num="0018">An observation apparatus <b>100</b> according to a first embodiment of the present invention will be described below with reference to <figref>FIGS. 1 to 8</figref>.</p>
      <p id="p-00019-en" num="0019">The observation apparatus <b>100</b> according to this embodiment is an endoscope apparatus and, as shown in <figref>FIG. 1</figref>, includes an elongated insertion portion <b>2</b> for insertion into a body; a light source <b>3</b>; an illumination unit <b>4</b> that radiates excitation light (special light) and white light (illumination light) from the light source <b>3</b> towards an observation target (subject) X from a distal end <b>2</b><i>a </i>of the insertion portion <b>2</b>; an image-acquisition unit <b>5</b> that obtains image information S<b>1</b> and S<b>2</b> of biological tissue, that is, the observation target X; an image processor (processor) <b>6</b> that is disposed at the base end of the insertion portion <b>2</b> and that processes the image information S<b>1</b> and S<b>2</b> obtained by the image-acquisition unit <b>5</b>; and a display <b>7</b> that displays an image G<b>1</b>′ processed by the image processor <b>6</b>.</p>
      <p id="p-00020-en" num="0020">The light source <b>3</b> includes a xenon lamp <b>31</b>, a filter <b>32</b> that extracts excitation light and white light from the light emitted from the xenon lamp <b>31</b>, and a coupling lens <b>33</b> that focuses the excitation light and the white light extracted by the filter <b>32</b>. The filter <b>32</b> selectively transmits light in a wavelength band of 400 nm to 740 nm, corresponding to the excitation light and the white light. In other words, in this embodiment, near-infrared light (wavelength band 700 nm to 740 nm) is used as the excitation light.</p>
      <p id="p-00021-en" num="0021">The illumination unit <b>4</b> includes a light guide fiber <b>41</b> that is disposed along substantially the entire length of the insertion portion <b>2</b> in the longitudinal direction thereof and an illumination optical system <b>42</b> that is provided at the distal end <b>2</b><i>a </i>of the insertion portion <b>2</b>. The light guide fiber <b>41</b> guides the excitation light and the white light focused by the coupling lens <b>33</b>. The illumination optical system <b>42</b> spreads out the excitation light and the white light guided thereto by the light guide fiber <b>41</b> and irradiates the observation target X, which faces the distal end <b>2</b><i>a </i>of the insertion portion <b>2</b>.</p>
      <p id="p-00022-en" num="0022">The image-acquisition unit <b>5</b> includes an objective lens <b>51</b> that collects light coming from the observation target X; a dichroic mirror <b>52</b> that reflects the excitation light and fluorescence (signal light) in the light collected by the objective lens <b>51</b> and transmits white light having a wavelength shorter than that of the excitation light (wavelength band 400 nm to 700 nm, return light); two focusing lenses <b>53</b> and <b>54</b> that respectively focus the fluorescence reflected by the dichroic mirror <b>52</b> and the white light transmitted through the dichroic mirror <b>52</b>; an image-acquisition device <b>55</b>, such as a color CCD, that captures the white light focused by the focusing lens <b>53</b>; and an image-acquisition device <b>56</b>, such as a high-sensitivity monochrome CCD, that captures the fluorescence focused by the focusing lens <b>54</b>. Reference sign <b>57</b> in the figure is an excitation-light cutting filter that selectively transmits the fluorescence (wavelength band 760 nm to 850 nm) in the light reflected by the dichroic mirror <b>52</b> and blocks the excitation light.</p>
      <p id="p-00023-en" num="0023">The image processor <b>6</b> includes a white-light-image generating portion (return-light-image generating portion) <b>61</b> that generates a white-light image (return-light image) from the white-light image information S<b>1</b> obtained by the image-acquisition device <b>55</b>; a fluorescence-image generating portion (special-light-image generating portion) <b>62</b> that generates a fluorescence image (special-light image) G<b>2</b> from the fluorescence image information S<b>2</b> obtained by the image-acquisition device <b>56</b>; an extraction portion <b>63</b> that extracts a region-of-interest (specific region), such as a lesion Y, from the fluorescence image G<b>2</b> generated by the fluorescence-image generating portion <b>62</b>; and an enhancement processing portion <b>64</b> that executes enhancement processing on a region in the white-light image G<b>1</b> that corresponds to the region-of-interest extracted by the extraction portion <b>63</b>.</p>
      <p id="p-00024-en" num="0024">The image processor <b>6</b> includes a central processing unit (CPU), a main storage device such as RAM (Random Access Memory), and an auxiliary storage device. The auxiliary storage device is a non-transitory computer-readable storage medium such as an optical disc or a magnetic disk, and stores an image processing program. The CPU loads the image processing program stored in the auxiliary storage device, and then executes the program, thereby to implement functions of the white-light-image generating portion <b>61</b>, the fluorescence-image generating portion <b>62</b>, the extraction portion <b>63</b>, and the enhancement processing portion <b>64</b>. Alternatively, the functions of those portions <b>61</b>, <b>62</b>, <b>63</b>, and <b>64</b> may be implemented by hardware such as ASIC (Application Specific Integrated Circuit).</p>
      <p id="p-00025-en" num="0025">The extraction portion <b>63</b> compares the gradation value of each pixel in the fluorescence image G<b>2</b> input thereto from the fluorescence-image generating portion <b>62</b> with a prescribed threshold, extracts pixels having a gradation value equal to or higher than the prescribed threshold as a region-of-interest, and outputs positions P of the extracted pixels to the enhancement processing portion <b>64</b>.</p>
      <p id="p-00026-en" num="0026">The enhancement processing portion <b>64</b> selects, from the white-light image G<b>1</b>, pixels at positions corresponding to the positions P of the pixels input thereto from the extraction portion <b>63</b>, enhances the color of the region-of-interest formed of the selected pixels, and outputs a white-light image G<b>1</b>′, in which the region-of-interest has been subjected to enhancement processing, to the display <b>7</b>.</p>
      <p id="p-00027-en" num="0027">More specifically, the enhancement processing portion <b>64</b> subjects the white-light image G<b>1</b> to hemoglobin index (IHb) color enhancement processing. IHb color enhancement is processing in which the color at positions on the mucous membrane covering the surface of biological tissue, that is, the observation target X, where the hemoglobin index is higher than average, is made more red, and the color at positions where the hemoglobin index is lower than the average is made more white. The absorption coefficients of hemoglobin in the green (G) and red (R) wavelength regions are different from each other. By using this fact, the hemoglobin index at each position in the white-light image G<b>1</b> is measured by calculating the ratio of the brightness levels of a G signal and an R signal from the white-light image information S<b>1</b>.</p>
      <p id="p-00028-en" num="0028">The lesion Y has a red tinge compared with normal parts around it. This is because the cells are more active and the blood flow is higher in the lesion Y. The color of this lesion Y can be enhanced via IHb color enhancement, which allows the user to perform more detailed examination of the lesion Y.</p>
      <p id="p-00029-en" num="0029">Next, the operation of the thus-configured observation apparatus <b>100</b> will be described.</p>
      <p id="p-00030-en" num="0030">To observe biological tissue inside a body, that is, the observation target X, by using the observation apparatus <b>100</b> according to this embodiment, a fluorescent substance that accumulates in the lesion Y is administered in advance to the observation target X. Then, the insertion portion <b>2</b> is inserted into the body so that the distal end <b>2</b><i>a </i>of the insertion portion <b>2</b> is disposed facing the observation target X. Next, by operating the light source <b>3</b>, the excitation light and white light are radiated onto the observation target X from the distal end <b>2</b><i>a </i>of the insertion portion <b>2</b>.</p>
      <p id="p-00031-en" num="0031">Fluorescence is generated in the observation target X as a result of excitation of the fluorescent substance contained in the lesion Y by the excitation light, and the white light is reflected at the surface of the observation target X. Parts of the fluorescence emitted from the observation target X and the white light reflected therefrom return to the distal end <b>2</b><i>a </i>of the insertion portion <b>2</b> and are collected by the objective lens <b>51</b>.</p>
      <p id="p-00032-en" num="0032">Of the light collected by the objective lens <b>51</b>, the white light is transmitted through the dichroic mirror <b>52</b> and is focused by the focusing lens <b>53</b>, and the white-light image information S<b>1</b> is obtained by the image-acquisition device <b>55</b>. On the other hand, the fluorescence collected by the objective lens <b>51</b> is reflected by the dichroic mirror <b>52</b> and, after the excitation light is removed therefrom by the excitation-light cutting filter <b>57</b>, is focused by the focusing lens <b>54</b>, and the fluorescence image information S<b>2</b> is obtained by the image-acquisition device <b>56</b>. The image information S<b>1</b> and S<b>2</b> obtained by the respective image-acquisition devices <b>55</b> and <b>56</b> are sent to the image processor <b>6</b>.</p>
      <p id="p-00033-en" num="0033">
        <figref>FIG. 2</figref> shows a flowchart for explaining image processing performed by the image processor <b>6</b>.</p>
      <p id="p-00034-en" num="0034">In the image processor <b>6</b>, the white-light image information S<b>1</b> is input to the white-light-image generating portion <b>61</b>, where the white-light image G<b>1</b> is generated, and the fluorescence image information S<b>2</b> is input to the fluorescence-image generating portion <b>62</b>, where the fluorescence image G<b>2</b> is generated (step S<b>1</b>).</p>
      <p id="p-00035-en" num="0035">The fluorescence image G<b>2</b> is sent to the extraction portion <b>63</b>, where the region-of-interest having gradation values equal to or higher than the prescribed threshold is extracted (step S<b>2</b>). The position P of the extracted region-of-interest is sent from the extraction portion <b>63</b> to the enhancement processing portion <b>64</b>, and the region-of-interest in the white-light image G<b>1</b> is subjected to color enhancement processing in the enhancement processing portion <b>64</b> (step S<b>3</b>). Then, the white-light image G<b>1</b>′ in which the region-of-interest has been subjected to enhancement processing is displayed on the display <b>7</b> (step S<b>4</b>). If a region-of-interest is not extracted in step S<b>2</b>, the unprocessed white-light image G<b>1</b> is displayed on the display <b>7</b> in step S<b>4</b>.</p>
      <p id="p-00036-en" num="0036">The extraction portion <b>63</b> in this embodiment may calculate the area of the region-of-interest from the number of pixels constituting the region-of-interest, and for a region-of-interest having an area equal to or larger than a threshold that is set in advance, the positions P of the extracted pixels may be output to the enhancement processing portion <b>64</b>. By doing so, regions-of-interest having extremely small areas can be removed as noise.</p>
      <p id="p-00037-en" num="0037">In this way, with this embodiment, when a region-of-interest such as the lesion Y exists in the viewing field of the white-light image G<b>1</b>, that region-of-interest is displayed in an enhanced manner. Therefore, an advantage is afforded in that the user can easily recognize the region-of-interest in the white-light image G<b>1</b>′ displayed on the display <b>7</b>, and in addition, he or she can confirm, in detail, the morphology of the region-of-interest by using the white-light image G<b>1</b>′. In addition, a peripheral region surrounding the region-of-interest, such as a normal area of the tissue is not changed from the color of the unprocessed white-light image G<b>1</b>, and the color contrast of only the region-of-interest in the tissue are enhanced.</p>
      <heading id="h-00010-en" level="1">First Modification</heading>
      <p id="p-00038-en" num="0038">Next, a first modification of the observation apparatus <b>100</b> according to the first embodiment will be described.</p>
      <p id="p-00039-en" num="0039">The observation apparatus according to this modification is one in which the details of the processing in the enhancement processing portion <b>64</b> of the observation apparatus <b>100</b> are modified.</p>
      <p id="p-00040-en" num="0040">In this modification, the enhancement processing portion <b>64</b> enhances the structure of the region-of-interest by extracting the outline of tissue in the region-of-interest from the white-light image G<b>1</b> and enhancing the outline of the tissue in the region-of-interest. To extract the outline, for example, edge extraction processing such as a differential filter is used. Thus, even when using structure enhancement processing instead of the color enhancement processing described above, it is possible to easily recognize the region-of-interest in the white-light image G<b>1</b>′, and the morphology of the region-of-interest can be examined in detail. In addition, a peripheral region surrounding the region-of-interest, such as a normal area of the tissue is not changed from the structure of the unprocessed white-light image G<b>1</b>, and the structure contrast of only the region-of-interest in the tissue are enhanced.</p>
      <p id="p-00041-en" num="0041">In this modification, the enhancement processing portion <b>64</b> may perform both structure enhancement processing and color enhancement processing. If the enhancement processing portion <b>64</b> is capable of executing both structure enhancement processing and color enhancement processing, an input unit (not illustrated in the drawing) for specifying, in the enhancement processing portion <b>64</b>, the enhancement processing to be applied to the white-light image G<b>1</b>, via a user selection, may be provided.</p>
      <heading id="h-00011-en" level="1">Second Modification</heading>
      <p id="p-00042-en" num="0042">Next, a second modification of the observation apparatus <b>100</b> according to the first embodiment will be described.</p>
      <p id="p-00043-en" num="0043">The observation apparatus according to this modification is one in which the image processor <b>6</b> in the observation apparatus <b>100</b> is modified; as shown in <figref>FIG. 3</figref>, a division portion <b>65</b> is further provided in the image processor <b>6</b>.</p>
      <p id="p-00044-en" num="0044">The division portion <b>65</b> receives the white-light image G<b>1</b> from the white-light-image generating portion <b>61</b> and receives the fluorescence image G<b>2</b> from the fluorescence-image generating portion <b>62</b>. Then, the division portion <b>65</b> generates a division image G<b>2</b>′ formed by dividing the fluorescence image G<b>2</b> by the white-light image G<b>1</b> and outputs the generated division image G<b>2</b>′ to the extraction portion <b>63</b>. Using the division image G<b>2</b>′ instead of the fluorescence image G<b>2</b>, the extraction portion <b>63</b> extracts the region-of-interest from the division image G<b>2</b>′.</p>
      <p id="p-00045-en" num="0045">The gradation values of the fluorescence image G<b>2</b> depend on the observation distance between the distal end <b>2</b><i>a </i>of the insertion portion <b>2</b> and the observation target X. In other words, even if it is assumed that the actual intensity of the fluorescence emitted from the observation target X is the same, the gradation values of the fluorescence image G<b>2</b> become smaller as the observation distance increases. This relationship between the observation distance and the gradation values also holds for the white-light image G<b>1</b>. Thus, dividing the gradation value of each pixel in the fluorescence image G<b>2</b> by the gradation value of each pixel in the white-light image G<b>1</b> yields the division image G<b>2</b>′, in which the observation-distance-dependent variations in the gradation value in the fluorescence image G<b>2</b> are removed. In this way, by using the division image G<b>2</b>′ which reflects the actual fluorescence intensity more accurately than the fluorescence image G<b>2</b>, an advantage is afforded in that it is possible to extract the region-of-interest more accurately.</p>
      <heading id="h-00012-en" level="1">Third Modification</heading>
      <p id="p-00046-en" num="0046">Next, a third modification of the observation apparatus <b>100</b> according to the first embodiment will be described.</p>
      <p id="p-00047-en" num="0047">The observation apparatus according to this modification is one in which the image processor <b>6</b> in the observation apparatus <b>100</b> is modified; as shown in <figref>FIG. 4</figref>, a mean-gradation-value calculating portion <b>66</b> and an enhancement-level setting portion <b>67</b> are further provided in the image processor <b>6</b>.</p>
      <p id="p-00048-en" num="0048">In this modification, the extraction portion <b>63</b> outputs the positions P of the pixels constituting the region-of-interest to the enhancement processing portion <b>64</b> and outputs gradation-values I of those pixels to the mean-gradation-value calculating portion <b>66</b>.</p>
      <p id="p-00049-en" num="0049">The mean-gradation-value calculating portion <b>66</b> calculates a mean m of the gradation values I of the pixels constituting the region-of-interest, extracted by the extraction portion <b>63</b>, and outputs the calculated mean m of the gradation values I to the enhancement-level setting portion <b>67</b>.</p>
      <p id="p-00050-en" num="0050">The enhancement-level setting portion <b>67</b> sets a degree of enhancement processing, α, in the enhancement processing portion <b>64</b> on the basis of the mean m of the gradation values I input from the mean-gradation-value calculating portion <b>66</b>. More specifically, the enhancement-level setting portion <b>67</b> holds a function with which the mean m of the gradation values I and the degree of enhancement processing, α, are associated. As shown in <figref>FIG. 5</figref>, for example, this function is set so that the degree of enhancement processing, a decreases as the mean m of the gradation values I increases. The enhancement-level setting portion <b>67</b> derives the degree of enhancement processing, α, corresponding to the mean m of the gradation values I from the function and outputs the derived degree α to the enhancement processing portion <b>64</b>.</p>
      <p id="p-00051-en" num="0051">The enhancement processing portion <b>64</b> executes enhancement processing on the region in the white-light image G<b>1</b> that corresponds to the position P of the region-of-interest input from the extraction portion <b>63</b> by using the degree α input from the enhancement-level setting portion <b>67</b>. That is, even if the hemoglobin indexes are at similar levels relative to the mean, if the degree α is high, the enhancement processing portion <b>64</b> subjects the white-light image G<b>1</b> to IHb color enhancement processing so that the relevant positions are made more red.</p>
      <p id="p-00052-en" num="0052">With the thus-configured observation apparatus according to this modification, as shown in <figref>FIG. 6</figref>, when the region-of-interest is extracted in step S<b>2</b>, the mean m of the gradation values I of that region-of-interest is calculated in the mean-gradation-value calculating portion <b>66</b> (step S<b>5</b>). Then, the degree of enhancement processing, α, is determined in the enhancement-level setting portion <b>67</b> based on the calculated mean m of the gradation values I (step S<b>6</b>), and the region-of-interest in the white-light image G<b>1</b> is subjected to enhancement processing in the enhancement processing portion <b>64</b> with the determined degree α (step S<b>3</b>).</p>
      <p id="p-00053-en" num="0053">In this way, it is possible to appropriately enhance the region-of-interest according to the degree of difference in color of the region-of-interest relative to the surrounding region. Specifically, by determining the degree of enhancement processing, α, by considering the fluorescence intensities of the entire region-of-interest, when the fluorescence of the region-of-interest becomes weak, the region-of-interest is more strongly enhanced. Accordingly, even for a region-of-interest in which the difference in morphology of the tissue is small relative to the surrounding region, as in an early-stage lesion Y, for example, it can be displayed in a manner sufficiently enhanced with respect to the surrounding region, which affords the advantage that it can be reliably recognized by the user.</p>
      <p id="p-00054-en" num="0054">The function that associates the mean m of the gradation values I and the degree of enhancement processing, α, may be set such that the degree of enhancement processing, α, increases as the mean m of the gradation values I increases. With this function, when the fluorescence of the region-of-interest is high, the region-of-interest is more strongly enhanced. Accordingly, an advantage is afforded in that the user can reliably recognize a region-of-interest in which the fluorescence intensity is high.</p>
      <heading id="h-00013-en" level="1">Fourth Modification</heading>
      <p id="p-00055-en" num="0055">Next, a fourth modification of the observation apparatus <b>100</b> according to the first embodiment will be described.</p>
      <p id="p-00056-en" num="0056">The observation apparatus according to this modification is one in which the image processor <b>6</b> in the observation apparatus <b>100</b> is modified; as shown in <figref>FIG. 7</figref>, a determination portion (display switching portion) <b>68</b> and a combining portion <b>69</b> are further provided in the image processor <b>6</b>.</p>
      <p id="p-00057-en" num="0057">The determination portion <b>68</b> determines the observation distance between the observation target X and the distal end <b>2</b><i>a </i>of the insertion portion <b>2</b> at which the objective lens <b>51</b> is disposed, by using the area of the region-of-interest in the fluorescence image G<b>2</b>. More specifically, the determination portion <b>68</b> receives the positions P of the pixels constituting the region-of-interest from the extraction portion <b>63</b> and calculates the area of the region-of-interest in the fluorescence image G<b>2</b>. The area of the region-of-interest in the fluorescence image G<b>2</b> increases as the observation distance decreases. Therefore, the determination portion <b>68</b> can appropriately determine the observation distance from the area of the region-of-interest with computational processing alone.</p>
      <p id="p-00058-en" num="0058">When the calculated area of the region-of-interest is smaller than a prescribed threshold, the determination portion <b>68</b> outputs that white-light image G<b>1</b> input thereto from the white-light-image generating portion <b>61</b> to the combining portion <b>69</b>. On the other hand, when the area of the region-of-interest is equal to or larger than the prescribed threshold, the determination portion <b>68</b> outputs the white-light image G<b>1</b> input thereto from the white-light-image generating portion <b>61</b> to the enhancement processing portion <b>64</b>.</p>
      <p id="p-00059-en" num="0059">Once the white-light image G<b>1</b> and the position P of the region-of-interest are input to the combining portion <b>69</b> from the determination portion <b>68</b>, the combining portion <b>69</b> creates a marker at the position of the region-of-interest, overlays this marker on the white-light image G<b>1</b>, and outputs a white-light image G<b>1</b>″ having the marker overlaid thereon to the display <b>7</b>. The marker is not particularly limited; a marker in which the region-of-interest is filled-in may be used, or a line showing the outline of the region-of-interest, an arrow indicating the location of the region-of-interest, or a marker in which only the region-of-interest is replaced with a special-light image may be used.</p>
      <p id="p-00060-en" num="0060">With the thus-configured observation apparatus according to this modification, as shown in <figref>FIG. 8</figref>, when the region-of-interest is extracted in step S<b>2</b>, the area of the region-of-interest is determined by the determination portion <b>68</b>. Then, if the area of the region-of-interest is smaller than the prescribed threshold (NO at step S<b>7</b>), the white-light image G<b>1</b>″ in which the marker is combined with the region-of-interest (step S<b>8</b>) is displayed on the display <b>7</b> (step S<b>9</b>). On the other hand, if the area of the region-of-interest is equal to or larger than the prescribed threshold (YES at step S<b>7</b>), the white-light image G<b>1</b>′ in which the region-of-interest has been subjected to enhancement processing by the enhancement processing portion <b>64</b> is displayed on the display <b>7</b> (step S<b>4</b>).</p>
      <p id="p-00061-en" num="0061">In this way, when the region-of-interest is observed from a position that is sufficiently far away, the white-light image G<b>1</b>″ in which the region-of-interest is indicated by the marker is displayed on the display <b>7</b>. Accordingly, the user can easily recognize the region-of-interest that exists in the viewing field, no matter how small it is. Then, after the region-of-interest is recognized, the user makes the observation distance sufficiently short by bringing the distal end <b>2</b><i>a </i>of the insertion portion <b>2</b> close to the region-of-interest, whereupon the white-light image G<b>1</b>″ displayed on the display <b>7</b> is replaced with the white-light image G<b>1</b>′. That is to say, in the white-light image being observed, the region-of-interest is subjected to enhancement processing, whereas the marker disappears. Therefore, the user can perform detailed examination of the region-of-interest. In other words, with this modification, by switching between the images G<b>1</b>′ and G<b>1</b>″ displayed on the display <b>7</b> according to the observation distance, an advantage is afforded in that it is possible to show the user an image that is more useful depending on the situation.</p>
      <p id="p-00062-en" num="0062">In this modification, the determination portion <b>68</b> may determine the observation distance by using gradation values of the white-light image G<b>1</b> instead of the area of the region-of-interest in the fluorescence image G<b>2</b>. The overall brightness of the white-light image G<b>1</b> increases as the observation distance decreases. Therefore, the determination portion <b>68</b> can determine the observation distance by using the gradation values of the white-light image G<b>1</b>, and, similarly to the case where the area of the region-of-interest is used, an image, G<b>1</b>′ or G<b>1</b>″, that is more useful to the user can be displayed on the display <b>7</b>.</p>
      <p id="p-00063-en" num="0063">More specifically, the determination portion <b>68</b> calculates the mean gradation value of the white-light image G<b>1</b>. Then, when the calculated mean gradation value is larger than a prescribed threshold, the determination portion <b>68</b> outputs the white-light image G<b>1</b> to the enhancement processing portion <b>64</b>. On the other hand, when the mean gradation value is less than or equal to the prescribed threshold, the determination portion <b>68</b> outputs the white-light image G<b>1</b> to the combining portion <b>69</b>.</p>
      <p id="p-00064-en" num="0064">In addition, in this modification, the combining portion <b>69</b> may change the display form of the marker depending on the observation distance. For example, the combining portion <b>69</b> may increase the transparency of the marker in inverse proportion to the observation distance, that is to say, in proportion to the area of the region-of-interest in the fluorescence image G<b>2</b> or the mean gradation value of the white-light image G<b>1</b>.</p>
      <p id="p-00065-en" num="0065">By doing so, as the distal end <b>2</b><i>a </i>of the insertion portion <b>2</b> is brought closer to the region-of-interest, the marker that is overlaid on the white-light image G<b>1</b> becomes progressively more transparent and soon disappears. After the marker disappears, the region-of-interest that is subjected to enhancement processing is displayed at that position. Accordingly, an advantage is afforded in that it is possible to switch between the two images G<b>1</b>′ and G<b>1</b>″ without causing a sense of incongruity in the user who is observing the display <b>7</b>.</p>
      <heading id="h-00014-en" level="1">Second Embodiment</heading>
      <p id="p-00066-en" num="0066">Next, an observation apparatus <b>200</b> according to a second embodiment of the present invention will be described with reference to <figref>FIGS. 9 and 10</figref>. In the description of this embodiment, mainly parts that differ from those in the observation apparatus <b>100</b> according to the first embodiment described above will be described, and the parts that are common to the observation apparatus <b>100</b> will be assigned the same reference signs, and descriptions thereof will be omitted.</p>
      <p id="p-00067-en" num="0067">The main difference between the observation apparatus <b>200</b> according to this embodiment and the observation apparatus <b>100</b> according to the first embodiment is that an NBI image G<b>3</b> is obtained instead of the fluorescence image G<b>2</b>, and a region-of-interest is extracted from the NBI image G<b>3</b> based on a hue H.</p>
      <p id="p-00068-en" num="0068">More specifically, as shown in <figref>FIG. 9</figref>, a light source <b>3</b> is provided with a turret <b>34</b> having three filters. These three filters pass light in specific wavelength bands from among the light emitted from the xenon lamp <b>31</b>. Specifically, the three filters selectively transmit white light in a wavelength band of 400 nm to 700 nm, green narrow-band light in a narrow wavelength band having a peak wavelength of 540 nm, and blue narrow-band light having a peak wavelength of 415 nm, respectively. By rotating the turret <b>34</b>, the white light, the green narrow-band light, and the blue narrow-band light are sequentially input to the illumination unit <b>4</b>.</p>
      <p id="p-00069-en" num="0069">The image-acquisition unit <b>5</b> includes a single image-acquisition device <b>55</b>, such as a color CCD, that captures the light collected by the objective lens <b>51</b>. The image-acquisition device <b>55</b> sequentially obtains three types of image information, namely, white-light image information S<b>1</b>, green-light image information S<b>3</b>, and blue-light image information S<b>4</b>, by sequentially irradiating the observation target X with the white light, the green narrow-band light, and the blue narrow-band light from the illumination optical system <b>42</b> in the illumination unit <b>4</b>. Then, the image-acquisition unit <b>5</b> outputs the obtained image information S<b>1</b>, S<b>3</b>, and S<b>4</b> in turn to the image processor <b>6</b>.</p>
      <p id="p-00070-en" num="0070">The image processor <b>6</b> includes a control portion <b>70</b> that stores the three types of image information S<b>1</b>, S<b>3</b>, and S<b>4</b> input thereto from the image-acquisition device <b>55</b> and an NBI-image generating portion <b>71</b> that generates an NBI image G<b>3</b> from the green-light image information S<b>3</b> and the blue-light image information S<b>4</b> stored in the control portion <b>70</b>.</p>
      <p id="p-00071-en" num="0071">The control portion <b>70</b> controls a motor <b>34</b><i>a </i>of the turret <b>34</b> so as to assign the white-light image information S<b>1</b> to the white-light-image generating portion <b>61</b> and so as to assign the green-light image information S<b>3</b> and the blue-light image information S<b>4</b> to the NBI-image generating portion <b>71</b>, in synchronization with the switching of the light that is radiated onto the observation target X according to the rotation of the turret <b>34</b>.</p>
      <p id="p-00072-en" num="0072">The NBI-image generating portion <b>71</b> generates a red-light image from the green-light image information S<b>3</b>, generates a green-light image and a blue-light image from the blue-light image information S<b>4</b>, and generates the NBI image G<b>3</b> by combining the red-light image, the green-light image, and the blue-light image.</p>
      <p id="p-00073-en" num="0073">The green narrow-band light and the blue narrow-band light have the property that they are easily absorbed by hemoglobin. In addition, the blue narrow-band light is reflected close to the surface of biological tissue, and the green narrow-band light is reflected at a comparatively deep position in biological tissue. Therefore, in the green-light image and the blue-light image formed by capturing the reflected light (signal light) of the blue narrow-band light from the biological tissue, capillary blood vessels that exist in the outer layer of the biological tissue are clearly captured. On the other hand, in the red-light image formed by capturing the reflected light (signal light) of the green narrow-band light from the biological tissue, thick blood vessels that exist at comparatively deep positions in the biological tissue are clearly captured. In the NBI image G<b>3</b>, in which these two color images are superimposed, a lesion Y such as a squamous cell carcinoma takes on a dark brown color.</p>
      <p id="p-00074-en" num="0074">The extraction portion <b>63</b> extracts a region-of-interest based on the hue H of the NBI image G<b>3</b>. Here, the hue H is one of the properties of a color (hue, saturation, lightness) and is an aspect of color (for example, red, blue, yellow) represented by a numerical value in the range 0 to 360 using the so-called Munsell color wheel. More specifically, the extraction portion <b>63</b> calculates the hue H of each pixel in the NBI image G<b>3</b> and extracts pixels having a dark-brown color (for example, a hue H of 5 to 35) as the region-of-interest.</p>
      <p id="p-00075-en" num="0075">Next, the operation of the thus-configured observation apparatus <b>200</b> will be described.</p>
      <p id="p-00076-en" num="0076">To observe biological tissue inside a body, that is, the observation target X, using the observation apparatus <b>200</b> according to this embodiment, as in the first embodiment, the insertion portion <b>2</b> is inserted inside the body, and the light source <b>3</b> is operated. The white light, the green narrow-band light, and the blue narrow-band light from the light source <b>3</b> are sequentially radiated onto the observation target X via the coupling lens <b>33</b>, the light guide fiber <b>41</b>, and the illumination optical system <b>42</b>.</p>
      <p id="p-00077-en" num="0077">In the observation target X, the white light, the green narrow-band light, an the blue narrow-band light are sequentially reflected and are collected by the objective lens <b>51</b>. The white light, the green narrow-band light, and the blue narrow-band light collected by the objective lens <b>51</b> are obtained in the form of the white-light image information S<b>1</b>, the green-light image information S<b>3</b>, and the blue-light image information S<b>4</b>, respectively. The image information S<b>1</b>, S<b>3</b>, and S<b>4</b> obtained by the image-acquisition device <b>55</b> are then sent to the image processor <b>6</b>.</p>
      <p id="p-00078-en" num="0078">In the image processor <b>6</b>, the image information S<b>1</b>, S<b>3</b>, and S<b>4</b> are stored in the control portion <b>70</b>. Next, the white-light image information S<b>1</b> is input to the white-light-image generating portion <b>61</b>, where the white-light image G<b>1</b> is generated. Also, the green-light image information S<b>3</b> and the blue-light image information S<b>4</b> are input to the NBI-image generating portion <b>71</b>, where the NBI image G<b>3</b> is generated. The generated NBI image G<b>3</b> is sent to the extraction portion <b>63</b>, where a region-of-interest having a dark-brown color is extracted. Subsequently, a white-light image G<b>1</b>′ in which the region-of-interest is subjected to enhancement processing is displayed on the display <b>7</b>, as in steps S<b>3</b> and S<b>4</b> in the first embodiment.</p>
      <p id="p-00079-en" num="0079">Thus, with the observation apparatus <b>200</b> according to this embodiment, by using the NBI image G<b>3</b> as the special-light image, the region-of-interest is extracted based on the hue H. By doing so, as in the first embodiment, an advantage is afforded in that it is possible to show the user the white-light image G<b>1</b>′ in which the region-of-interest can be easily distinguished and the morphology of the region-of-interest can be confirmed in detail.</p>
      <p id="p-00080-en" num="0080">The individual modifications described in the first embodiment can also be suitably employed in this embodiment.</p>
      <heading id="h-00015-en" level="1">Modification</heading>
      <p id="p-00081-en" num="0081">Next, a modification of the observation apparatus <b>200</b> according to the second embodiment will be described.</p>
      <p id="p-00082-en" num="0082">The observation apparatus according to this modification is one in which the image processor <b>6</b> in the observation apparatus <b>200</b> is modified; as shown in <figref>FIG. 10</figref>, a mean-hue calculating portion <b>72</b> and an enhancement-level setting portion <b>73</b> are further provided in the image processor <b>6</b>.</p>
      <p id="p-00083-en" num="0083">In this modification, the extraction portion <b>63</b> outputs the positions P of pixels constituting the region-of-interest to the enhancement processing portion <b>64</b>, and outputs the hues H of those pixels to the mean-hue calculating portion <b>72</b>.</p>
      <p id="p-00084-en" num="0084">The mean-hue calculating portion <b>72</b> calculates the mean n of the hues H of the pixels constituting the region-of-interest, which were extracted by the extraction portion <b>63</b>. Then, the mean-hue calculating portion <b>72</b> outputs the calculated mean n of the hues H to the enhancement-level setting portion <b>73</b>.</p>
      <p id="p-00085-en" num="0085">The enhancement-level setting portion <b>73</b> sets a degree of enhancement processing, β, in the enhancement processing portion <b>64</b> on the basis of the mean n of the hues H input thereto from the mean-hue calculating portion <b>72</b>. More specifically, the enhancement-level setting portion <b>73</b> holds a table in which the mean n of the hues H and the degree of enhancement processing, β, are associated with each other. This table is set so that, for example, the degree of enhancement processing, β, increases as the mean n of the hues H approaches red or yellow, which are located on either side of dark-brown in the color wheel. The enhancement-level setting portion <b>73</b> derives the degree of enhancement processing, β, corresponding to the mean n of the hues H from the table and outputs the derived degree β to the enhancement processing portion <b>64</b>.</p>
      <p id="p-00086-en" num="0086">The enhancement processing portion <b>64</b> executes enhancement processing on a region in the white-light image G<b>1</b> corresponding to the position P of the region-of-interest input thereto from the extraction portion <b>63</b>, using the degree β input thereto from the enhancement-level setting portion <b>73</b>.</p>
      <p id="p-00087-en" num="0087">With the thus-configured observation apparatus according to this modification, once the region-of-interest is extracted in the extraction portion <b>63</b>, the mean n of the hues H of that region-of-interest is calculated in the mean-hue calculating portion <b>72</b>. Then, the degree of enhancement processing, β, is determined in the enhancement-level setting portion <b>73</b> on the basis of the calculated mean n of the hues H, and the region-of-interest in the white-light image G<b>1</b> is subjected to enhancement processing in the enhancement processing portion <b>64</b> with the determined degree β.</p>
      <p id="p-00088-en" num="0088">In this way, by determining the degree of enhancement processing, β, by considering the hues H of the entire region-of-interest, when the hues H of the region-of-interest approach red or yellow, the region-of-interest is more strongly enhanced. Accordingly, even for a region-of-interest in which the difference in morphology of the tissue is small relative to the surrounding region, as in an early-stage lesion Y, for example, it can be displayed in a manner sufficiently enhanced with respect to the surrounding region, which affords the advantage that it can be reliably recognized by the user.</p>
      <p id="p-00089-en" num="0089">The table that associates the mean n of the hues H and the level of enhancement processing is set so that the degree of enhancement processing, β, increases as the mean n of the hues H approaches dark-brown in the color wheel. With this function, when the hue H of the region-of-interest is close to dark-brown, the region-of-interest is more strongly enhanced. Accordingly, an advantage is afforded in that a region-of-interest with a high concentration of blood vessels can be reliably recognized by the user.</p>
      <heading id="h-00016-en" level="1">Third Embodiment</heading>
      <p id="p-00090-en" num="0090">Next, an observation apparatus <b>300</b> according to a third embodiment of the present invention will be described with reference to <figref>FIG. 11</figref>.</p>
      <p id="p-00091-en" num="0091">In the description of this embodiment, mainly parts that differ from those in the observation apparatus <b>100</b> according to the first embodiment described above will be described, and parts that are common to the observation apparatus <b>100</b> will be assigned the same reference signs, and descriptions thereof will be omitted.</p>
      <p id="p-00092-en" num="0092">The main difference between the observation apparatus <b>300</b> according to this embodiment and the observation apparatus <b>100</b> is that it obtains an autofluorescence image G<b>4</b> instead of the fluorescence image G<b>2</b>, and extracts a region-of-interest from the autofluorescence image G<b>4</b> on the basis of the hue H.</p>
      <p id="p-00093-en" num="0093">More specifically, as shown in <figref>FIG. 11</figref>, the light source <b>3</b> is provided with a turret <b>34</b> having three filters. These three filters pass light in specific wavelength bands from among the light emitted from the xenon lamp <b>31</b>. Specifically, the three filters selectively transmit white light in a wavelength band of 400 nm to 700 nm, green reference light having a peak wavelength of 550 nm, and blue excitation light having a peak wavelength of 400 nm, respectively. By rotating the turret <b>34</b>, the white light, the reference light, and the excitation light are sequentially input to the illumination unit <b>4</b>.</p>
      <p id="p-00094-en" num="0094">The image acquisition unit <b>5</b> is a binocular system that obtains white-light image information S<b>1</b> and autofluorescence image information S<b>5</b> and S<b>6</b> with separate optical systems. In other words, the image-acquisition unit <b>5</b> includes two optical systems each having an objective lens <b>51</b> that collects light coming from the observation target X, a focusing lens <b>53</b> that focuses the light emerging from the objective lens <b>51</b>, and an image-acquisition device <b>55</b> or <b>56</b> that captures the light focused by the focusing lens <b>53</b>. These two optical systems are provided side-by-side at the distal end of the insertion portion <b>2</b>.</p>
      <p id="p-00095-en" num="0095">The first optical system obtains the white-light image information S<b>1</b> with the image-acquisition device <b>55</b>, such as a color CCD.</p>
      <p id="p-00096-en" num="0096">The second optical system further includes an excitation-light cutting filter <b>57</b> between the objective lens <b>51</b> and the focusing lens <b>53</b> and obtains the autofluorescence image information S<b>5</b> and S<b>6</b> by capturing autofluorescence emitted from the observation target X and green return light with the image-acquisition device <b>56</b>, such as a high-sensitivity monochrome CCD. In this embodiment, the excitation-light cutting filter <b>57</b> selectively transmits light in a wavelength band of 500 to 630 nm, corresponding to the autofluorescence of the observation target X and the green return light, and blocks the excitation light.</p>
      <p id="p-00097-en" num="0097">By sequentially irradiating the observation target X with the white light, the reference light, and the excitation light from the illumination optical system <b>42</b> in the illumination unit <b>4</b>, the image-acquisition devices <b>55</b> and <b>56</b> sequentially obtain three types of image information, namely, the white-light image information S<b>1</b>, first autofluorescence image information S<b>5</b>, and second autofluorescence image information S<b>6</b>. Then, each of the image-acquisition devices <b>55</b> and <b>56</b> outputs the obtained image information S<b>1</b>, S<b>5</b>, and S<b>6</b> in turn to the image processor <b>6</b>.</p>
      <p id="p-00098-en" num="0098">The image processor <b>6</b> includes a control portion <b>70</b> that stores the three types of image information S<b>1</b>, S<b>5</b>, and S<b>6</b> obtained by the image-acquisition devices <b>55</b> and <b>56</b> and an autofluorescence-image generating portion <b>74</b> that generates an autofluorescence image G<b>4</b> from the first autofluorescence image information S<b>5</b> and the second autofluorescence image information S<b>6</b> stored in the control portion <b>70</b>.</p>
      <p id="p-00099-en" num="0099">The control portion <b>70</b> controls the motor <b>34</b><i>a </i>of the turret <b>34</b> so as to assign the white-light image information S<b>1</b> to the white-light-image generating portion <b>61</b> and so as to assign the first autofluorescence image information S<b>5</b> and the second autofluorescence image information S<b>6</b> to the autofluorescence-image generating portion <b>74</b>, in synchronization with the switching of the light that is radiated onto the observation target X according to the rotation of the turret <b>34</b>.</p>
      <p id="p-00100-en" num="0100">The autofluorescence-image generating portion <b>74</b> generates the first autofluorescence image from the first autofluorescence image information S<b>5</b> and generates the second autofluorescence image from the second autofluorescence image information S<b>6</b>. At this time, the autofluorescence-image generating portion <b>74</b> pseudo-colors the first autofluoresence image with red and blue and pseudo-colors the second autofluorescence image with green. Then, the autofluorescence-image generating portion <b>74</b> generates a color autofluorescence image G<b>4</b> by combining the pseudo-colored first autofluorescence image and second autofluorescence image. In the autofluorescence image G<b>4</b>, the lesion Y is displayed as a red-violet (for example, a hue H from 300 to 350) region.</p>
      <p id="p-00101-en" num="0101">The extraction portion <b>63</b> extracts the region-of-interest based on the hues H of the autofluorescence image G<b>4</b>. More specifically, the extraction portion <b>63</b> calculates the hue H of each pixel of the autofluorescence image G<b>4</b> and extracts pixels having a red-violet color (for example, a hue H of 300 to 350) as a region-of-interest.</p>
      <p id="p-00102-en" num="0102">Next, the operation of the thus-configured observation apparatus <b>300</b> will be described.</p>
      <p id="p-00103-en" num="0103">To observe biological tissue inside a body, that is, the observation target X, using the observation apparatus <b>300</b> according to this embodiment, the observation target is sequentially irradiated with the white light, the reference light, and the excitation light, similarly to the second embodiment.</p>
      <p id="p-00104-en" num="0104">The white light is reflected at the surface of the observation target X. On the other hand, the excitation light excites a substance contained in the observation target X, thereby emitting autofluorescence from the observation target X. The white light collected by the first objective lens <b>51</b> is obtained in the form of the white-light image information S<b>1</b> by the image-acquisition device <b>55</b>. The reference light and the autofluorescence collected by the second objective lens <b>51</b> are respectively obtained in the form of the first autofluorescence image information S<b>5</b> and the second autofluorescence image information S<b>6</b> by the image-acquisition device <b>56</b>. The image information S<b>1</b>, S<b>5</b>, and S<b>6</b> obtained by the image-acquisition devices <b>55</b> and <b>56</b> are sent to the image processor <b>6</b>.</p>
      <p id="p-00105-en" num="0105">In the image processor <b>6</b>, the image information S<b>1</b>, S<b>5</b>, and S<b>6</b> are stored in the control portion <b>70</b>. Then, the white-light image information S<b>1</b> is input to the white-light-image generating portion <b>61</b>, where the white-light image G<b>1</b> is generated. On the other hand, the first autofluorescence image information S<b>5</b> and the second autofluorescence image information S<b>6</b> are input to the autofluorescence-image generating portion <b>74</b>, where the autofluorescence image G<b>4</b> is generated. The generated autofluorescence image G<b>4</b> is sent to the extraction portion <b>63</b>, where a region-of-interest having a red-violet color is extracted. Subsequently, the white-light image G<b>1</b>′ in which the region-of-interest is subjected to enhancement processing is displayed on the display <b>7</b>, similarly to steps S<b>3</b> and S<b>4</b> in the first embodiment.</p>
      <p id="p-00106-en" num="0106">In this way, with the observation apparatus <b>300</b> according to this embodiment, the region-of-interest is extracted on the basis of the hue H, using the autofluorescence image G<b>4</b> as a special-light image. In this way, too, as with the first embodiment, an advantage is afforded in that it is possible to show the user the white-light image G<b>1</b>′ in which the region-of-interest can be easily distinguished and the morphology of the region-of-interest can be confirmed in detail.</p>
      <p id="p-00107-en" num="0107">The individual modifications described in the first embodiment and the second embodiment can also be suitably employed in this embodiment.</p>
      <heading id="h-00017-en" level="1">Fourth Embodiment</heading>
      <p id="p-00108-en" num="0108">Next, an observation apparatus <b>400</b> according to a fourth embodiment of the present invention will be described with reference to <figref>FIG. 12</figref>.</p>
      <p id="p-00109-en" num="0109">The observation apparatus <b>400</b> according to this embodiment is a combination of the first embodiment and the second embodiment. Therefore, in the description of this embodiment, parts that are common to the first embodiment and the second embodiment are assigned the same reference signs, and descriptions thereof will be omitted.</p>
      <p id="p-00110-en" num="0110">As shown in <figref>FIG. 12</figref>, the light source <b>3</b> includes a turret <b>34</b> having three filters. These three filters pass light in specific wavelength bands from among the light emitted from the xenon lamp <b>31</b>. In this embodiment, one of the three filters is the same as the filter <b>32</b> in the first embodiment and selectively transmits the excitation light and the white light. The other two filters are the same as two of the filters in the second embodiment and selectively transmit green narrow-band light and blue narrow-band light, respectively. By rotating the turret <b>34</b>, the excitation light and white light, the green narrow-band light, and the blue narrow-band light are sequentially input in a time-division manner to the illumination unit <b>4</b>.</p>
      <p id="p-00111-en" num="0111">The image processor <b>6</b> includes both the fluorescence-image generating portion <b>62</b> and the NBI-image generating portion <b>71</b>. Furthermore, the image processor <b>6</b> includes two extraction portions <b>63</b> that extract regions-of-interest from the fluorescence image G<b>2</b> and the NBI image G<b>3</b>, respectively. The first extraction portion <b>63</b> extracts the region-of-interest on the basis of gradation values from the fluorescence image G<b>2</b> input thereto from the fluorescence-image generating portion <b>62</b>, similarly to the extraction portion <b>63</b> in the first embodiment. The second extraction portion <b>63</b> extracts the region-of-interest on the basis of hues H from the NBI image G<b>3</b> input thereto from the NBI-image generating portion <b>71</b>, similarly to the extraction portion <b>63</b> in the second embodiment.</p>
      <p id="p-00112-en" num="0112">The enhancement processing portion <b>64</b> compares the positions P in the two regions-of-interest received from each of the extraction portions <b>63</b> and executes enhancement processing on the region that is common to these two regions-of-interest.</p>
      <p id="p-00113-en" num="0113">With the thus-configured observation apparatus <b>400</b> according to this embodiment, by using the fluorescence image G<b>2</b> and the NBI image G<b>3</b> as special-light images, a region that is common to the two regions-of-interest extracted from these two special-light images serves as the final region-of-interest. Accordingly, since the region-of-interest, such as a lesion Y in the observation target X, is more accurately extracted, the user can more-accurately recognize the position of the region-of-interest.</p>
      <p id="p-00114-en" num="0114">The individual modifications described in the first embodiment and the second embodiment can also be suitably employed in this embodiment.</p>
      <heading id="h-00018-en" level="1">REFERENCE SIGNS LIST</heading>
      <p id="p-00115-en" num="0000">
        <ul id="ul-00001-en" list-style="none">
          <li>
            <b>100</b>, <b>200</b>, <b>300</b>, <b>400</b> Observation apparatus</li>
          <li>
            <b>2</b> Insertion portion</li>
          <li>
            <b>2</b>
            <i>a </i>Distal end</li>
          <li>
            <b>3</b> Light source</li>
          <li>
            <b>31</b> Xenon lamp</li>
          <li>
            <b>32</b> Filter</li>
          <li>
            <b>33</b> Coupling lens</li>
          <li>
            <b>34</b> Turret</li>
          <li>
            <b>4</b> Illumination unit</li>
          <li>
            <b>41</b> Light guide fiber</li>
          <li>
            <b>42</b> Illumination optical system</li>
          <li>
            <b>5</b> Image-acquisition unit</li>
          <li>
            <b>51</b> Objective lens</li>
          <li>
            <b>52</b> Dichroic mirror</li>
          <li>
            <b>53</b>, <b>54</b> Focusing lens</li>
          <li>
            <b>55</b>, <b>56</b> Image-acquisition device</li>
          <li>
            <b>57</b> Excitation-light cutting filter</li>
          <li>
            <b>6</b> Image processor (processor)</li>
          <li>
            <b>61</b> White-light-image generating portion (return-light-image generating portion)</li>
          <li>
            <b>62</b> Fluorescence-image generating portion (special-light-image generating portion)</li>
          <li>
            <b>63</b> Extraction portion</li>
          <li>
            <b>64</b> Enhancement processing portion</li>
          <li>
            <b>65</b> Division portion</li>
          <li>
            <b>66</b> Mean-gradation-value calculating portion</li>
          <li>
            <b>67</b> Enhancement-level setting portion</li>
          <li>
            <b>68</b> Determination portion (display switching portion)</li>
          <li>
            <b>69</b> Combining portion</li>
          <li>
            <b>70</b> Control portion</li>
          <li>
            <b>71</b> NBI-image generating portion (special-light-image generating portion)</li>
          <li>
            <b>72</b> Mean-hue calculating portion</li>
          <li>
            <b>73</b> Enhancement-level setting portion</li>
          <li>
            <b>74</b> Autofluorescence-image generating portion (special-light-image generating portion)</li>
          <li>G<b>1</b> White-light image (return-light image)</li>
          <li>G<b>2</b> Fluorescence image (special-light image)</li>
          <li>G<b>2</b>′ Division image</li>
          <li>G<b>3</b> NBI image (special-light image)</li>
          <li>G<b>4</b> Autofluorescence image (special-light image)</li>
          <li>X Observation target</li>
          <li>Y Lesion</li>
        </ul>
      </p>
    </detailed-desc>
  </description>
  <claims id="claims_eng" lang="eng" format="original" date-changed="20150917">
    <claim num="1" id="clm-00001-en" independent="true">
      <claim-text>
        <b>1</b>. An observation apparatus comprising:
<claim-text>a light source that irradiates a subject with illumination light and special light in a wavelength band different from that of the illumination light, which acts on a specific region of the subject; and</claim-text><claim-text>a processor comprising hardware, wherein the processor is configured to implement:</claim-text><claim-text>a return-light-image generating portion that generates a return-light image based on captured return light emitted from the subject due to irradiation with the illumination light from the light source;</claim-text><claim-text>a special-light-image generating portion that generates a special-light image based on captured signal light emitted from the subject due to irradiation with the special light from the light source;</claim-text><claim-text>an extraction portion that extracts the specific region from the special-light image generated by the special-light-image generating portion; and</claim-text><claim-text>an enhancement processing portion that performs enhancement processing, which is based on return-light image information, on the return-light image generated by the return-light-image generating portion, in a region corresponding to the specific region extracted by the extraction portion.</claim-text></claim-text>
    </claim>
    <claim num="2" id="clm-00002-en">
      <claim-text>
        <b>2</b>. The observation apparatus according to <claim-ref idref="clm-00001-en">claim 1</claim-ref>, wherein the enhancement processing portion enhances the contrast of at least one of structure and color.</claim-text>
    </claim>
    <claim num="3" id="clm-00003-en">
      <claim-text>
        <b>3</b>. The observation apparatus according to <claim-ref idref="clm-00001-en">claim 1</claim-ref>,
<claim-text>wherein, as the special light, the light source radiates excitation light that excites a fluorescent substance contained in the specific region, and</claim-text><claim-text>wherein the special-light-image generating portion generates, as the special-light image, a fluorescence image based on captured fluorescence from the fluorescent substance.</claim-text></claim-text>
    </claim>
    <claim num="4" id="clm-00004-en">
      <claim-text>
        <b>4</b>. The observation apparatus according to <claim-ref idref="clm-00001-en">claim 1</claim-ref>, wherein the extraction portion extracts, as the specific region, a region having a gradation value equal to or higher than a prescribed threshold.</claim-text>
    </claim>
    <claim num="5" id="clm-00005-en">
      <claim-text>
        <b>5</b>. The observation apparatus according to <claim-ref idref="clm-00004-en">claim 4</claim-ref>, wherein the processor is further configured to implement an enhancement-level setting portion that sets, for the specific region, a degree of enhancement to be performed by the enhancement processing portion, on the basis of the gradation value of the specific region extracted by the extraction portion.</claim-text>
    </claim>
    <claim num="6" id="clm-00006-en">
      <claim-text>
        <b>6</b>. The observation apparatus according to <claim-ref idref="clm-00001-en">claim 1</claim-ref>, wherein the light source radiates narrow-band light as the special light, and
<claim-text>wherein the special-light-image generating portion generates, as the special-light image, a narrow-band-light image based on captured return light from the subject irradiated with the narrow-band light.</claim-text></claim-text>
    </claim>
    <claim num="7" id="clm-00007-en">
      <claim-text>
        <b>7</b>. The observation apparatus according to <claim-ref idref="clm-00001-en">claim 1</claim-ref>, wherein the light source radiates, as the special light, excitation light that excites autofluorescence in a substance contained in the subject, and
<claim-text>wherein the special-light-image generating portion generates, as the special-light image, an autofluorescence image based on captured autofluorescence from the substance.</claim-text></claim-text>
    </claim>
    <claim num="8" id="clm-00008-en">
      <claim-text>
        <b>8</b>. The observation apparatus according to <claim-ref idref="clm-00001-en">claim 1</claim-ref>, wherein the extraction portion extracts, as the specific region, a region having a prescribed hue.</claim-text>
    </claim>
    <claim num="9" id="clm-00009-en">
      <claim-text>
        <b>9</b>. The observation apparatus according to <claim-ref idref="clm-00008-en">claim 8</claim-ref>, wherein the processor further configured to implement an enhancement-level setting portion that sets, for the region, a degree of enhancement to be performed by the enhancement processing portion on the basis of a hue of the specific region extracted by the extraction portion.</claim-text>
    </claim>
    <claim num="10" id="clm-00010-en">
      <claim-text>
        <b>10</b>. The observation apparatus according to <claim-ref idref="clm-00001-en">claim 1</claim-ref>, further comprising a display that displays an image,
<claim-text>wherein the processor is further configured to implement:</claim-text><claim-text>a combining portion that combines a marker showing the specific region extracted by the extraction portion with the return-light image generated by the return-light-image generating portion;</claim-text><claim-text>a determination portion that determines an observation distance to the subject; and</claim-text><claim-text>a display switching portion that selectively displays, on the display, the return-light image in which the specific region is enhanced by the enhancement processing portion and the return-light image with which the marker is combined by the combining portion, on the basis of the observation distance determined by the determination portion.</claim-text></claim-text>
    </claim>
    <claim num="11" id="clm-00011-en">
      <claim-text>
        <b>11</b>. The observation apparatus according to <claim-ref idref="clm-00010-en">claim 10</claim-ref>, wherein the determination portion determines the observation distance by using a gradation value of the return-light image generated by the return-light-image generating portion.</claim-text>
    </claim>
    <claim num="12" id="clm-00012-en">
      <claim-text>
        <b>12</b>. The observation apparatus according to <claim-ref idref="clm-00010-en">claim 10</claim-ref>, wherein the determination portion determines the observation distance by using an area, in the special-light image, of the specific region extracted by the extraction portion. </claim-text>
    </claim>
  </claims>
  <drawings id="drawings" format="original">
    <figure num="1">
      <img he="N/A" wi="N/A" file="US20150257635A1_00001.PNG" alt="clipped image" img-content="drawing" img-format="png" original="US20150257635A1-20150917-D00000.TIF" />
    </figure>
    <figure num="2">
      <img he="N/A" wi="N/A" file="US20150257635A1_00002.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257635A1-20150917-D00001.TIF" />
    </figure>
    <figure num="3">
      <img he="N/A" wi="N/A" file="US20150257635A1_00003.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257635A1-20150917-D00002.TIF" />
    </figure>
    <figure num="4">
      <img he="N/A" wi="N/A" file="US20150257635A1_00004.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257635A1-20150917-D00003.TIF" />
    </figure>
    <figure num="5">
      <img he="N/A" wi="N/A" file="US20150257635A1_00005.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257635A1-20150917-D00004.TIF" />
    </figure>
    <figure num="6">
      <img he="N/A" wi="N/A" file="US20150257635A1_00006.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257635A1-20150917-D00005.TIF" />
    </figure>
    <figure num="7">
      <img he="N/A" wi="N/A" file="US20150257635A1_00007.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257635A1-20150917-D00006.TIF" />
    </figure>
    <figure num="8">
      <img he="N/A" wi="N/A" file="US20150257635A1_00008.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257635A1-20150917-D00007.TIF" />
    </figure>
    <figure num="9">
      <img he="N/A" wi="N/A" file="US20150257635A1_00009.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257635A1-20150917-D00008.TIF" />
    </figure>
    <figure num="10">
      <img he="N/A" wi="N/A" file="US20150257635A1_00010.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257635A1-20150917-D00009.TIF" />
    </figure>
    <figure num="11">
      <img he="N/A" wi="N/A" file="US20150257635A1_00011.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257635A1-20150917-D00010.TIF" />
    </figure>
    <figure num="12">
      <img he="N/A" wi="N/A" file="US20150257635A1_00012.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257635A1-20150917-D00011.TIF" />
    </figure>
    <figure num="13">
      <img he="N/A" wi="N/A" file="US20150257635A1_00013.PNG" alt="drawing sheet" img-content="drawing" img-format="png" original="US20150257635A1-20150917-D00012.TIF" />
    </figure>
    <figure num="14">
      <img he="N/A" wi="N/A" file="US20150257635A1_00014.PNG" alt="thumbnail image" img-content="drawing" img-format="png" original="US20150257635A1-20150917-D00000.TIF" />
    </figure>
  </drawings>
  <image file="US20150257635A1.PDF" type="pdf" size="863586" pages="22" />
</lexisnexis-patent-document>